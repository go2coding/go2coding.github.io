

<!DOCTYPE html>
<html lang="zh-CN">

<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1" />
    <meta name="viewport"
        content="width=device-width, initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no" />
    <meta name="theme-color" content="#f9f9f9" />

	<title>大模型未来发展：RAGvs长文本，谁更胜一筹？｜Z沙龙第8期 作者： 目标检测和深度学习 来源： 目标检测和深度学习 「Z计划」 是智谱 AI 面向未上市初创企业与优秀独立开发者/团队，提供 Tokens 赞助、投资支持和技术支持等资源的创新加速计划。面向全球，持续招募中！（点击报名）「Z沙龙」 是支持该计划的面向大模型领域的线下活  | AiBard123| ai工具网址导航,ai最新产品</title>
	<link rel="shortcut icon" href="/assets/images/favicon.png" />
    <meta name="keywords" content="chatgpt,AI,AI聊天,AI文本生成,AI绘画,AI编程,AI电商" />
    <meta name="description" content="AiBard123 网址导航 | 免费chatgpt 汇集各类先进的人工智能产品，旨在帮助用户更快速地了解和使用这些产品,轻松地浏览不同领域的AI产品，包括语音识别、图像处理、自然语言处理。" />
    
    <meta name="baidu-site-verification" content="codeva-cCAOSG8MBO" />
    
    <link rel="stylesheet" id="block-library-css"
        href="/assets/css/block-library.min-5.6.2.css" type="text/css" media="all" />
    <link rel="stylesheet" id="iconfont-css" href="/assets/css/iconfont-3.03029.1.css"
        type="text/css" media="all" />

    
    <link href="/scss/style.min.css" rel="stylesheet" />
    
		    <link rel="stylesheet" id="iowen-css" href="/assets/css/style-3.03029.1.css"
        type="text/css" media="all" />
    <link rel="stylesheet" id="custom-css" href="/assets/css/custom-style.css"
        type="text/css" media="all" />
		
		<link rel="stylesheet" href=/plugins/font-awesome/css/font-awesome.min.css />


    <link rel="stylesheet" id="fortawesome-css" href="/assets/fontawesome-5.15.4/css/all.min.css" type="text/css" />


    <script type="text/javascript" src="/assets/js/jquery.min-3.2.1.js" id="jquery-js"></script>
    <script type="text/javascript" src="/assets/js/content-search.js"  id="content-search-js"></script>

    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2073588164294660"
     crossorigin="anonymous"></script>

	
    <script>
        

		var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?8450bc732b2a86f7e4aec4ebd9fd8252";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();

        
    </script>
    

    
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-7071W80M2K"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-7071W80M2K');
    </script>

</head>


    <div class="page-container">
	
	<div id="sidebar" class="sticky sidebar-nav fade animate-nav" style="width: 170px">
        
            <div class="modal-dialog h-100 sidebar-nav-inner">
                <div class="sidebar-logo border-bottom border-color">
                    
                    <div class="logo overflow-hidden">
                        <a href="https://aibard123.com/" class="logo-expanded">
                            <img src="/assets/images/bt8-expand-light.png" height="40" class="logo-light"
                                alt="AiBard123| ai工具网址导航,ai最新产品">
                            <img src="/assets/images/bt8-expand-dark.png" height="40" class="logo-dark d-none"
                                alt="AiBard123| ai工具网址导航,ai最新产品">
                        </a>
                        <a href="https://aibard123.com/" class="logo-collapsed">
                            <img src="/assets/images/bt.png" height="40" class="logo-light"
                                alt="AiBard123| ai工具网址导航,ai最新产品">
                            <img src="/assets/images/bt.png" height="40" class="logo-dark d-none"
                                alt="AiBard123| ai工具网址导航,ai最新产品">
                        </a>
                    </div>
                    
                </div>
                <div class="sidebar-menu flex-fill">
                    <div class="sidebar-scroll">
                        <div class="sidebar-menu-inner">
                            <ul>
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#00834a9dd147b04c5d53d4368cdb0b57" class="smooth">
                                            <i class="fas fa-sun fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>本月热门</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#db0311e7ecfedd24d157f0ceb4a0897f" class="smooth">
                                            <i class="fas fa-star-and-crescent fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>热门网站</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#21b5cbb2c769010fec3ce029a5f8a4a3" class="smooth">
                                            <i class="far fa-star fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>国内热门</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#8310718935e8ec25ce0350de01e3f7dc" class="smooth">
                                            <i class="fas fa-phone fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>对话工具</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#d58e850d9115797306c2edf61ac6ddd8" class="smooth">
                                            <i class="fas fa-newspaper fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>写作</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#2a7418a5f8f1ca4e054364a9300657df" class="smooth">
                                            <i class="fas fa-image fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>图像生成</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#7808a68ee1b34dab43011429a12de19e" class="smooth">
                                            <i class="fas fa-image fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>图像处理</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#6729afc51f5ac49a828812fa0eb0c82f" class="smooth">
                                            <i class="fas fa-video fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>音视频</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#e5ce844860451fff3faf3d8f8894971d" class="smooth">
                                            <i class="fas fa-music fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>音乐生成</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#db53804b7d726967c58fcc8c9ca03d27" class="smooth">
                                            <i class="fas fa-language fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>办公</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#47b7af9547e034d28fe6f6d439968ac8" class="smooth">
                                            <i class="fas fa-copy fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>提示词</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#41282bf95e43c64d579757573a03cdde" class="smooth">
                                            <i class="fas fa-code fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>编程</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#fd71852fd52d5e18ef4f9a252f1eac58" class="smooth">
                                            <i class="fas fa-search fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>AI搜索</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#81b1637fbe47625dbdf2094acd3b6683" class="smooth">
                                            <i class="fas fa-language fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>文本翻译</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#2e9ba3fa6e1ed0e9311b3e97f97f9a40" class="smooth">
                                            <i class="fas fa-book fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>学习网站</span>
                                        </a>
                                    </li>
                                    
                                
                            </ul>           
                        </div>
                    </div>
                </div>
                <div class="border-top py-2 border-color">
                    <div class="flex-bottom">
                        <ul>
			    <li id="menu-item-212"
                                 class="menu-item menu-item-type-custom menu-item-object-custom menu-item-212 sidebar-item">
                                 <a href="#friendlink" class="smooth">
                                     <i class="fab fa-staylinked icon-fw icon-lg mr-2"></i>
                                     <span>友情链接</span>
                                 </a>
                            </li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
    </div>


<div class="flex-fill grid-bg">
    <div class="big-header-banner">
        <div id="header" class="page-header sticky">
            <div class="navbar navbar-expand-md">
                <div class="container-fluid p-0">

                    <a href="" class="navbar-brand d-md-none" title="AiBard123| ai工具网址导航,ai最新产品">
                        <img src="/assets/images/bt.png" class="logo-light"
                            alt="AiBard123| ai工具网址导航,ai最新产品">
                        <img src="/assets/images/bt.png" class="logo-dark d-none"
                            alt="AiBard123| ai工具网址导航,ai最新产品">
                    </a>

                    <div class="collapse navbar-collapse order-2 order-md-1">
                        <div class="header-mini-btn">
                            <label>
                                <input id="mini-button" type="checkbox">
                                <svg viewbox="0 0 100 100" xmlns="http://www.w3.org/2000/svg">
                                    <path class="line--1" d="M0 40h62c18 0 18-20-17 5L31 55"></path>
                                    <path class="line--2" d="M0 50h80"></path>
                                    <path class="line--3" d="M0 60h62c18 0 18 20-17-5L31 45"></path>
                                </svg>
                            </label>

                        </div>

                        <ul class="navbar-nav site-menu" style="margin-right: 16px;">
                        
			<li >
				<a href="/">
                                    <i class="fa fa-home fa-lg mr-2"></i>
                                    <span>首页</span>
                                </a>
				<ul class="sub-menu">
				
				</ul>
			    </li>
			
			</ul>

                        
                        <div class="rounded-circle weather">
                            <div id="he-plugin-simple" style="display: contents;"></div>
                            <script>WIDGET = {
                                    CONFIG: {
                                        "modules": "01234",
                                        "background": 5,
                                        "tmpColor": "008000",
                                        "tmpSize": 14,
                                        "cityColor": "008000",
                                        "citySize": 14,
                                        "aqiColor": "#008000",
                                        "aqiSize": 14,
                                        "weatherIconSize": 24,
                                        "alertIconSize": 18,
                                        "padding": "10px 10px 10px 10px",
                                        "shadow": "1",
                                        "language": "auto",
                                        "borderRadius": 5,
                                        "fixed": "false",
                                        "vertical": "middle",
                                        "horizontal": "left",
                                        "key": "085791e805a24491b43b06cf58ab31e7"
                                    }
                                }
                            </script>
                            <script src="https://widget.qweather.net/simple/static/js/he-simple-common.js?v=2.0"></script>
                        </div>
                        
                    </div>

                    <ul class="nav navbar-menu text-xs order-1 order-md-2">
                        
                        
                        <li class="nav-item mr-3 mr-lg-0 d-none d-lg-block">
                            <script>
                                fetch('https://v1.hitokoto.cn')
                                    .then(response => response.json())
                                    .then(data => {
                                    const hitokoto = document.getElementById('hitokoto_text')
                                    hitokoto.href = 'https://hitokoto.cn/?uuid=' + data.uuid
                                    hitokoto.innerText = data.hitokoto
                                    })
                                    .catch(console.error)
                            </script>                           
                            <div id="hitokoto"><a href="#" target="_blank" id="hitokoto_text">疏影横斜水清浅，暗香浮动月黄昏。</a></div>
                        </li>
                        
                        
                        <li class="nav-search ml-3 ml-md-4">
                            <a href="javascript:" data-toggle="modal" data-target="#search-modal"><i
                                    class="iconfont icon-search icon-2x"></i></a>
                        </li>
                        <li class="nav-item d-md-none mobile-menu ml-3 ml-md-4">
                            <a href="javascript:" id="sidebar-switch" data-toggle="modal"
                                data-target="#sidebar"><i class="iconfont icon-classification icon-2x"></i></a>
                        </li>
                    </ul>
                </div>
            </div>
        </div>
        <div class="placeholder" style="height:74px"></div>
    </div>




<body class="page-body boxed-container  io-grey-mode">
    <main role="main" class="flex-shrink-0">
    <div class="container">
        
        <div class="content">
            <style>
    body{
	    background: #f9f9f9;
	}

    h1, h2, h3, h4, h5, h6 {
        margin-top: 1.5rem;
        margin-bottom: 1.5rem;
    }


 
@media (min-width: 1000px) {
  .container, .container-sm {
    max-width: 800px;
  }
}

</style>

<div class="featured-post-content">

    <a href="/digest/" class="featured-post-title">
       AI 文摘
    </a>

</div>

<section class="blog-single">
  <div class="container">
    <div class="row">

      <div class="col-lg-12 order-1 order-lg-2">
        <article class="single-blog">
          <p class="title">大模型未来发展：RAGvs长文本，谁更胜一筹？｜Z沙龙第8期</p>
            <br/>
          <ul class="meta">
            <li>
              By <a href=https://aibard123.com/about>AiBard123</a>
            </li>
            <li>
              <i class="fa fa-clock-o"></i>
              March 26, 2024 - 2 min read
            </li>
          </ul>

          <div class="_1NCGf">
              <img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/JPyOwicUdYCohc7CyQ0oFz3SHNAwibUXsA9OYGh8iaiaFCJapcG8jdY5fjJwGkSib8ALFic20S01FmIIoiaAEiartMR9tA/640?wx_fmt=png&amp;from=appmsg" width="640" >
          </div>
            <br>
            <br>
            <br>
          
          <div class="single-blog-content">
            <p>作者： 目标检测和深度学习  来源： <a href="https://mp.weixin.qq.com/s/N7SH2tp7ZILUG8W36CuwEw">目标检测和深度学习</a></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/JPyOwicUdYCo8hFuENwXvkwicJLVSEiaXfaltdwV1FCF8bbMj80W0E8dlp2TYs4AwhQWXaBsZ57lbbYtVMkf5ocJg/640?wx_fmt=other&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1&amp;tp=webp" alt=""></p>
<p><strong>「Z计划」</strong>  是智谱 AI 面向未上市初创企业与优秀独立开发者/团队，提供 Tokens 赞助、投资支持和技术支持等资源的创新加速计划。<a href="http://mp.weixin.qq.com/s?__biz=MzkyMDU5NzQ2Mg==&amp;mid=2247483667&amp;idx=1&amp;sn=b0bf8af76ea56c21ec294ed2a9d316a5&amp;chksm=c19127aaf6e6aebc1d23d1c8f9092818d99ea422c25e1a2eb6d18f4afc1283f7bae40750090d&amp;scene=21#wechat_redirect">面向全球，持续招募中！（点击报名）</a><strong>「Z沙龙」</strong> 是支持该计划的面向大模型领域的线下活动品牌。为鼓励自由发言，人人发言，我们暂时不披露参与者个人信息。本文不代表智谱公司认同文中任何观点。</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/JPyOwicUdYCqVDM3QcSIbibOkFU0jeNvOE6Hic0Enjic4caQzHricm5AwOOlh1Sy1icfIyr0hW3KaWy2obOcAKF2pfUA/640?wx_fmt=png&amp;from=appmsg" alt=""></p>
<p>编者按：当前，AIGC的迭代速度正以指数级的速度增长。2024 年 2 月，谷歌发布的 Gemini 1.5 Pro；</p>
<p>再次将上下文刷新为 100 万 token，创下了最长上下文窗口的纪录，相当于 1 小时的视频或者 70 万个单词。</p>
<p>由于 Gemini 在处理长上下文方面表现出色，甚至有人高喊“RAG 已死”。爱丁堡大学博士付尧表示：“一个拥有 1000 万 token 上下文窗口的大模型击败了 RAG。</p>
<p>大语言模型已经是非常强大的检索器，那么为什么还要花时间构建一个弱小的检索器，并将时间花在解决分块、嵌入和索引问题上呢？”</p>
<p><strong>随着模型上下文长度的提升，一个问题逐渐显现：RAG技术是否会被取代？</strong> 由此，我们在 3 月 9 日举办了 Z 沙龙第八期：<strong>Long-context &amp; RAG。</strong></p>
<p>我们邀请了产业界和学术界的朋友们共同碰撞思想，交流观点；他们分享了关于于 Long-context 和 RAG 的看法，并对 Context length 是否存在摩尔定律展开了精彩讨论。</p>
<p>同时，投资人与产业从业者也分享了 Long-context 及 RAG 如何赋能 AI 应用。对于研究人员与大模型从业者关心的点，罗列如下，方便大家结合目录使用：</p>
<p>*<strong>Long-context 将取代 RAG（2.2）</strong></p>
<p>*<strong>Context length 存在摩尔定律（3.1）</strong></p>
<p>*<strong>大海捞针是否是长文本测试唯一的标准（4.3）</strong></p>
<p>*<strong>Long-context 与 RAG 未来的发展趋势（5.5）</strong></p>
<p><strong>目录<strong><strong>建议结合要点进行针对性阅读</strong></strong>。👇</strong></p>
<p>**一、**<strong>长文本 &amp; RAG 发展近况</strong></p>
<p>1、长文本发展近况</p>
<p>2、RAG 发展近况</p>
<p>**二、**<strong>RAG vs 长文本，谁更胜一筹？</strong></p>
<p>1、观点一：RAG 与长文本各有所长</p>
<p>2、观点二：长文本将取代 RAG</p>
<p>3、观点三：RAG 和长文本分工已经明确，不存在争议空间</p>
<p>4、观点四：长文本和 RAG 需要结合</p>
<p>5、观点五：RAG 是大模型发展的中间态，短期内长文本无法替代 RAG</p>
<p>**三、**<strong>Context length是否存在摩尔定律？</strong></p>
<p>1、观点一：存在</p>
<p>2、观点二：不存在</p>
<p>3、观点三：不确定</p>
<p>**四、**<strong>模型层：大模型如何优化？如何有效对大模型测试？</strong></p>
<p>1、模型优化——优化数据质量</p>
<p>2、模型优化——节省计算资源</p>
<p>3、模型测试——大海捞针是否是唯一？</p>
<p>**五、**<strong>长文本及 RAG 在大模型场景落地时的角色</strong></p>
<p>1、投资人的看法</p>
<p>2、情感陪伴</p>
<p>3、教育产品</p>
<p>4、医疗领域‍</p>
<p>5、未来发展趋势</p>
<p><strong>#1.</strong></p>
<p><strong>长文本 &amp; RAG 发展近况</strong></p>
<p><strong>1、长文本发展近况</strong></p>
<p>随着大模型上下文窗口长度不断增加，各个厂商对于文本生成模型呈现出“军备竞赛”的态势。</p>
<p>目前，主流的文本生成模型是聊天模型，比如GPT、Claude 3 等，也有少部分 Base 模型，例如 Yi-34 开源模型。</p>
<p>两位技术研究人员分享了他们对于大模型的看法：</p>
<ul>
<li>
<p>用户使用最多的是 GPT，但对外开放的版本性能较差，用户交互端无法传输大文件，只能通过 API 接口上传。</p>
</li>
<li>
<p>月之暗面的 Kimi 模型大海捞针测试分数很高，但实际使用效果没有达到理想状态。</p>
</li>
<li>
<p>百川 192K 的闭源模型，对于 6 万字的长文本，其表现的推理能力和回答效果很优秀。</p>
</li>
<li>
<p>各种长文本的跑分数据，最高的是 Claude 3 模型。‍</p>
</li>
</ul>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/JPyOwicUdYCohc7CyQ0oFz3SHNAwibUXsAicUcntPquFgwUu0yI0GyazCjb4GTr4TkpRk7phleDIjM3p7ovBftT3w/640?wx_fmt=png&amp;from=appmsg" alt=""></p>
<p><strong>2、RAG 发展近况</strong></p>
<p>目前，大部分公司倾向于使用RAG 方法进行信息检索，因为相比长文本的使用成本，使用向量数据库的成本更低。</p>
<p>而在 RAG 应用过程中，一些公司会使用微调的 Embedding Model，以增强 RAG 的检索能力；而有些公司会选择使用知识图谱或者 ES 等非向量数据库的 RAG 方法。</p>
<p><strong>一个正常的模型使用 RAG 仍然是当前的主流选择。</strong></p>
<p>由于大语言模型信息的滞后性以及不包含业务知识的特点，我们经常需要外挂知识库来协助大模型解决一些问题。</p>
<p>在外挂知识库的过程中，Embedding 模型的召回效果直接影响大模型的回答效果，因此，在许多场景下，我们都需要微调 Embedding 模型来提高召回效果。</p>
<p>来自马里兰大学、劳伦斯利弗莫尔国家实验室、纽约大学的研究学者提出了一个大模型微调的方法；‍</p>
<p>在微调时只需要简单的在Embedding 层上加随机噪声即可大幅度提升微调模型的对话能力，而且也不会削弱模型的推理能力。</p>
<p>用 Alpaca 微调 LLaMA-2-7B 可以在在 AlpacaEval 上取得 29.79%的表现，而用加了噪声的嵌入则提高到 64.69%。不过该工作只在较小的模型上进行微调。</p>
<p>论文地址：</p>
<p><a href="https://arxiv.org/pdf/2310.05914.pdf">https://arxiv.org/pdf/2310.05914.pdf</a></p>
<p>代码链接：</p>
<p><a href="https://github.com/neelsjain/NEFTune">https://github.com/neelsjain/NEFTune</a></p>
<p><strong>#2.</strong></p>
<p><strong>RAG vs 长文本，谁更胜一筹？</strong></p>
<p><strong>1、观点一：RAG 与长文本各有所长</strong></p>
<p>人们普遍认为将文本切片，然后进行相应的检索是最节省资源的方式。但因为检索是速度检索，受到阈值的影响，可能要多次反复检索，反而会造成一些token 消耗的问题。</p>
<p>在多轮对话过程中，特别是在金融分析和客服场景，需要使用长文本来解决问题。如果进行切片处理，可能会丢失上下文之间的相互依赖关系。</p>
<p>对于大模型厂商，选择长文本或者 RAG 应该考虑<strong>哪种方式最节省 token。</strong></p>
<p><strong>一位投资人分享了一个项目：</strong> 国内有一个做代码生成工具的公司，相比仅仅生成代码，他们更注重软件工程。</p>
<p>因为GitHub 或 Copilot 生成代码分析和代码片段的能力已经很完美，国内真正需要解决的是能够围绕多个指标进行策略生成；</p>
<p>以操作系统为例，当我们想在操作系统中增加 AI 助手时，大模型不仅能实现底层部署，还能生成交互界面。</p>
<p>这种生成能力依赖于向模型输入的数据规模，可能涉及到的代码量会达到百万行、甚至千万行。如果仍然使用比较原始的一次性输入方式，可能会遇到很多问题。</p>
<p>对此，这位投资人分享了两个观点：</p>
<p>*<strong>长文本是一种智力能力。</strong> 拥有一个更好的上下文窗口，可以更好地解决代码的相互依赖和逻辑性问题。</p>
<p>‍如果只是用 RAG 方式去分段代码，然后再连接起来，再分段提问，是无法满足需求的。</p>
<p>*<strong>RAG更像是能力的边界。</strong> 如果只使用上下文窗口，而没有好好利用 RAG 基于检索的方式，很难解决同一个代码工程在多个模块，或者在多个功能上的问题。</p>
<p>‍只能解决比较局部的问题，无法处理多个模块之间的相互关联，例如进行联调测试，而合理使用 RAG 辅助可以拓展模型的知识边界。</p>
<p>编者按：</p>
<p>*<strong>长文本是一种智力能力：</strong> 从认知科学的角度看，人类处理长文本信息的能力是高级智力的体现。</p>
<p>阅读理解一本小说，写作一篇论文，都需要在大脑中维护一个宏大的上下文，同时进行逻辑推理、情节关联等复杂的认知活动。</p>
<p>这种能力区别于对简单句子或短语的机械处理。对语言模型而言，长文本建模能力意味着更强的抽象和归纳能力。</p>
<p>*<strong>RAG更像是能力的边界：</strong> RAG 通过检索相关片段来辅助生成，在一定程度上弥补了语言模型在长文本建模上的不足。</p>
<p>‍它提供了一种即时获取背景知识的机制，减轻了模型的记忆负担，但它并不能取代模型本身的语言理解和推理能力。</p>
<p>针对代码生成，研究人员分享了一个最新技术：<strong>Task Weaver</strong> 。Task Weaver 是微软的框架，用 GPT 的一个常规模型来完成。</p>
<p>本质是把一个复杂任务拆成很多小部分，然后再把每个小部分再去做 code intervention，中间用代码的形式来交互。</p>
<p>在每一个小部分里面，开始套各种套模板。这种用在长文本的话，可以解决掉内容丢失的问题。但是这个模型上下文不长，超过 8K 就结束了。</p>
<p>特别是它里面有个 Tools 叫 RAG，它占用上下文很大，每次调用 Tools，就会把 RAG 里面的东西全部抛进来，RAG 会作为一个 Tools 的 Observation 返回给 Agent。</p>
<p>之后，把整个 Agent 的结果成为下一个 RAG 的内容，在下一次 Agent 的时候再套，再把这个记录套回去。<strong>如果长文本技术的发展提升，Agent 上限可能会提高。</strong></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/JPyOwicUdYCohc7CyQ0oFz3SHNAwibUXsAZOQg4B1IBDMr2j5VEJISrwR9SYVDFOOn8GZ2oKWqjvPEVHed4VfYdA/640?wx_fmt=png&amp;from=appmsg" alt=""></p>
<p>TaskWeaver 是一款代码优先的 Agent 框架，能将用户的自然语言请求转化为可执行代码，并支持海量数据结构、动态插件选择以及专业领域适应的规划过程。</p>
<p>作为开源框架，TaskWeaver 充分发挥了大语言模型的潜力，通过可定制的示例和插件融入特定领域知识，让用户能够轻松打造个性化虚拟助手。</p>
<p>TaskWeaver 项目已在 GitHub 上开源，并于发布当日登上 GitHub 趋势榜。</p>
<p>论文地址：</p>
<p><a href="https://export.arxiv.org/abs/2311.17541">https://export.arxiv.org/abs/2311.17541</a></p>
<p>项目主页：</p>
<p><a href="https://microsoft.github.io/TaskWeaver/">https://microsoft.github.io/TaskWeaver/</a></p>
<p>项目地址：</p>
<p><a href="https://github.com/microsoft/TaskWeaver">https://github.com/microsoft/TaskWeaver</a></p>
<p><strong>2、观点二：长文本将取代 RAG</strong></p>
<p>参会嘉宾引用了付尧的观点，即<strong>长文本正在取代 RAG</strong> 。长文本相比于 RAG 在解码过程中检索具有明显的优越性。</p>
<p>爱丁堡大学博士付尧在评价Gemini 1.5 Pro 的帖子中写道：</p>
<p>“一个有 1000 万 token 上下文窗口的大模型击败了 RAG。LLM 已是强大的检索器，那为什么还构建一个弱小的检索器并花时间在解决分块、嵌入和索引问题上呢？”</p>
<p>他表示，1000 万 token 上下文杀死了 RAG。</p>
<p>Twitter地址：</p>
<p><a href="https://twitter.com/Francis_YAO_/status/1758934303655030929">https://twitter.com/Francis_YAO_/status/1758934303655030929</a></p>
<p>虽然当前上下文模型的计算成本很高，上下文窗口的消耗成本和时间消耗是非线性增长的，但有人认为未来可能会有更好的方式来<strong>重复利用缓存</strong> ，从而释放压力。</p>
<p>从 AI 的历史发展来看，现有模型的成本能降低 90%，RAG 可能会从现在的 50%的应用场景缩减到 10%。</p>
<p>编者按：在大规模语言模型中，重复利用缓存是一种优化策略，旨在提高模型的推理效率和速度。</p>
<p>它的基本思路是：将模型在处理长文本时生成的中间结果（如隐藏状态、注意力矩阵等）存储在缓存中；</p>
<p>当遇到相似的上下文时，直接从缓存中读取这些中间结果，而不是重新计算。比较常见的是Key-Value Cache、Hidden State Cache 等</p>
<p>对于长文本替代RAG，有人提出了一个很有意思的 idea：如果有一个无限长的上下文模型，直接将 wiki 里面所有的文本和相关信息全部输入，然后再去问问题。</p>
<p>实际上就相当于大模型直接做 RAG，不需要有任何外部的知识库，再去进行上游检索。模型的<strong>推理成本</strong> 是个门槛，即模型输入的信息越多，模型推理的时间越长，成本越高。</p>
<p>但依旧存在可行的解决方案，即信息压缩：交给 RAG 或在线数据库处理的信息，本质上是可以被压缩的。</p>
<p>比如检查 GitHub 里的 Star 数量，或者 wiki 上的访问量，贡献的数量等，都是可以被压缩的，进而转化为结构化的信息。</p>
<p>但此方法的前提条件是，需要找出哪些数据真的可以被压缩，并且它的压缩失真情况在接受的范围内。</p>
<p><strong>3、观点三：RAG 和长文本分工已经明确，不存在争议空间</strong></p>
<p><strong>对于一些严肃的场景中，如法规条文、保险或教育等，RAG可以更好解决的问题。</strong> 在进行向量化的初期，开发者设计的就是认为里面的内容是法定正确的；</p>
<p>或者至少为大模型提供向量数据库时，我们认为这些是客观事实，不应该对这些事实进行歪曲或改变。</p>
<p>如果将其交给大模型的幻觉或者概率去判断，实际上可能会出现问题。如果完全依赖长文本，结果一定是不准确的。</p>
<p><strong>对于多轮对话的场景，RAG能解决的问题并不是很清晰。</strong> 如客服场景，很多大模型会出现与它对话的时候，会做一些后端的成本精简，不需要动用全部算力来解答一个问题。</p>
<p>如果反复去确认，要给一个真实答案，这个时候只能交给长文本去解决这个问题，而 RAG 只是去把它向量化。</p>
<p>此外，对于<strong>软件工程领域，</strong> 涉及到代码的补全、翻译或重构时，输入 token 会非常大，只交给滑动窗口去处理，会存在理解的障碍。</p>
<p>编者按：</p>
<p>Devin是全球首个 AI 软件工程师，由公司 Cognition 推出。它有全栈技能，包括开发工具集，如 shell、代码编辑器、沙箱浏览器等，并能用它们来高效编程。</p>
<p>Devin 在经过长期的推理训练后，能够规划并完成复杂的任务，包括构建和部署应用程序、自主查找并修复 Bug、训练和微调自己的 AI 模型等。</p>
<p>官方网址：</p>
<p><a href="https://www.cognition-labs.com">https://www.cognition-labs.com</a></p>
<p><strong>4、观点四：长文本和 RAG 需要结合</strong></p>
<p><strong>RAG的特点是准确、事实性和时效性。</strong> 用 RAG 的方式，可以将原有系统的元素变成多维标签，甚至将系统本身做成一个端到端的向量，或是一个标签化的端到端的实体，以防信息损失。</p>
<p>但如果只用 RAG 的方法去做模型，可能在多轮对话后，它就不知道说什么了。<strong>长上下文在解决问题时，是一个泛化和上下文理解的过程，避免信息丢失。</strong></p>
<p>长文本和RAG 都比较依赖于上游检索的输出。如果大模型对上下文的容纳程度比较低，那对检索的要求就更高，必须把最重要的信息检索出来。</p>
<p>但是，如果大模型可以接受更多的上下文，那么对检索的要求就相对降低，而对数据准备的要求就会相对提高。</p>
<p>对于大模型厂商来说，无论是做大模型基座还是其他，未来最终都是要转向消费端。只有当消费端起来之后，大模型才可能有一个大的爆发。</p>
<p><strong>从消费端来看，一般考虑的是成本性能、泛化能力以及信息丢失。</strong> 在消费端应用的场景下，最终是希望成本越来越低，性能越来越快，泛化能力越来越强。</p>
<ul>
<li>
<p>如果不能接受信息损失，需要在系统里面投入更高的 RAG 成本。</p>
</li>
<li>
<p>如果只是进行角色扮演，或者是给出一个笼统的回答，那么长文本比较合适。</p>
</li>
</ul>
<p>长文本和RAG 的结合更像是一种趋势，在输入大模型之前，我们不仅可以通过向量库去做文本检索；</p>
<p>还可以通过一些 function 去获取更多的文本来做集中的召回，通过大模型做能力整合再做 RAG。长上下文能够代表所有情况，但 RAG 系统仍然会存在。</p>
<p>以大模型基座为例，我们觉得它最终在市场上的竞争方向有两个：</p>
<ul>
<li>
<p>长文本</p>
</li>
<li>
<p>性能越来越好，可以远程部署</p>
</li>
</ul>
<p><strong>5、观点五：RAG 是大模型发展的中间态，短期内长文本无法替代 RAG</strong></p>
<p>无论是传统还是新架构，不断扩大模型的处理长度后，其性能必然会有所损失。目前的大模型而言，可能较合适的处理窗口是 4K 到 8K，因为预训练是在这个长度范围内。</p>
<p>RAG 相当于我们把模型的存储扩展到了无限，我们要做的是把有用的、最重要的信息给大模型。</p>
<p>因此，RAG 一定是很重要的，只不过它未来可能会有多种形态，不一定是现在这种大模型和向量检索分开的形态，它的形态可能会有所不同。</p>
<p>但是，这种通过一些方法提前对信息进行精炼和提取的思想，一定会在大模型的发展中长期发挥重要的作用。</p>
<p>长文本处理和RAG 这两个技术会共同发展。对长文本处理已经有一些优化的方法。比如，通过微调的方法把训练的参数量已经提升到了十亿或者是百亿；</p>
<p>在推理上的话，减少长文本的处理开销也有一些优化方法，比如 MIT 的韩松实验室有一个<strong>Streaming LLM</strong> 的方法，可以识别出长文本中哪些是重点的 Context 或者 Token；‍</p>
<p>然后保留这些部分和最近的一些信息，可以进行推理长度的优化，从而降低推理的成本。</p>
<p>除了长文本处理在不断进步之外，RAG 最近也有很多新的技术，未来可能会结合 agent，在其他方面提高模型解决具体实际问题的能力。</p>
<p>来自MIT、Meta AI、CMU 的研究者提出了一种名为 Streaming LLM 的方法，使语言模型能够流畅地处理无穷无尽的文本。</p>
<p>使用 StreamingLLM，包括 Llama-2- 7/13/70B、MPT- 7/30B 在内的模型可以可靠地模拟 400 万个 token，甚至更多。</p>
<p>与唯一可行的 baseline——重新计算滑动窗口相比，StreamingLLM 的速度提高了 22.2 倍，而没有损耗性能。</p>
<p>论文地址：</p>
<p><a href="https://arxiv.org/abs/2309.17453">https://arxiv.org/abs/2309.17453</a></p>
<p>论文代码：</p>
<p><a href="https://github.com/mit-han-lab/streaming-llm">https://github.com/mit-han-lab/streaming-llm</a></p>
<p>以目前的推理成本来看，RAG必不可少，可能会隐藏在产品里。比如说网易的<strong>逆水寒</strong> ，它里面做了很多 AI 的具体应用，比如 NPC 对话。</p>
<p>MiniMax 的模型有一个功能叫做<strong>Glyph</strong> ，它可以去控制模型输出的结果，可以标准化它的格式，对于很多场景来说，它的推理是非常有帮助的。</p>
<p><strong>逆水寒：</strong></p>
<ul>
<li>《逆水寒》手游中的智能 NPC 系统，是利用网易伏羲 AI 技术，实装了国内首个游戏 GPT。</li>
</ul>
<p>这是一种基于深度学习的自然语言生成模型，可以根据上下文和输入，生成合理的文本输出。</p>
<p>在游戏中，这意味着 NPC 不再是固定的对话框和任务分配者，而是可以与玩家自由对话，并且基于对话内容，自主给出有逻辑的行为反馈。</p>
<p><strong>MiniMax</strong></p>
<ul>
<li>限制返回格式（glyph）：该功能可以帮助用户强制要求模型按照配置的固定格式返回内容。</li>
</ul>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/JPyOwicUdYCohc7CyQ0oFz3SHNAwibUXsAyT1sAf7crVuFl6Q5oVutTS7j8Y9qo1XlN4NpSShvKN009O9biabQeiaA/640?wx_fmt=png&amp;from=appmsg" alt=""></p>
<p>逆水寒AI NPC</p>
<p><strong>#3.</strong></p>
<p><strong>Context length 是否存在摩尔定律？</strong></p>
<p><strong>1、观点一：存在</strong></p>
<p>目前，Context length正在持续增长，并且其增长速度远超摩尔定律。如果按照 18 个月翻倍的标准来计算，从之前的几百万、几千万，到现在达到十兆；</p>
<p>Context length 在一年内的变化就已经远远超过翻倍。这种增长速度本身就已经打破了摩尔定律所描述的增长曲线。</p>
<p>随着Context length 的增长，算力将成为一个瓶颈。当所有的推理和训练任务都转移到处理 Context 时，我们会发现仍然需要大量的能源。</p>
<p>以前可能只需要一张 A100 显卡，而现在可能需要一整台 A100 服务器才能完成任务。从产业界的角度来看，无论是算力还是能源，都会限制其增长速度。</p>
<p>因此，在考虑 Context 增长的同时，<strong>还需要考虑到成本和资源限制的问题</strong> 。</p>
<p>近日，<strong>Kimi 智能助手在长上下文窗口技术上再次取得突破，无损上下文长度提升了一个数量级到 200 万字。</strong></p>
<p>从模型预训练到对齐、推理环节月之暗面均有原生的重新设计和开发。月之暗面认为，大模型无损上下文长度的数量级提升，也会扩大对 AI 应用场景的想象力；</p>
<p>包括完整代码库的分析理解、可以自主帮人类完成多步骤复杂任务的智能体 Agent、不会遗忘关键信息的终身助理、真正统一架构的多模态模型等等。</p>
<p>月之暗面创始人杨植麟表示，“上下文长度可能存在摩尔定律，但需要同时优化长度和无损压缩水平两个指标，才是有意义的规模化。”</p>
<p>新闻地址：</p>
<p><a href="https://mp.weixin.qq.com/s?__biz=Mzk0NDU1MDkyNg==&amp;mid=2247483934&amp;idx=1&amp;sn=d1486460932ab2106e65347c506eb619&amp;scene=21#wechat_redirect">‍https://mp.weixin.qq.com/s/UjXKic9IAaf55ARqbnJ3Pg</a></p>
<p><strong>2、观点二：不存在</strong></p>
<p>Context的增长是包含了各个单元之间的逻辑关系，其复杂度的增长会高于计算能力的增长。</p>
<p>而且，现在大模型还是有非常多问题的，即使是顶尖的大型模型，在应用于工业产品时，也需要将需求范围缩小到非常具体的领域。</p>
<p>当需求被高度收敛时，相应的用户需求也会减小，这可能导致一种螺旋下降的趋势：投资减少，进一步导致研究和开发的动力减弱。</p>
<p>编者按：</p>
<p>*<strong>应用价值的不确定性：</strong> Context length 的增加能带来多大的应用价值提升，还缺乏足够的实证支撑。</p>
<p>一些研究表明，过长的 Context 可能引入噪音，对模型性能的提升效果并不明显。如果投入产出不成正比，继续增加 Context length 的动力会减弱。</p>
<p>*<strong>数据质量的瓶颈制约：</strong> 高质量的长文本数据是Context length 增长的基础，但现有的数据质量普遍不高，噪音、错误、不一致等问题严重。</p>
<p>数据瓶颈可能成为 Context length 增长的羁绊，单纯增加 Context length 而不解决数据问题，效果可能适得其反。</p>
<p><strong>3、观点三：不确定</strong></p>
<p>摩尔定律是基于一段时间技术积累后观察到的规律，需要大量的资本投入和成本控制来驱动。</p>
<p>对于大模型和RAG 这类技术，业界目前可能还处于探索阶段，从时间窗口来看还非常短暂，仅仅一两年的时间，并且没有大规模投入到特定场景中应用；</p>
<p>因此还没有足够的数据来进行经验总结。从这个角度来看，与晶体管发展的摩尔定律相比，<strong>Context length 的增长规律还不够成熟。</strong></p>
<p><strong>#4.</strong></p>
<p><strong>模型层：大模型如何优化？如何有效对大模型测试？</strong></p>
<p><strong>1、模型优化——优化数据质量</strong></p>
<p>在训练模型的时候，数据量并不是越大越好。<strong>真正重要的是训练数据的质量，而不仅仅是数量。</strong> 使用 RAG 进行搜索的过程中，当数据量大了以后，它匹配出来的结果可能会有很多冗余。</p>
<p>比如，我们去搜索一个新的领域，不知道哪些文章是最好的，如果搜索出了 100 篇，不可能让模型全部去处理，需要加一些权重；</p>
<p>比如，文章的影响因子，或者是它的引用率、引用次数等，把这些因素考虑进去，然后对结果进行排序。</p>
<p>但这涉及到一些问题，有些优秀的文章并不一定引用率很高，特别是在一些特定的领域，它们可能引用的文章也相对较少。针对此问题，研究人员提出了一些想法：</p>
<p>*<strong>学科的交叉会使得大模型效果更好。</strong> 对于学科交叉的问题，最好的解决办法既不是依赖于长文本处理，也不是 RAG，而是微调。</p>
<p>‍在训练模型的过程中，我们需要考虑如何控制在各个大的领域里进行搜索。我们现在面临的是海量的文献，不可能把所有的数据都加进去，还需要人工智能来辅助。</p>
<p>‍现在面临的一个挑战，不仅要深度学习，还要广度学习，而且还要控制好搜索的范围，否则成本就会急剧上升。</p>
<ul>
<li>我们的平台每天都有大量的科研数据，包括用户的行为数据和点赞数据等，这些数据对我们来说非常有用，当我们将这些数据纳入训练时，效果就非常明显。</li>
</ul>
<p>‍所以现在的挑战是如何检索出大量的文本，并从中筛选出真正有价值的信息，将其他的信息过滤掉，然后再将这些信息放入模型中。</p>
<ul>
<li>在应用层面，包括成本和产品质量，问题的核心在于是否需要数据的可靠性。如果要可靠的数据，就要使用Agent。如果数据可以压缩或者有损，需要考虑其他的方法。</li>
</ul>
<p><strong>2、模型优化——节省计算资源</strong></p>
<p>现在大部分模型，即使是长文本模型，在反向传播阶段，从第一步到最后一步文本窗口不可能一直保持很长。</p>
<p>一定是在最后的时候去解决这个问题，以节约计算资源。在科研中，我们接触到的预训练阶段的长度是4k 或 8k。</p>
<p>学术界也有人提出，我们应该尽量让一个窗口内的数据尽可能相似，即在一个窗口或者一个数据条中，数据应该是相似的主题或内容。从论文来看，这可能对预训练有好处。</p>
<p><strong>3、模型测试——大海捞针是否是唯一？</strong></p>
<p>目前主流测试还是靠大海捞针，现在有一些新的测试，提出了一个<strong>更加复杂的大海捞针 Benchmark。</strong></p>
<p><strong>从产品侧，需要看受众端的用户。</strong> 来自教育产品的从业者分享其观点：我们去试过把哈利波特做成一个鲜活的角色，帮助用户了解哈利波特内容。</p>
<p>但家长对于内容的真实性和准确性要求是很高的，我们的产品无法达到他们的要求，所以这个方案就暂时搁置了。</p>
<p>对于非家长的产品，用户直接面向小孩，这种精确度就比较适合孩子体验。所以，从应用侧讲，测试大模型需要考虑受众端的内容。</p>
<p>来自情感陪伴的从业者分享观点：我们较关注用户的使用时长、满意度、分页系数等，对于不同的模型，我们直接进行AB 测试，哪个测试高，我们就会选择这个模型。</p>
<p>编者按：</p>
<p>目前大海捞针广泛用于长文本测试，这种方法并不完全合理，尤其是对于需要检索多个事实并在此基础上进行推理的应用。</p>
<p>对此，研究人员提出了<strong>多针检索加推理测试</strong> ，通过扩展 Greg Kamradt 的“LLMTest_NeedleInAHaystack”项目，以支持多针评估，评估工具使用了 LangSmith 。</p>
<p>项目地址：</p>
<p><a href="https://blog.langchain.dev/multi-needle-in-a-haystack/">https://blog.langchain.dev/multi-needle-in-a-haystack/</a></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/JPyOwicUdYCohc7CyQ0oFz3SHNAwibUXsA9OYGh8iaiaFCJapcG8jdY5fjJwGkSib8ALFic20S01FmIIoiaAEiartMR9tA/640?wx_fmt=png&amp;from=appmsg" alt=""></p>
<p><strong>#5.</strong></p>
<p><strong>长文本及 RAG 在大模型场景落地时的角色</strong></p>
<p><strong>1、投资人的看法</strong></p>
<p>*<strong>投资人目前关注内存的增长。</strong> 内存的增大使任务或应用有了更丰富的展现，从以前玩的简单游戏，到现在复杂的 3A 大作，上下文窗口的提升肯定能提升整个应用的能力。</p>
<p>*<strong>RAG的外挂知识库可能是很重要的资产。</strong> 有些人会把 RAG 或者留存下来的外挂知识库看作是没有长期价值的资产。</p>
<p>‍有些人认为 RAG 里面会留存下来一些有价值的东西，例如，对于某些客户或某一类行业的客户，会在库里面封装一些客户业务逻辑的知识。</p>
<p>‍将来去服务这一类客户，或者满足这个客户的长期需求的过程中，无论用哪个模型，这个模型是无法知道这些私密、个人化的信息或路径的。</p>
<p>‍这一部分对于公司将来能持续在这一类行业里面交付能力，是有长期价值的。投资人会评估哪种行业能够留存下这方面的东西。</p>
<p>‍比如代码生成能力，在不断地积累人和代码生成的监督过程中，RAG 里面留存下来的信息可以持续帮助到模型。</p>
<p><strong>2、情感陪伴</strong></p>
<p>一位情感陪伴行业的从业者分享了他的观点：<strong>我们认为RAG 是对 Long-Context 的补充，特别是对外部知识的补充。</strong></p>
<p>如果没有 RAG，每次都需要将知识输入到 Context 中，但 Context 的长度有限，而且 Token 的使用也要成本。因此，RAG 可以使 Context 的内容更丰富，同时节省成本。</p>
<p>在情感陪伴方面，为了让人物更加细腻，我们通常会使用prompt 来解决问题。在面向消费者的应用层面，将 Context 和 RAG 结合在一起是每个人在情感上最需要的。</p>
<p>对于情感陪伴来说，回忆是非常重要的。如果能让 Context 和 RAG 结合，直接作为大脑使用，那就达到了目的。</p>
<p>图片和其他角度可以增加想象力，就像微信可以发送图片、视频、语音和进行语音电话一样。这些功能对于微信的发展非常重要。</p>
<p>对于情感应用来说，如果你可以发送图片，然后你的朋友圈下面有人可以回复，这将为用户提供很大的情绪价值。目前，Agent 聊天仍然能够明显感觉到对方不是真人。</p>
<p><strong>3、教育产品</strong></p>
<p>一位教育产品领域的从业者分享了他的见解：<strong>在教育产品中，我们需要打通孩子不同年龄段的信息，以提供更有逻辑性的服务。</strong></p>
<p>比如，学龄前的一个产品，它的登录是通过家长的手机端的APP，就是他的微信和手机号。目前我们只能通过标签的方式把这件事给连接起来，但这种方式是比较低效的。</p>
<p>会场上一位专家提供了解决思路：可以采用特定的Agent，比如 Read Agent，来处理这个问题。</p>
<p>他建议将 3-6 岁和 7-12 岁儿童的信息分别存储在两个数据库中，并使用大型模型对 3-6 岁儿童的信息进行总结，然后在每次需要读取时将其放入第二个数据库。</p>
<p>这种方法的核心是利用数据压缩技术，以提高处理效率。</p>
<p>Read Agent 是由 Google DeepMind 开发的一个类似人类阅读的 LLM 智能体系统，它能将有效上下文长度扩大 3-20 倍，同时取得更高的准确率和 ROUGE 得分。</p>
<p>Read Agent 系统通过三个主要步骤实现：</p>
<ul>
<li>
<p>分割成片段，根据 LLM 的提示决定在连续文本中的何处暂停，形成片段；</p>
</li>
<li>
<p>摘要记忆，将每个片段压缩成更短的摘要，关联上下文信息；</p>
</li>
<li>
<p>交互查找，在给定任务和完整的摘要记忆中，决定查找哪些片段，将摘要与原始文本结合，解决任务。</p>
</li>
</ul>
<p>ReadAgent 系统可以通过提示经过训练的 LLM 来实现。</p>
<p>论文链接：</p>
<p><a href="https://arxiv.org/abs/2402.09727">https://arxiv.org/abs/2402.09727</a></p>
<p>项目链接：</p>
<p><a href="https://read-agent.github.io/">https://read-agent.github.io/</a></p>
<p><strong>4、医疗领域</strong></p>
<p>在医疗领域，大模型在理解文本和图像方面表现出色，<strong>但它们在Mapping 上存在不足，传统的 RAG 和 Embedding model 可能效果不佳。</strong></p>
<p>与医疗公司建立合作关系成为一种有效的解决策略。通过合作，让医疗公司在 Embedding 的过程贡献他们的算法；</p>
<p>包括他们对病例的诊断，将这些信息加到 Embedding 的工具库里，这些数据的向量数大致在百万到千万之间。同时，为保证技术真正应用，需找到有实际付费能力的客户。</p>
<p>有研究人员发现，引入了In context learning，可以显著提升了效果。以 COVID-19 的 X 光诊断为例，我们可以先向模型展示一些样本，包括阴性和阳性病例。</p>
<p>先给模型看一张阳性病例的图片，然后是阴性病例的。接下来，当模型再次看到新图片并询问其是阳性或阴性时，通过学习，判断效果会比无预先学习的情况下好很多。</p>
<p>相比于那些已经通过人工标注训练的模型，<strong>如果能够实现 CNN 方法，它可能会比使用 RAG 方法更加经济高效。</strong></p>
<p><strong>5、未来发展趋势</strong></p>
<p>随着视频和图像时代的到来，信息传递的方式将发生显著变化，这时传统的文本编码和解码方式将不再适用。</p>
<p>在这个新时代，&ldquo;Token&quot;不再仅仅代表一个文字，而是可能代表更复杂的信息单元，因此传统的NLP 方法将不足以处理计算机视觉领域的问题。</p>
<p>在算力方面，一些公司下一代的计算芯片放弃GPU 架构，自己有一套硬件架构做深度学习，而且性能更高，耗电量会更少。</p>
<p>从 2014 年至今，谷歌已经构建了 6 种不同的 TPU 芯片。虽然单体性能仍然与 H100 差距明显，但 TPU 更贴合谷歌自己生态内的系统。</p>
<p>这也促使 Gemini 的内容生成速度非常快的，虽然精度没有那么高，但生成速度远超 GPT 和 Claude。<strong>下图以 Gemini pro 和 Claude 3-Haik 代码生成速度为例。</strong></p>
<p>Gemini pro 代码生成速度示例</p>
<p>Claude 3-Haik 代码生成速度示例</p>
<p>在谷歌发布 Gemini 大模型的同时，DeepMind 团队还写了 60 页技术报告阐述 Gemini 多模态的技术原理，报告提到谷歌用 TPU v5e 和 TPU v4 来训练 Gemini。</p>
<p>当日，谷歌还发布了 TPU v5p，称训练速度比前代快 2.8 倍，有望帮助开发者和企业客户更快地训练大规模生成式 AI 模型。</p>
<p>训练大模型需要大量的计算能力，因为它们通常在包含数十亿个单词的数据集上进行训练。</p>
<p>传统的CPU 和 GPU 架构难以处理这种计算负载，通常会减慢训练过程并限制大模型的功能。Google TPU 专门针对矩阵乘法和二维卷积进行了优化。</p>
<p>据谷歌的解析 TPU v4 论文，<strong>相较用英伟达 A100 构建的超级计算机，用谷歌 TPUv4 建的超级计算机速度快 1.2-1.7 倍，功耗降低 1.3-1.9 倍。</strong> 目前，谷歌超过 90% 的 AI 训练都在 TPU 上。</p>
<p>技术报告地址：</p>
<p><a href="https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf">https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf</a></p>
<p>‍‍ ‍ ‍<img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_gif/Y7Vuiauiafia7JMFLQB4StwVPkyHqPpuneUlqLaxiaEtGmd7MCzd3pMW409lsiaujWvQ8ic0S9fMRrwRH0cQBYZwuCBg/640?wx_fmt=gif&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1&amp;tp=webp" alt=""></p>
<p><strong>#合作伙伴招募</strong></p>
<p>🐋 回顾：<a href="https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzkyMDU5NzQ2Mg==&amp;action=getalbum&amp;album_id=3193804089940492293#wechat_redirect">往期所有 Z 沙龙活动纪要合集</a></p>
<p>Z 计划发布至今积累了 700 + 早期优质大模型创业项目。<strong>Z 计划面向大模型创业投资领域的行业伙伴，开放产业生态资源</strong>**。** 如果你是关注大模型领域的<strong>VC 投资机构、大模型方向开发者社区</strong> 等，欢迎直接找我们聊聊！🙋 微信联系：zhipu_eco</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/JPyOwicUdYCojwicEuVuiaOyI3ibSibHC2O7gWQiczMFtzU7Hapo2AyanCuibdMzYPKVgBp4ibxuj3RoOwTwdZkRx85iayw/640?wx_fmt=png&amp;from=appmsg" alt=""></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/JPyOwicUdYCozNaPOB45B1PtVkQHZ8P4bUE9HGlw07ich1CCIZycxv8z4NUchETUOIEj4EZtdXuBZ1RQXxp9OBWg/640?wx_fmt=png&amp;from=appmsg" alt=""></p>
<p>—end—</p>
<p>作者 | Z 沙龙 25 位参会者</p>
<p>主持｜严宽、zR</p>
<p>整理｜崔浩、Yizheng、严宽、郑寒、汪柯璇</p>
<p>排版｜郑寒</p>
<p>审核｜李文珏、邓瑞恒‍‍‍‍</p>
<p>*本文不代表智谱认同以上任何观点。</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/JPyOwicUdYCo6tRws1nkYSsLdaH09aaphhgwfRBHoG5cYRTRkaSWvibGJTlelo2gWQaAUdwvibmoDPKKB5cdD5msA/640?wx_fmt=png&amp;from=appmsg" alt=""></p>
<p>更多AI工具，参考<a href="https://aibard123.com/">Github-AiBard123</a>，<a href="https://aibard123.com/">国内AiBard123</a></p>



          </div>

可关注我们的公众号：每天AI新工具

<p><img src="/images/aitools/2024/03/qrcode_for_gh_dde1b429630d_258.jpg" alt=""></p>

        </article>

      </div>
    </div>
  </div>
</section>
        </div>
    </div>
    </main>




<script type='text/javascript' src='/assets/js/jquery.ui.touch-punch.min-0.2.2.js' id='jqueryui-touch-js'></script>
<script type='text/javascript' src='/assets/js/clipboard.min-5.6.2.js' id='clipboard-js'></script>
<script type='text/javascript' src='/assets/js/tooltip-extend.js' id='iplaycode-nav-js'></script>
<script type='text/javascript' id='popper-js-extra'>
 

var theme = {"ajaxurl":"","addico":"https:\/\/nav.baidu.cn\/wp-content\/themes\/onenav\/images\/add.png","order":"asc","formpostion":"top","defaultclass":"io-grey-mode","isCustomize":"1","icourl":"","icopng":".png","urlformat":"1","customizemax":"10","newWindow":"0","lazyload":"1","minNav":"1","loading":"1","hotWords":"baidu","classColumns":" col-sm-6 col-md-4 col-xl-5a col-xxl-6a ","apikey":"TWpBeU1UVTNOekk1TWpVMEIvZ1M2bFVIQllUMmxsV1dZelkxQTVPVzB3UW04eldGQmxhM3BNWW14bVNtWk4="};
 
</script>
<script type='text/javascript' src='/assets/js/popper.min.js' id='popper-js'></script>
<script type='text/javascript' src='/assets/js/bootstrap.min-4.3.1.js' id='bootstrap-js'></script>
<script type='text/javascript' src='/assets/js/theia-sticky-sidebar-1.5.0.js' id='sidebar-js'></script>
<script type='text/javascript' src='/assets/js/lazyload.min-12.4.0.js' id='lazyload-js'></script>
<script type='text/javascript' src='/assets/js/fancybox.min-3.5.7.js' id='lightbox-js-js'></script>

<script type='text/javascript' src='/assets/js/app-anim.js' id='appanim-js'></script>

<script type="text/javascript">
    $(document).ready(function(){
        var siteWelcome = $('#loading');
        siteWelcome.addClass('close');
        setTimeout(function() {
            siteWelcome.remove();
        }, 600);
    });
</script>
<script>        
    $(document).ready(function(){
        setTimeout(function () {
            if ($('a.smooth[href="' + window.location.hash + '"]')[0]) {
                $('a.smooth[href="' + window.location.hash + '"]').click();
            }else if (window.location.hash != '') {
                $("html, body").animate({
                    scrollTop: $(window.location.hash).offset().top - 90
                }, {
                    duration: 500,
                    easing: "swing"
                });
            }
        }, 300);
        $(document).on('click','a.smooth',function(ev) {
            if($('#sidebar').hasClass('show') && !$(this).hasClass('change-href')){
                $('#sidebar').modal('toggle');
            }
            if($(this).attr("href").substr(0, 1) == "#"){
                $("html, body").animate({
                    scrollTop: $($(this).attr("href")).offset().top - 90
                }, {
                    duration: 500,
                    easing: "swing"
                });
            }
            if($(this).hasClass('go-search-btn')){
                $('#search-text').focus();
            }
            if(!$(this).hasClass('change-href')){
                var menu =  $("a"+$(this).attr("href"));
                menu.click();
                toTarget(menu.parent().parent(),true,true);
            }
        });
        $(document).on('click','a.tab-noajax',function(ev) {
            var url = $(this).data('link');
            if(url)
                $(this).parents('.d-flex.flex-fill.flex-tab').children('.btn-move.tab-move').show().attr('href', url);
            else
                $(this).parents('.d-flex.flex-fill.flex-tab').children('.btn-move.tab-move').hide();
        });
        
    });
</script>

<script>

(function(){
    if(document.cookie.replace(/(?:(?:^|.*;\s*)night\s*\=\s*([^;]*).*$)|^.*$/, "$1") === ''){
        if(new Date().getHours() > 22 || new Date().getHours() < 6){
            document.body.classList.remove('io-black-mode');
            document.body.classList.add('io-grey-mode');
            document.cookie = "night=1;path=/";
            console.log('夜间模式开启');
        }else{
            document.body.classList.remove('night');
            document.cookie = "night=0;path=/";
            console.log('夜间模式关闭');
        }
    }else{
        var night = document.cookie.replace(/(?:(?:^|.*;\s*)night\s*\=\s*([^;]*).*$)|^.*$/, "$1") || '0';
        if(night == '0'){
            document.body.classList.remove('night');
        }else if(night == '1'){
            document.body.classList.add('night');
        }
    }
})();

$("#search-bg").css("background", "linear-gradient(#e2c4c4, #d8d8d8)");   
function switchNightMode(){
    var night = document.cookie.replace(/(?:(?:^|.*;\s*)night\s*\=\s*([^;]*).*$)|^.*$/, "$1") || '0';
    if(night == '0'){
	$("#search-bg").css("background", "linear-gradient(#e2c4c4, #d8d8d8)");
        document.body.classList.remove('io-grey-mode');
        document.body.classList.add('io-black-mode');
        document.cookie = "night=1;path=/"
        console.log(' ');
        $(".switch-dark-mode").attr("data-original-title","日间模式");
        $(".mode-ico").removeClass("icon-night");
        $(".mode-ico").addClass("icon-light");
    }else{
	$("#search-bg").css("background", "linear-gradient(#4f4040, #1b1d1f)");
        document.body.classList.remove('io-black-mode');
        document.body.classList.add('io-grey-mode');
        document.cookie = "night=0;path=/"
        console.log(' ');
        $(".switch-dark-mode").attr("data-original-title","夜间模式");
        $(".mode-ico").removeClass("icon-light");
        $(".mode-ico").addClass("icon-night");
    }
}
</script>


<script>
    var newsContainer = document.getElementById('news-container');
    var newsItems = document.getElementsByClassName('news-item');
    var currentItem = 0;

    setInterval(function() {
        
        newsItems[currentItem].classList.remove('show');
        newsItems[currentItem].style.transform = 'translateY(-20px)';
        
        currentItem = (currentItem + 1) % newsItems.length;
        newsItems[currentItem].style.transform = 'translateY(' + (newsContainer.offsetHeight - 20) + 'px)';
        setTimeout(function() {
            newsItems[currentItem].classList.add('show');
        }, 500);
    }, 8000);
</script>

</body>
</html>





<!DOCTYPE html>
<html lang="zh-CN">

<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1" />
    <meta name="viewport"
        content="width=device-width, initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no" />
    <meta name="theme-color" content="#f9f9f9" />

	<title>社区供稿Mixtral-8x7BPytorch实现 作者： Hugging Face 来源： Hugging Face 0.前言 本文从代码角度来谈下 Mixtral 8x7B 混合专家Pytorch 的实现 1.论文概述 Mixtral-8x7B 引爆了MoE 的技术方向，更多针对MoE 优化的Trick 出现，回归模型本身来解析： Mixtral 8x7B 采用了sMoE 模型结构，模型的细节如何？路由负载均衡如何计算？  | AiBard123| ai工具网址导航,ai最新产品</title>
	<link rel="shortcut icon" href="/assets/images/favicon.png" />
    <meta name="keywords" content="chatgpt,AI,AI聊天,AI文本生成,AI绘画,AI编程,AI电商" />
    <meta name="description" content="AiBard123 网址导航 | 免费chatgpt 汇集各类先进的人工智能产品，旨在帮助用户更快速地了解和使用这些产品,轻松地浏览不同领域的AI产品，包括语音识别、图像处理、自然语言处理。" />
    
    <meta name="baidu-site-verification" content="codeva-cCAOSG8MBO" />
    
    <link rel="stylesheet" id="block-library-css"
        href="/assets/css/block-library.min-5.6.2.css" type="text/css" media="all" />
    <link rel="stylesheet" id="iconfont-css" href="/assets/css/iconfont-3.03029.1.css"
        type="text/css" media="all" />

    
    <link href="/scss/style.min.css" rel="stylesheet" />
    
		    <link rel="stylesheet" id="iowen-css" href="/assets/css/style-3.03029.1.css"
        type="text/css" media="all" />
    <link rel="stylesheet" id="custom-css" href="/assets/css/custom-style.css"
        type="text/css" media="all" />
		
		<link rel="stylesheet" href=/plugins/font-awesome/css/font-awesome.min.css />


    <link rel="stylesheet" id="fortawesome-css" href="/assets/fontawesome-5.15.4/css/all.min.css" type="text/css" />


    <script type="text/javascript" src="/assets/js/jquery.min-3.2.1.js" id="jquery-js"></script>
    <script type="text/javascript" src="/assets/js/content-search.js"  id="content-search-js"></script>

    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2073588164294660"
     crossorigin="anonymous"></script>

	
    <script>
        

		var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?8450bc732b2a86f7e4aec4ebd9fd8252";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();

        
    </script>
    

    
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-7071W80M2K"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-7071W80M2K');
    </script>

</head>


    <div class="page-container">
	
	<div id="sidebar" class="sticky sidebar-nav fade animate-nav" style="width: 170px">
        
            <div class="modal-dialog h-100 sidebar-nav-inner">
                <div class="sidebar-logo border-bottom border-color">
                    
                    <div class="logo overflow-hidden">
                        <a href="https://aibard123.com/" class="logo-expanded">
                            <img src="/assets/images/bt8-expand-light.png" height="40" class="logo-light"
                                alt="AiBard123| ai工具网址导航,ai最新产品">
                            <img src="/assets/images/bt8-expand-dark.png" height="40" class="logo-dark d-none"
                                alt="AiBard123| ai工具网址导航,ai最新产品">
                        </a>
                        <a href="https://aibard123.com/" class="logo-collapsed">
                            <img src="/assets/images/bt.png" height="40" class="logo-light"
                                alt="AiBard123| ai工具网址导航,ai最新产品">
                            <img src="/assets/images/bt.png" height="40" class="logo-dark d-none"
                                alt="AiBard123| ai工具网址导航,ai最新产品">
                        </a>
                    </div>
                    
                </div>
                <div class="sidebar-menu flex-fill">
                    <div class="sidebar-scroll">
                        <div class="sidebar-menu-inner">
                            <ul>
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#00834a9dd147b04c5d53d4368cdb0b57" class="smooth">
                                            <i class="fas fa-sun fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>本月热门</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#db0311e7ecfedd24d157f0ceb4a0897f" class="smooth">
                                            <i class="fas fa-star-and-crescent fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>热门网站</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#21b5cbb2c769010fec3ce029a5f8a4a3" class="smooth">
                                            <i class="far fa-star fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>国内热门</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#8310718935e8ec25ce0350de01e3f7dc" class="smooth">
                                            <i class="fas fa-phone fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>对话工具</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#d58e850d9115797306c2edf61ac6ddd8" class="smooth">
                                            <i class="fas fa-newspaper fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>写作</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#2a7418a5f8f1ca4e054364a9300657df" class="smooth">
                                            <i class="fas fa-image fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>图像生成</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#7808a68ee1b34dab43011429a12de19e" class="smooth">
                                            <i class="fas fa-image fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>图像处理</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#6729afc51f5ac49a828812fa0eb0c82f" class="smooth">
                                            <i class="fas fa-video fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>音视频</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#e5ce844860451fff3faf3d8f8894971d" class="smooth">
                                            <i class="fas fa-music fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>音乐生成</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#db53804b7d726967c58fcc8c9ca03d27" class="smooth">
                                            <i class="fas fa-language fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>办公</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#47b7af9547e034d28fe6f6d439968ac8" class="smooth">
                                            <i class="fas fa-copy fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>提示词</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#41282bf95e43c64d579757573a03cdde" class="smooth">
                                            <i class="fas fa-code fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>编程</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#fd71852fd52d5e18ef4f9a252f1eac58" class="smooth">
                                            <i class="fas fa-search fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>AI搜索</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#81b1637fbe47625dbdf2094acd3b6683" class="smooth">
                                            <i class="fas fa-language fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>文本翻译</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#2e9ba3fa6e1ed0e9311b3e97f97f9a40" class="smooth">
                                            <i class="fas fa-book fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>学习网站</span>
                                        </a>
                                    </li>
                                    
                                
                            </ul>           
                        </div>
                    </div>
                </div>
                <div class="border-top py-2 border-color">
                    <div class="flex-bottom">
                        <ul>
			    <li id="menu-item-212"
                                 class="menu-item menu-item-type-custom menu-item-object-custom menu-item-212 sidebar-item">
                                 <a href="#friendlink" class="smooth">
                                     <i class="fab fa-staylinked icon-fw icon-lg mr-2"></i>
                                     <span>友情链接</span>
                                 </a>
                            </li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
    </div>


<div class="flex-fill grid-bg">
    <div class="big-header-banner">
        <div id="header" class="page-header sticky">
            <div class="navbar navbar-expand-md">
                <div class="container-fluid p-0">

                    <a href="" class="navbar-brand d-md-none" title="AiBard123| ai工具网址导航,ai最新产品">
                        <img src="/assets/images/bt.png" class="logo-light"
                            alt="AiBard123| ai工具网址导航,ai最新产品">
                        <img src="/assets/images/bt.png" class="logo-dark d-none"
                            alt="AiBard123| ai工具网址导航,ai最新产品">
                    </a>

                    <div class="collapse navbar-collapse order-2 order-md-1">
                        <div class="header-mini-btn">
                            <label>
                                <input id="mini-button" type="checkbox">
                                <svg viewbox="0 0 100 100" xmlns="http://www.w3.org/2000/svg">
                                    <path class="line--1" d="M0 40h62c18 0 18-20-17 5L31 55"></path>
                                    <path class="line--2" d="M0 50h80"></path>
                                    <path class="line--3" d="M0 60h62c18 0 18 20-17-5L31 45"></path>
                                </svg>
                            </label>

                        </div>

                        <ul class="navbar-nav site-menu" style="margin-right: 16px;">
                        
			<li >
				<a href="/">
                                    <i class="fa fa-home fa-lg mr-2"></i>
                                    <span>首页</span>
                                </a>
				<ul class="sub-menu">
				
				</ul>
			    </li>
			
			</ul>

                        
                        <div class="rounded-circle weather">
                            <div id="he-plugin-simple" style="display: contents;"></div>
                            <script>WIDGET = {
                                    CONFIG: {
                                        "modules": "01234",
                                        "background": 5,
                                        "tmpColor": "008000",
                                        "tmpSize": 14,
                                        "cityColor": "008000",
                                        "citySize": 14,
                                        "aqiColor": "#008000",
                                        "aqiSize": 14,
                                        "weatherIconSize": 24,
                                        "alertIconSize": 18,
                                        "padding": "10px 10px 10px 10px",
                                        "shadow": "1",
                                        "language": "auto",
                                        "borderRadius": 5,
                                        "fixed": "false",
                                        "vertical": "middle",
                                        "horizontal": "left",
                                        "key": "085791e805a24491b43b06cf58ab31e7"
                                    }
                                }
                            </script>
                            <script src="https://widget.qweather.net/simple/static/js/he-simple-common.js?v=2.0"></script>
                        </div>
                        
                    </div>

                    <ul class="nav navbar-menu text-xs order-1 order-md-2">
                        
                        
                        <li class="nav-item mr-3 mr-lg-0 d-none d-lg-block">
                            <script>
                                fetch('https://v1.hitokoto.cn')
                                    .then(response => response.json())
                                    .then(data => {
                                    const hitokoto = document.getElementById('hitokoto_text')
                                    hitokoto.href = 'https://hitokoto.cn/?uuid=' + data.uuid
                                    hitokoto.innerText = data.hitokoto
                                    })
                                    .catch(console.error)
                            </script>                           
                            <div id="hitokoto"><a href="#" target="_blank" id="hitokoto_text">疏影横斜水清浅，暗香浮动月黄昏。</a></div>
                        </li>
                        
                        
                        <li class="nav-search ml-3 ml-md-4">
                            <a href="javascript:" data-toggle="modal" data-target="#search-modal"><i
                                    class="iconfont icon-search icon-2x"></i></a>
                        </li>
                        <li class="nav-item d-md-none mobile-menu ml-3 ml-md-4">
                            <a href="javascript:" id="sidebar-switch" data-toggle="modal"
                                data-target="#sidebar"><i class="iconfont icon-classification icon-2x"></i></a>
                        </li>
                    </ul>
                </div>
            </div>
        </div>
        <div class="placeholder" style="height:74px"></div>
    </div>




<body class="page-body boxed-container  io-grey-mode">
    <main role="main" class="flex-shrink-0">
    <div class="container">
        
        <div class="content">
            <style>
    body{
	    background: #f9f9f9;
	}

    h1, h2, h3, h4, h5, h6 {
        margin-top: 1.5rem;
        margin-bottom: 1.5rem;
    }


 
@media (min-width: 1000px) {
  .container, .container-sm {
    max-width: 800px;
  }
}

</style>

<div class="featured-post-content">

    <a href="/digest/" class="featured-post-title">
       AI 文摘
    </a>

</div>

<section class="blog-single">
  <div class="container">
    <div class="row">

      <div class="col-lg-12 order-1 order-lg-2">
        <article class="single-blog">
          <p class="title">社区供稿Mixtral-8x7BPytorch实现</p>
            <br/>
          <ul class="meta">
            <li>
              By <a href=https://aibard123.com/about>AiBard123</a>
            </li>
            <li>
              <i class="fa fa-clock-o"></i>
              February 1, 2024 - 2 min read
            </li>
          </ul>

          <div class="_1NCGf">
              <img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/kF2wpSFV2cic7iaqLh1ibXYXNhzGHYuh5UTRAES11ChMYCfBTgliboWnjkKocyDpsxYJJQlwtB0XQVqNyDyOQvuKGA/640?wx_fmt=png&amp;from=appmsg&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" width="640" >
          </div>
            <br>
            <br>
            <br>
          
          <div class="single-blog-content">
            <p>作者： Hugging Face  来源： <a href="https://mp.weixin.qq.com/s/HProBDSA9WxyD-JuKpJ9ew">Hugging Face</a></p>
<h4 id="0前言">0.前言</h4>
<p>本文从代码角度来谈下 Mixtral 8x7B
混合专家Pytorch
的实现</p>
<h4 id="1论文概述">1.论文概述</h4>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/kF2wpSFV2cic7iaqLh1ibXYXNhzGHYuh5UTuoSNBPUGNMBuAnyr5iarwBAzVSTUYiaqBibauhuhBZ5ZZPdOz50KjIdfQ/640?wx_fmt=png&amp;from=appmsg&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=""></p>
<p>Mixtral-8x7B
引爆了MoE
的技术方向，更多针对MoE
优化的Trick
出现，回归模型本身来解析：</p>
<ol>
<li>
<p>Mixtral 8x7B
采用了sMoE
模型结构，模型的细节如何？路由负载均衡如何计算？代码如何实现？</p>
</li>
<li>
<p>Mixtral 8x7B
的训练流程和推理流程是怎么样的，如何提高训练和推理效率？</p>
</li>
<li>
<p>Mixtral 8x7B
的模型参数是如何计算的？</p>
</li>
<li>
<p>Mixtral 8x7B
性能硬刚LLaMA2-70B
和GPT-3.5
， 性能一线水准，在MBPP
代码能力超越3.5</p>
</li>
</ol>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/kF2wpSFV2cic7iaqLh1ibXYXNhzGHYuh5UTrMqqP2zjr5crXp0PJUXdo4k8Ss79kneb0ScYc9qKBht9ZuYsumDSMA/640?wx_fmt=png&amp;from=appmsg&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=""></p>
<h4 id="2-mixtral-8x7b-模型架构和计算流程">2. Mixtral 8x7B 模型架构和计算流程</h4>
<blockquote>
<p>Mixtral is based on a<strong>transformer architecture [31]</strong>  and uses the same<strong>modifications</strong> <strong>as described in [18]</strong> , with the notable exceptions that Mixtral supports a fully dense context length of 32k tokens, and the<strong>feed forward</strong> <strong>blocks are replaced by Mixture-of-Expert layers</strong>  (Section 2.1). The model architecture parameters are summarized in Table 1.</p>
</blockquote>
<ul>
<li>
<p>base
的模型结构为Transformers
的改版Mistral-7B</p>
</li>
<li>
<p>MoE
作用在Feed Forward Blocks
上</p>
</li>
</ul>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/kF2wpSFV2cic7iaqLh1ibXYXNhzGHYuh5UTicGwfhibW32mZFejCibt31NDh4G6xfRtzPZOMLtARwb7YNnE9ZMDrjvOw/640?wx_fmt=png&amp;from=appmsg&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=""></p>
<h4 id="21-mixtral-模型架构">2.1 Mixtral 模型架构</h4>
<blockquote>
<p>In a Transformer model, the MoE layer is applied independently per token and replaces the feed-forward (FFN) sub-block of the transformer block. For Mixtral we use the same SwiGLU architecture as the expert function Ei(x) and set K = 2. This means each token is routed to two SwiGLU sub-blocks with different sets of weights. Taking this all together, the output y for an input token x is computed as:</p>
</blockquote>
<ul>
<li>
<p>以LLaMA2
或Mistral-7B
来说其MLP
都是SwiGLU
形式</p>
</li>
<li>
<p>在Mixtral-8x7B
中<em>每层</em>的Decoder
层的MLP
都以sMoE
来替换掉</p>
</li>
</ul>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/kF2wpSFV2cic7iaqLh1ibXYXNhzGHYuh5UTgQEL8IicriaUTWzStDOvccYeWGeo9vVWBiafPuvBuOp1m3oCWblzonMMA/640?wx_fmt=png&amp;from=appmsg&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=""></p>
<p>Transformers Mixtral-of-Expert</p>
<p>代码实现:</p>
<p>在Huggingface
的Transformers
框架中, Mixtral
主要有两部分组成</p>
<ul>
<li>
<p>MixtralDecoderLayer</p>
</li>
<li>
<p>MixtralSparseMoeBlock
：替换掉原有的MLP层</p>
<p>MixtralForCausalLM(<br>
(model): MixtralModel(<br>
(embed_tokens): Embedding(32000, 128)<br>
(layers): ModuleList(<br>
(1): MixtralDecoderLayer(<br>
(self_attn): MixtralAttention(<br>
(q_proj): Linear(in_features=128, out_features=128, bias=False)<br>
(k_proj): Linear(in_features=128, out_features=128, bias=False)<br>
(v_proj): Linear(in_features=128, out_features=128, bias=False)<br>
(o_proj): Linear(in_features=128, out_features=128, bias=False)<br>
(rotary_emb): MixtralRotaryEmbedding()<br>
)<br>
(block_sparse_moe): MixtralSparseMoeBlock(<br>
(gate): Linear(in_features=128, out_features=8, bias=False)<br>
(experts): ModuleList(<br>
(0-7): 8 x MixtralBLockSparseTop2MLP(<br>
(w1): Linear(in_features=128, out_features=256, bias=False)<br>
(w2): Linear(in_features=256, out_features=128, bias=False)<br>
(w3): Linear(in_features=128, out_features=256, bias=False)<br>
(act_fn): SiLU()<br>
)<br>
)<br>
)<br>
(input_layernorm): MixtralRMSNorm()<br>
(post_attention_layernorm): MixtralRMSNorm()<br>
)<br>
)<br>
(norm): MixtralRMSNorm()<br>
)</p>
</li>
</ul>
<h4 id="22-smoe-层实现">2.2 SMoE 层实现</h4>
<h4 id="221-单个-expert-实现">2.2.1 单个 Expert 实现</h4>
<pre><code>import torch  
from torch import nn  
from transformers import MixtralConfig  
  
class MixtralBLockSparseTop2MLP(nn.Module):  
    def __init__(self, config: MixtralConfig):  
        super().__init__()  
        self.ffn_dim = config.intermediate_size  
        self.hidden_dim = config.hidden_size  
  
        self.w1 = nn.Linear(self.hidden_dim, self.ffn_dim, bias=False)  
        self.w2 = nn.Linear(self.ffn_dim, self.hidden_dim, bias=False)  
        self.w3 = nn.Linear(self.hidden_dim, self.ffn_dim, bias=False)  
  
        self.act_fn = nn.SiLU()  
  
    # Forward 是 SwiGLU  
    def forward(self, hidden_states):  
        y = self.act_fn(self.w1(hidden_states)) * self.w3(hidden_states)  
        y = self.w2(y)  
        return y  
  
x = torch.randn(1, 64, 128)  
expert = MixtralBLockSparseTop2MLP(config)  
print('单个专家为原LLaMA的MLP层')  
print(expert)  
g = expert(x)  
print('单个专家输入:', x.shape)  
print('单个专家输出结果：', g.shape)
</code></pre>
<p>结果</p>
<pre><code>单个专家为原LLaMA的MLP层  
MixtralBLockSparseTop2MLP(  
  (w1): Linear(in_features=128, out_features=256, bias=False)  
  (w2): Linear(in_features=256, out_features=128, bias=False)  
  (w3): Linear(in_features=128, out_features=256, bias=False)  
  (act_fn): SiLU()  
)  
单个专家输入:  
torch.Size([1, 64, 128])  
单个专家输出结果：  
torch.Size([1, 64, 128])  
</code></pre>
<h4 id="222-混合expert实现">2.2.2 混合Expert实现</h4>
<pre><code>class MixtralSparseMoeBlock(nn.Module):  
    def __init__(self, config):  
        super().__init__()  
        self.hidden_dim = config.hidden_size  
        self.ffn_dim = config.intermediate_size  
        self.num_experts = config.num_local_experts  
        self.top_k = config.num_experts_per_tok  
  
        # gating  
        self.gate = nn.Linear(self.hidden_dim, self.num_experts, bias=False)  
  
        # 多个 SwiGLU MLP 层组成混合专家  
        self.experts = nn.ModuleList([MixtralBLockSparseTop2MLP(config) \  
                                      for _ in range(self.num_experts)])  
  
x = torch.randn(1, 64, 128)  
experts = MixtralSparseMoeBlock(config)  
print('多个专家混合专家')  
print(experts)  
</code></pre>
<p>在以上我们实现了模型的关键结构， 但是这里的sMoE
的Forward
并没有实现</p>
<h4 id="23-smoe-计算流程">2.3 SMoE 计算流程</h4>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/kF2wpSFV2cic7iaqLh1ibXYXNhzGHYuh5UTHTBtZJLLibRpzjnqY87nfhic8WLagIrepVPibbYTnfS1ylmNUPzibNiaHKw/640?wx_fmt=png&amp;from=appmsg&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=""></p>
<h4 id="231-gating流程">2.3.1 Gating流程</h4>
<p>以下表示为多个token
的gating
计算流程</p>
<pre><code># 阶段一  
# 计算稀疏 gating 值  
tokens = 6  
x = torch.randn(1, tokens, 128) # 6个token  
hidden_states = x  
batch_size, sequence_length, hidden_dim = hidden_states.shape  
hidden_states = hidden_states.view(-1, hidden_dim)  
  
 # 每层都会产生router_logits, 将用于最后作 load balance loss  
router_logits = experts.gate(hidden_states)  
print(f'experts.gate output router logits : \n {router_logits}')  
  
# 计算 TopK 的 专家 logits 和 Top2 专家的位置  
routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)  
print(f'softmax weight  : \n {routing_weights}')  
  
routing_weights, selected_experts = torch.topk(routing_weights, \  
                                               experts.top_k, dim=-1)  
print(f'expert select : \n {selected_experts}')  
print(f'topk : \n {routing_weights}')  
  
routing_weights /= routing_weights.sum(dim=-1, keepdim=True)  
print(f'topk归一化 : \n {routing_weights}')  
  
routing_weights = routing_weights.to(hidden_states.dtype)  
  
## One Hot 编码  
expert_mask = torch.nn.functional.one_hot(selected_experts, \  
                                          num_classes=experts.num_experts).permute(2, 1, 0)  
for i in range(tokens):  
    print(f'【token_{i}】\n', expert_mask[:,:,i])  
</code></pre>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/kF2wpSFV2cic7iaqLh1ibXYXNhzGHYuh5UTYkor07f36ONoCBsUUtcTia2yibl3ROkwTVSw5nkAk8Jojgh6iarKibqalg/640?wx_fmt=png&amp;from=appmsg&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=""></p>
<p>追踪x3
的结果</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/kF2wpSFV2cic7iaqLh1ibXYXNhzGHYuh5UTzlxq6SOtVb5jVuFM7jvibIPSt2Y56J2qadvzxpEVFC1HlKRy6oAAfYw/640?wx_fmt=png&amp;from=appmsg&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=""></p>
<h4 id="232-expert-流程">2.3.2 Expert 流程</h4>
<ul>
<li>
<p>sMoE
中是基于专家来选择token
来计算的</p>
</li>
<li>
<p>token
先序：左图为token3
选择expert 2
, expert 3
号来计算sMoE
结果</p>
</li>
<li>
<p>expert
先序：右图为依次计算expert2
和expert3
才得出token3
的sMoE
结果</p>
</li>
</ul>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/kF2wpSFV2cic7iaqLh1ibXYXNhzGHYuh5UTibLQMaru4UNsJNMZiavv70jibJpBRHy9FbplMlvuqI36aVCKoQk3UuXibw/640?wx_fmt=png&amp;from=appmsg&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=""></p>
<p>代码实现结果为：</p>
<pre><code>## 最终结果  
final_hidden_states = torch.zeros(  
    (batch_size * sequence_length, hidden_dim), \  
        dtype=hidden_states.dtype, device=hidden_states.device  
)  
print(f'final moe result shape for each token: {final_hidden_states.shape}')  
  
# 每个专家收集需要计算token  
for expert_idx in range(experts.num_experts):  
  
    print(f'--------expert {expert_idx} ---------')  
  
    expert_layer = experts.experts[expert_idx]  
    print(expert_mask[expert_idx])  
    idx, top_x = torch.where(expert_mask[expert_idx])  
    print(f'专家 {expert_idx} 计算的样本编号:',top_x.tolist()) # select x_idx for expert top1  
    print(f'专家 {expert_idx} top1:0, top2:1 ',idx.tolist()) # 0 is top1 ,1 is top2  
    print(f'有 {len(top_x)} / {x.shape[1]} token 选到专家 {expert_idx}')  
      
    top_x_list = top_x.tolist()  
    idx_list = idx.tolist()  
  
    current_state = hidden_states[None, top_x_list].reshape(-1, hidden_dim)  
  
    # expert_0(x) * routing_weights  
    current_hidden_states = expert_layer(current_state)  \  
                            * routing_weights[top_x_list, idx_list, None]  
  
    # 将计算的单个专家结果填入到结果表里  
    final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))  
  
    print(current_state.shape)   
    print(routing_weights[top_x_list, idx_list, None].shape)  
    print(current_hidden_states.shape)  
    print(final_hidden_states.shape)  
</code></pre>
<p>输出结果为:</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/kF2wpSFV2cic7iaqLh1ibXYXNhzGHYuh5UTRAES11ChMYCfBTgliboWnjkKocyDpsxYJJQlwtB0XQVqNyDyOQvuKGA/640?wx_fmt=png&amp;from=appmsg&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=""></p>
<h4 id="24-router-load-balence-计算">2.4 Router Load Balence 计算</h4>
<p>路由负载均衡的实现来自Switch Transformers</p>
<blockquote>
<p>Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch. See Switch Transformer for more details. This function implements the loss function presented in equations (4) - (6) of the paper. It aims at penalizing cases where the routing between experts is too unbalanced.</p>
</blockquote>
<h4 id="241-switch-transformers-load-balance-loss">2.4.1 Switch Transformers Load Balance Loss</h4>
<p>该算法为sMoE
简化版load balance
, 去除了原版 balance loss 估计</p>
<p>fi
:在一个batch
中第i
专家分配到token
的数量概率</p>
<p>Pi
:在一个batch
中T
个tokens
，各个专家选到tokens
的概率和</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/kF2wpSFV2cic7iaqLh1ibXYXNhzGHYuh5UTzpEvq5LblYzBK4H60BkycqKzGGGxsLg6NHqibNluYFgl6E7Vqde7A4g/640?wx_fmt=png&amp;from=appmsg&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="">2.4.2 手撕Mixtral Load Balance Loss 计算流程</p>
<p>可以想象下layer norm
只是在<strong>当前层</strong> 里对所有tokens
做，而负载均衡处理范围更广，对<strong>所有层</strong> 的tokens
，在每个expert
的纵向计算出单专家负载值，求和便得到整个网络的负载均衡 loss</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/kF2wpSFV2cic7iaqLh1ibXYXNhzGHYuh5UTIkNicrMD31J23ymQZH1UBoDRoLA2UUX6OGCy3R3gSzQNE0CwC3icKhUA/640?wx_fmt=png&amp;from=appmsg&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=""></p>
<h4 id="243-手撕mixtral-load-balance">2.4.3 手撕Mixtral Load Balance</h4>
<pre><code>import torch  
  
num_experts = 8  
batch = 10  
seq_length = 6  
top_k = 2  
  
print(f'sMoE num_experts:{num_experts} top_k:{top_k} batch:{batch} seq_length:{seq_length}')  
  
router_logits_1 = torch.randn(batch, seq_length, num_experts).view(-1,num_experts) # layer 1  
router_logits_2 = torch.randn(batch, seq_length, num_experts).view(-1,num_experts) # layer 2  
router_logits = [router_logits_1, router_logits_2]   
  
concatenated_gate_logits = torch.cat(router_logits, dim = 0)  
print('单层gating的路由logits:', router_logits_1.shape)   
print('两层gating的路由logits:', concatenated_gate_logits.shape)  
  
print('根据logits top-k 计算热独编码')  
routing_weights = torch.nn.functional.softmax(concatenated_gate_logits, dim=-1)  
_, selected_experts = torch.topk(routing_weights, top_k, dim=-1)  
expert_mask = torch.nn.functional.one_hot(selected_experts, num_experts)  
print(expert_mask.shape)  
  
tokens_sum_expert = torch.sum(expert_mask.float(), dim=0)  
tokens_per_expert = torch.mean(expert_mask.float(), dim=0)  
print(f'top1 每个专家平均处理的token   :', tokens_sum_expert[0])  
print(f'top2 每个专家平均处理的token fi:', tokens_per_expert[1])  
print(f'top1与top2水平合计', tokens_per_expert.sum(dim=1))  
  
# Compute the average probability of routing to these experts  
router_prob_per_expert = torch.mean(routing_weights, dim=0)  
print('router_prob_per_expert Pi: ' , router_prob_per_expert)  
  
print( '每个专家的负载：',  tokens_per_expert * router_prob_per_expert.unsqueeze(0))  
overall_loss = torch.sum(tokens_per_expert * router_prob_per_expert.unsqueeze(0))  
print('final loss:', overall_loss)  
</code></pre>
<p>计算结果</p>
<pre><code>sMoE num_experts:8 top_k:2 batch:10 seq_length:6  
单层gating的路由logits:  
torch.Size([60, 8])  
两层gating的路由logits:  
torch.Size([120, 8])  
根据logits top-k 计算热独编码  
torch.Size([120, 2, 8])  
top1 每个专家平均处理的token   : tensor([10., 14., 19., 17., 14.,  9., 17., 20.])  
top2 每个专家平均处理的token fi: tensor([0.1667, 0.1333, 0.1833, 0.0833, 0.1167, 0.1500, 0.0667, 0.1000])  
top1与top2水平合计 tensor([1., 1.])  
router_prob_per_expert Pi:  tensor([0.1236, 0.1184, 0.1351, 0.1168, 0.1311, 0.1147, 0.1156, 0.1447])  
每个专家的负载：tensor([[0.0103, 0.0138, 0.0214, 0.0165, 0.0153, 0.0086, 0.0164, 0.0241],  
        [0.0206, 0.0158, 0.0248, 0.0097, 0.0153, 0.0172, 0.0077, 0.0145]])  
final loss: tensor(0.2520)  
</code></pre>
<p>这里的gating logits
是跨batch
跨层的，作用在每个token
上</p>
<h4 id="3-mixtral-8x7b-参数量计算">3. Mixtral 8x7B 参数量计算</h4>
<h4 id="31-原论文描述">3.1 原论文描述</h4>
<p>这里的13B
是指单个 token
涉及的模型参数量，实际推理时每个token
都有不同的expert
，那么实际运行还是跑47B
参数的, 使用了sMoE 并不会减少显存占用。</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/kF2wpSFV2cic7iaqLh1ibXYXNhzGHYuh5UT7fAtt8hqIJDtn1ehX96BKZvqkXJ4wQd8T8tPqYsnJJw4MGcdJwTpeQ/640?wx_fmt=png&amp;from=appmsg&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=""></p>
<h4 id="32-模型参数量计算">3.2 模型参数量计算</h4>
<p>忽略GQA
计算</p>
<pre><code>dim = 4096  
n_layers = 32  
head_dim = 128  
hidden_dim = 14336  
n_heads = 32  
n_kv_heads = 8# ignore GQA  
vocab_size = 32000  
num_experts = 8  
top_k_experts = 2  
  
# attention mlp layernorm  
llama_num = n_layers * (dim * dim * 4 + hidden_dim * dim * 3 + 2 * dim ) \  
        + 2 * vocab_size * dim   
print('llama:', llama_num)  
  
# attention 【mlp*8】 layernorm  
moe_num = n_layers * (dim * dim * 4 + hidden_dim * dim * 3 * 8 + 2 * dim ) \  
        + 2 * vocab_size * dim   
print('moe:', moe_num)  
  
# attention 【mlp*2】 layernorm  
# ToP2-inference  
moe_num = n_layers * (dim * dim * 4 + hidden_dim * dim * 3 * 2 + 2 * dim ) \  
        + 2 * vocab_size * dim   
print('moe top-2:', moe_num)  
</code></pre>
<p>结果</p>
<pre><code>llama: 8047034368  
moe: 47507046400  
moe top-2: 13684178944  
</code></pre>
<h4 id="4-moe-扩展">4. MoE 扩展</h4>
<h4 id="41-megablocks">4.1 MegaBlocks</h4>
<blockquote>
<p>MoE layers can be run efficiently on single GPUs with high performance specialized kernels. For example,<strong>Megablocks</strong></p>
</blockquote>
<p>MegaBlocks
实现稀疏的MoE
计算</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/kF2wpSFV2cic7iaqLh1ibXYXNhzGHYuh5UTonzLFbjmhCJSjN2GmXZDFXxNVGcPXWVeJkBgvwZzHFIAJtvxq6nbYw/640?wx_fmt=png&amp;from=appmsg&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=""></p>
<p>题外话：XFormers
也实现了类似思想的算子，batch
里的attention
通过Mask
实现多序列稀疏计算。</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/kF2wpSFV2cic7iaqLh1ibXYXNhzGHYuh5UTQt5FjUveq3vT2co7iacAP7uo3uwShBlzG2iacUvdYMYGv7EUtKrUjLHg/640?wx_fmt=png&amp;from=appmsg&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=""></p>
<h4 id="42-gshard">4.2 GShard</h4>
<p>Mixtral
论文里在load balance
里提了一下GShard
, 是首篇将MoE
引入到Transformers
的工作</p>
<blockquote>
<p>This formulation is similar to the GShard architecture [21], with the exceptions that we replace all FFN sub-blocks by MoE layers while GShard replaces every other block, and that GShard uses a more elaborate gating strategy for the second expert assigned to each token.</p>
</blockquote>
<p>GShard
在不同GPU
上分配不同的专家，其他参数都共享，数据派发到专家，专家结果汇总都由All-to-All
算子实现</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/kF2wpSFV2cic7iaqLh1ibXYXNhzGHYuh5UTTuUKYUsII09M7ExW6HpEn8paOEsqFA11g7nju7KmhSbczvklYh34XQ/640?wx_fmt=png&amp;from=appmsg&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=""></p>
<p>DeepSpeed-MoE源码对All-to-All
的实现如下</p>
<pre><code>class _AllToAll(torch.autograd.Function):  
  
    @staticmethod  
    def forward(  
            ctx: Any,  
            # TODO: replace with DS process group  
            group: torch.distributed.ProcessGroup,  
            input: Tensor) -&gt; Tensor:# type: ignore  
        ctx.group = group  
        input = input.contiguous()  
        output = torch.empty_like(input)  
        dist.all_to_all_single(output, input, group=group)  
        return output  
  
    @staticmethod  
    def backward(ctx: Any, *grad_output: Tensor) -&gt; Tuple[None, Tensor]:  
        return (None, _AllToAll.apply(ctx.group, *grad_output))  
          
class MOELayer(Base):  
     # ...  
     def forward(self, *input: Tensor,**kwargs: Any) -&gt; Tensor:  
        # ...  
        dispatched_input = _AllToAll.apply(self.ep_group, dispatched_input)  
  
        # Re-shape after all-to-all: ecm -&gt; gecm  
        dispatched_input = dispatched_input.reshape(self.ep_size, self.num_local_experts, -1, d_model)  
  
        expert_output = self.experts(dispatched_input)  
  
  
        expert_output = _AllToAll.apply(self.ep_group, expert_output)  
  
    #...  
</code></pre>
<h4 id="43-deepspeed-moe">4.3 DeepSpeed-MoE</h4>
<ul>
<li>
<p>更加工程化的实现可以看DeepSpeed-MoE
的开源方案</p>
</li>
<li>
<p>MoE
层使用Expert-Paralallelism
做并行 AlltoAll
实现如上</p>
</li>
<li>
<p>非MoE
层使用TP+DP</p>
</li>
</ul>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/kF2wpSFV2cic7iaqLh1ibXYXNhzGHYuh5UTiaRIAgSYgPFHNWNTGVvu3GaLN2yqpHtya4GygMQiceRGovOWjPiaaVV3A/640?wx_fmt=png&amp;from=appmsg&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=""></p>
<h4 id="44-llama-moe">4.4 LLaMA-MoE</h4>
<p>Mixtral 8x7B
训不动？试试将LLaMA
原MLP
改造成LLaMA-MoE</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/kF2wpSFV2cic7iaqLh1ibXYXNhzGHYuh5UTblKVIicEKAo8EoyKGTN9Mgwmpl628iaG0P4wlVQsIhzkcaNicDbR3vlxg/640?wx_fmt=png&amp;from=appmsg&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=""></p>
<p>LLaMA-MoE 上关键代码是用LinearGLUExperts
代替原本LLaMA
里的SwiGLU
层</p>
<pre><code> class LinearGLUExperts(nn.Module):  
    # ...  
    def __init__(...):  
        # ...   
        # 每个专家都创建SwiGLU MLP层  
        for i in range(num_experts):  
            # this matrix will be transposed when performing linear forwarding  
            this_expert_weight_gate = nn.Parameter(  
                torch.empty((size_experts[i], in_features),**factory_kwargs)  
            )  
            # this matrix will be transposed when performing linear forwarding  
            this_expert_weight_up = nn.Parameter(  
                torch.empty((size_experts[i], in_features),**factory_kwargs)  
            )  
            # this matrix will be transposed when performing linear forwarding  
            this_expert_weight_down = nn.Parameter(  
                torch.empty((out_features, size_experts[i]),**factory_kwargs)  
            )  
            self.weight_gate.append(this_expert_weight_gate)  
            self.weight_up.append(this_expert_weight_up)  
            self.weight_down.append(this_expert_weight_down)  
        # ...  
</code></pre>
<h4 id="heading"></h4>
<h4 id="5-mixtral-8x7b-总结--进一步阅读">5. Mixtral 8x7B 总结 &amp; 进一步阅读</h4>
<ul>
<li>
<p>Mixtral 8x7B
实现并不复杂，其中load-balance loss
是expert-wise
维度计算的</p>
</li>
<li>
<p>当前发布的模型还是围绕模型结构展开的， 期待mistral.AI
上线创新的对齐方案</p>
</li>
<li>
<p>涉及到多机多卡的sMoE
分布式训练非常需要工程技巧, 不同的模型架构和集群可以有多种DP\TP\EP..
组合方案，</p>
</li>
<li>
<p>在·Mixtral·中对于实验反直觉论点 专家的知识是作用在 token 级别，而不是domain级别，对 MoE 感兴趣的话可以进一步开盒分析</p>
</li>
</ul>
<h4 id="reference">Reference</h4>
<ol>
<li>
<p>Mixture of Experts Explained</p>
</li>
<li>
<p>方佳瑞：MoE训练论文解读之Megablocks：打破动态路由限制</p>
</li>
<li>
<p>方佳瑞：MoE训练系统之JANUS：参数服务器助力MoE训练</p>
</li>
<li>
<p>方佳瑞：MoE训练论文解读之Tutel: 动态切换并行策略实现动态路由</p>
</li>
<li>
<p>西门宇少：对MoE大模型的训练和推理做分布式加速——DeepSpeed-MoE论文速读</p>
</li>
<li>
<p>吃果冻不吐果冻皮：大模型分布式训练并行技术（八）-MOE并行</p>
</li>
<li>
<p>孟繁续：Mixtral-8x7B 模型挖坑</p>
</li>
<li>
<p>Mixtral-of-experts</p>
</li>
<li>
<p>Mistral-7B</p>
</li>
<li>
<p>Gshard</p>
</li>
<li>
<p>Switch Transformers</p>
</li>
<li>
<p>sMoE</p>
</li>
<li>
<p>Transformers-Mixtral-of-Experts</p>
</li>
<li>
<p>DeepSpeed-MoE</p>
</li>
<li>
<p>Megablocks</p>
</li>
<li>
<p>LLaMA-MoE</p>
</li>
</ol>
<h4 id="heading-1"></h4>
<p>本文由 Hugging Face 中文社区内容共建项目提供，稿件由社区成员投稿，经授权发布于 Hugging Face 公众号。文章内容不代表官方立场，文中介绍的产品和服务等均不构成投资建议。了解更多请关注公众号:</p>
<p>如果你有与开源 AI、Hugging Face 相关的技术和实践分享内容，以及最新的开源 AI 项目发布，希望通过我们分享给更多 AI 从业者和开发者们，请通过下面的链接投稿与我们取得联系:</p>
<p><a href="https://hf.link/tougao">https://hf.link/tougao</a></p>
<p>更多AI工具，参考<a href="https://aibard123.com/">Github-AiBard123</a>，<a href="https://aibard123.com/">国内AiBard123</a></p>



          </div>

可关注我们的公众号：每天AI新工具

<p><img src="/images/aitools/2024/03/qrcode_for_gh_dde1b429630d_258.jpg" alt=""></p>

        </article>

      </div>
    </div>
  </div>
</section>
        </div>
    </div>
    </main>




<script type='text/javascript' src='/assets/js/jquery.ui.touch-punch.min-0.2.2.js' id='jqueryui-touch-js'></script>
<script type='text/javascript' src='/assets/js/clipboard.min-5.6.2.js' id='clipboard-js'></script>
<script type='text/javascript' src='/assets/js/tooltip-extend.js' id='iplaycode-nav-js'></script>
<script type='text/javascript' id='popper-js-extra'>
 

var theme = {"ajaxurl":"","addico":"https:\/\/nav.baidu.cn\/wp-content\/themes\/onenav\/images\/add.png","order":"asc","formpostion":"top","defaultclass":"io-grey-mode","isCustomize":"1","icourl":"","icopng":".png","urlformat":"1","customizemax":"10","newWindow":"0","lazyload":"1","minNav":"1","loading":"1","hotWords":"baidu","classColumns":" col-sm-6 col-md-4 col-xl-5a col-xxl-6a ","apikey":"TWpBeU1UVTNOekk1TWpVMEIvZ1M2bFVIQllUMmxsV1dZelkxQTVPVzB3UW04eldGQmxhM3BNWW14bVNtWk4="};
 
</script>
<script type='text/javascript' src='/assets/js/popper.min.js' id='popper-js'></script>
<script type='text/javascript' src='/assets/js/bootstrap.min-4.3.1.js' id='bootstrap-js'></script>
<script type='text/javascript' src='/assets/js/theia-sticky-sidebar-1.5.0.js' id='sidebar-js'></script>
<script type='text/javascript' src='/assets/js/lazyload.min-12.4.0.js' id='lazyload-js'></script>
<script type='text/javascript' src='/assets/js/fancybox.min-3.5.7.js' id='lightbox-js-js'></script>

<script type='text/javascript' src='/assets/js/app-anim.js' id='appanim-js'></script>

<script type="text/javascript">
    $(document).ready(function(){
        var siteWelcome = $('#loading');
        siteWelcome.addClass('close');
        setTimeout(function() {
            siteWelcome.remove();
        }, 600);
    });
</script>
<script>        
    $(document).ready(function(){
        setTimeout(function () {
            if ($('a.smooth[href="' + window.location.hash + '"]')[0]) {
                $('a.smooth[href="' + window.location.hash + '"]').click();
            }else if (window.location.hash != '') {
                $("html, body").animate({
                    scrollTop: $(window.location.hash).offset().top - 90
                }, {
                    duration: 500,
                    easing: "swing"
                });
            }
        }, 300);
        $(document).on('click','a.smooth',function(ev) {
            if($('#sidebar').hasClass('show') && !$(this).hasClass('change-href')){
                $('#sidebar').modal('toggle');
            }
            if($(this).attr("href").substr(0, 1) == "#"){
                $("html, body").animate({
                    scrollTop: $($(this).attr("href")).offset().top - 90
                }, {
                    duration: 500,
                    easing: "swing"
                });
            }
            if($(this).hasClass('go-search-btn')){
                $('#search-text').focus();
            }
            if(!$(this).hasClass('change-href')){
                var menu =  $("a"+$(this).attr("href"));
                menu.click();
                toTarget(menu.parent().parent(),true,true);
            }
        });
        $(document).on('click','a.tab-noajax',function(ev) {
            var url = $(this).data('link');
            if(url)
                $(this).parents('.d-flex.flex-fill.flex-tab').children('.btn-move.tab-move').show().attr('href', url);
            else
                $(this).parents('.d-flex.flex-fill.flex-tab').children('.btn-move.tab-move').hide();
        });
        
    });
</script>

<script>

(function(){
    if(document.cookie.replace(/(?:(?:^|.*;\s*)night\s*\=\s*([^;]*).*$)|^.*$/, "$1") === ''){
        if(new Date().getHours() > 22 || new Date().getHours() < 6){
            document.body.classList.remove('io-black-mode');
            document.body.classList.add('io-grey-mode');
            document.cookie = "night=1;path=/";
            console.log('夜间模式开启');
        }else{
            document.body.classList.remove('night');
            document.cookie = "night=0;path=/";
            console.log('夜间模式关闭');
        }
    }else{
        var night = document.cookie.replace(/(?:(?:^|.*;\s*)night\s*\=\s*([^;]*).*$)|^.*$/, "$1") || '0';
        if(night == '0'){
            document.body.classList.remove('night');
        }else if(night == '1'){
            document.body.classList.add('night');
        }
    }
})();

$("#search-bg").css("background", "linear-gradient(#e2c4c4, #d8d8d8)");   
function switchNightMode(){
    var night = document.cookie.replace(/(?:(?:^|.*;\s*)night\s*\=\s*([^;]*).*$)|^.*$/, "$1") || '0';
    if(night == '0'){
	$("#search-bg").css("background", "linear-gradient(#e2c4c4, #d8d8d8)");
        document.body.classList.remove('io-grey-mode');
        document.body.classList.add('io-black-mode');
        document.cookie = "night=1;path=/"
        console.log(' ');
        $(".switch-dark-mode").attr("data-original-title","日间模式");
        $(".mode-ico").removeClass("icon-night");
        $(".mode-ico").addClass("icon-light");
    }else{
	$("#search-bg").css("background", "linear-gradient(#4f4040, #1b1d1f)");
        document.body.classList.remove('io-black-mode');
        document.body.classList.add('io-grey-mode');
        document.cookie = "night=0;path=/"
        console.log(' ');
        $(".switch-dark-mode").attr("data-original-title","夜间模式");
        $(".mode-ico").removeClass("icon-light");
        $(".mode-ico").addClass("icon-night");
    }
}
</script>


<script>
    var newsContainer = document.getElementById('news-container');
    var newsItems = document.getElementsByClassName('news-item');
    var currentItem = 0;

    setInterval(function() {
        
        newsItems[currentItem].classList.remove('show');
        newsItems[currentItem].style.transform = 'translateY(-20px)';
        
        currentItem = (currentItem + 1) % newsItems.length;
        newsItems[currentItem].style.transform = 'translateY(' + (newsContainer.offsetHeight - 20) + 'px)';
        setTimeout(function() {
            newsItems[currentItem].classList.add('show');
        }, 500);
    }, 8000);
</script>

</body>
</html>



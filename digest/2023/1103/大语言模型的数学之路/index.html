

<!DOCTYPE html>
<html lang="zh-CN">

<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1" />
    <meta name="viewport"
        content="width=device-width, initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no" />
    <meta name="theme-color" content="#f9f9f9" />

	<title>大语言模型的数学之路 作者： PaperWeekly 来源： PaperWeekly ©PaperWeekly 原创 · 作者 | 台运鹏 研究方向 | NLP、ML 问题 LLM 通过大量的语料来建模下一个 token 的概率，这种训练方式促成 LLM 成为一个「文科生」，那么我们不禁对以下几个问题好奇： LLM 目前在数学问题上取得的进展； 有哪些技术路线在未  | AiBard123| ai工具网址导航,ai最新产品</title>
	<link rel="shortcut icon" href="/assets/images/favicon.png" />
    <meta name="keywords" content="chatgpt,AI,AI聊天,AI文本生成,AI绘画,AI编程,AI电商" />
    <meta name="description" content="AiBard123 网址导航 | 免费chatgpt 汇集各类先进的人工智能产品，旨在帮助用户更快速地了解和使用这些产品,轻松地浏览不同领域的AI产品，包括语音识别、图像处理、自然语言处理。" />
    
    <meta name="baidu-site-verification" content="codeva-cCAOSG8MBO" />
    
    <link rel="stylesheet" id="block-library-css"
        href="/assets/css/block-library.min-5.6.2.css" type="text/css" media="all" />
    <link rel="stylesheet" id="iconfont-css" href="/assets/css/iconfont-3.03029.1.css"
        type="text/css" media="all" />

    
    <link href="/scss/style.min.css" rel="stylesheet" />
    
		    <link rel="stylesheet" id="iowen-css" href="/assets/css/style-3.03029.1.css"
        type="text/css" media="all" />
    <link rel="stylesheet" id="custom-css" href="/assets/css/custom-style.css"
        type="text/css" media="all" />
		
		<link rel="stylesheet" href=/plugins/font-awesome/css/font-awesome.min.css />


    <link rel="stylesheet" id="fortawesome-css" href="/assets/fontawesome-5.15.4/css/all.min.css" type="text/css" />


    <script type="text/javascript" src="/assets/js/jquery.min-3.2.1.js" id="jquery-js"></script>
    <script type="text/javascript" src="/assets/js/content-search.js"  id="content-search-js"></script>

    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2073588164294660"
     crossorigin="anonymous"></script>

	
    <script>
        

		var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?8450bc732b2a86f7e4aec4ebd9fd8252";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();

        
    </script>
    

    
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-7071W80M2K"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-7071W80M2K');
    </script>

</head>


    <div class="page-container">
	
	<div id="sidebar" class="sticky sidebar-nav fade animate-nav" style="width: 170px">
        
            <div class="modal-dialog h-100 sidebar-nav-inner">
                <div class="sidebar-logo border-bottom border-color">
                    
                    <div class="logo overflow-hidden">
                        <a href="https://aibard123.com/" class="logo-expanded">
                            <img src="/assets/images/bt8-expand-light.png" height="40" class="logo-light"
                                alt="AiBard123| ai工具网址导航,ai最新产品">
                            <img src="/assets/images/bt8-expand-dark.png" height="40" class="logo-dark d-none"
                                alt="AiBard123| ai工具网址导航,ai最新产品">
                        </a>
                        <a href="https://aibard123.com/" class="logo-collapsed">
                            <img src="/assets/images/bt.png" height="40" class="logo-light"
                                alt="AiBard123| ai工具网址导航,ai最新产品">
                            <img src="/assets/images/bt.png" height="40" class="logo-dark d-none"
                                alt="AiBard123| ai工具网址导航,ai最新产品">
                        </a>
                    </div>
                    
                </div>
                <div class="sidebar-menu flex-fill">
                    <div class="sidebar-scroll">
                        <div class="sidebar-menu-inner">
                            <ul>
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#00834a9dd147b04c5d53d4368cdb0b57" class="smooth">
                                            <i class="fas fa-sun fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>本月热门</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#db0311e7ecfedd24d157f0ceb4a0897f" class="smooth">
                                            <i class="fas fa-star-and-crescent fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>热门网站</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#21b5cbb2c769010fec3ce029a5f8a4a3" class="smooth">
                                            <i class="far fa-star fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>国内热门</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#8310718935e8ec25ce0350de01e3f7dc" class="smooth">
                                            <i class="fas fa-phone fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>对话工具</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#d58e850d9115797306c2edf61ac6ddd8" class="smooth">
                                            <i class="fas fa-newspaper fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>写作</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#2a7418a5f8f1ca4e054364a9300657df" class="smooth">
                                            <i class="fas fa-image fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>图像生成</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#7808a68ee1b34dab43011429a12de19e" class="smooth">
                                            <i class="fas fa-image fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>图像处理</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#6729afc51f5ac49a828812fa0eb0c82f" class="smooth">
                                            <i class="fas fa-video fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>音视频</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#e5ce844860451fff3faf3d8f8894971d" class="smooth">
                                            <i class="fas fa-music fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>音乐生成</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#db53804b7d726967c58fcc8c9ca03d27" class="smooth">
                                            <i class="fas fa-language fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>办公</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#47b7af9547e034d28fe6f6d439968ac8" class="smooth">
                                            <i class="fas fa-copy fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>提示词</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#41282bf95e43c64d579757573a03cdde" class="smooth">
                                            <i class="fas fa-code fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>编程</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#fd71852fd52d5e18ef4f9a252f1eac58" class="smooth">
                                            <i class="fas fa-search fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>AI搜索</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#81b1637fbe47625dbdf2094acd3b6683" class="smooth">
                                            <i class="fas fa-language fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>文本翻译</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#2e9ba3fa6e1ed0e9311b3e97f97f9a40" class="smooth">
                                            <i class="fas fa-book fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>学习网站</span>
                                        </a>
                                    </li>
                                    
                                
                            </ul>           
                        </div>
                    </div>
                </div>
                <div class="border-top py-2 border-color">
                    <div class="flex-bottom">
                        <ul>
			    <li id="menu-item-212"
                                 class="menu-item menu-item-type-custom menu-item-object-custom menu-item-212 sidebar-item">
                                 <a href="#friendlink" class="smooth">
                                     <i class="fab fa-staylinked icon-fw icon-lg mr-2"></i>
                                     <span>友情链接</span>
                                 </a>
                            </li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
    </div>


<div class="flex-fill grid-bg">
    <div class="big-header-banner">
        <div id="header" class="page-header sticky">
            <div class="navbar navbar-expand-md">
                <div class="container-fluid p-0">

                    <a href="" class="navbar-brand d-md-none" title="AiBard123| ai工具网址导航,ai最新产品">
                        <img src="/assets/images/bt.png" class="logo-light"
                            alt="AiBard123| ai工具网址导航,ai最新产品">
                        <img src="/assets/images/bt.png" class="logo-dark d-none"
                            alt="AiBard123| ai工具网址导航,ai最新产品">
                    </a>

                    <div class="collapse navbar-collapse order-2 order-md-1">
                        <div class="header-mini-btn">
                            <label>
                                <input id="mini-button" type="checkbox">
                                <svg viewbox="0 0 100 100" xmlns="http://www.w3.org/2000/svg">
                                    <path class="line--1" d="M0 40h62c18 0 18-20-17 5L31 55"></path>
                                    <path class="line--2" d="M0 50h80"></path>
                                    <path class="line--3" d="M0 60h62c18 0 18 20-17-5L31 45"></path>
                                </svg>
                            </label>

                        </div>

                        <ul class="navbar-nav site-menu" style="margin-right: 16px;">
                        
			<li >
				<a href="/">
                                    <i class="fa fa-home fa-lg mr-2"></i>
                                    <span>首页</span>
                                </a>
				<ul class="sub-menu">
				
				</ul>
			    </li>
			
			</ul>

                        
                        <div class="rounded-circle weather">
                            <div id="he-plugin-simple" style="display: contents;"></div>
                            <script>WIDGET = {
                                    CONFIG: {
                                        "modules": "01234",
                                        "background": 5,
                                        "tmpColor": "008000",
                                        "tmpSize": 14,
                                        "cityColor": "008000",
                                        "citySize": 14,
                                        "aqiColor": "#008000",
                                        "aqiSize": 14,
                                        "weatherIconSize": 24,
                                        "alertIconSize": 18,
                                        "padding": "10px 10px 10px 10px",
                                        "shadow": "1",
                                        "language": "auto",
                                        "borderRadius": 5,
                                        "fixed": "false",
                                        "vertical": "middle",
                                        "horizontal": "left",
                                        "key": "085791e805a24491b43b06cf58ab31e7"
                                    }
                                }
                            </script>
                            <script src="https://widget.qweather.net/simple/static/js/he-simple-common.js?v=2.0"></script>
                        </div>
                        
                    </div>

                    <ul class="nav navbar-menu text-xs order-1 order-md-2">
                        
                        
                        <li class="nav-item mr-3 mr-lg-0 d-none d-lg-block">
                            <script>
                                fetch('https://v1.hitokoto.cn')
                                    .then(response => response.json())
                                    .then(data => {
                                    const hitokoto = document.getElementById('hitokoto_text')
                                    hitokoto.href = 'https://hitokoto.cn/?uuid=' + data.uuid
                                    hitokoto.innerText = data.hitokoto
                                    })
                                    .catch(console.error)
                            </script>                           
                            <div id="hitokoto"><a href="#" target="_blank" id="hitokoto_text">疏影横斜水清浅，暗香浮动月黄昏。</a></div>
                        </li>
                        
                        
                        <li class="nav-search ml-3 ml-md-4">
                            <a href="javascript:" data-toggle="modal" data-target="#search-modal"><i
                                    class="iconfont icon-search icon-2x"></i></a>
                        </li>
                        <li class="nav-item d-md-none mobile-menu ml-3 ml-md-4">
                            <a href="javascript:" id="sidebar-switch" data-toggle="modal"
                                data-target="#sidebar"><i class="iconfont icon-classification icon-2x"></i></a>
                        </li>
                    </ul>
                </div>
            </div>
        </div>
        <div class="placeholder" style="height:74px"></div>
    </div>




<body class="page-body boxed-container  io-grey-mode">
    <main role="main" class="flex-shrink-0">
    <div class="container">
        
        <div class="content">
            <style>
    body{
	    background: #f9f9f9;
	}

    h1, h2, h3, h4, h5, h6 {
        margin-top: 1.5rem;
        margin-bottom: 1.5rem;
    }


 
@media (min-width: 1000px) {
  .container, .container-sm {
    max-width: 800px;
  }
}

</style>

<div class="featured-post-content">

    <a href="/digest/" class="featured-post-title">
       AI 文摘
    </a>

</div>

<section class="blog-single">
  <div class="container">
    <div class="row">

      <div class="col-lg-12 order-1 order-lg-2">
        <article class="single-blog">
          <p class="title">大语言模型的数学之路</p>
            <br/>
          <ul class="meta">
            <li>
              By <a href=https://aibard123.com/about>AiBard123</a>
            </li>
            <li>
              <i class="fa fa-clock-o"></i>
              November 3, 2023 - 2 min read
            </li>
          </ul>

          <div class="_1NCGf">
              <img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDFqVRuEmSNBst7xN2ZyPMCjBfibAPG72NSy8AEq9hK27cvj9TyxKEuC2k2K9x88PeLxKviboPz7RDyg/640?wx_fmt=png" width="640" >
          </div>
            <br>
            <br>
            <br>
          
          <div class="single-blog-content">
            <p>作者： PaperWeekly  来源： <a href="https://mp.weixin.qq.com/s/BXQVY7rjlwjAkSxWg_dtcQ">PaperWeekly</a></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_gif/Psho9dm7oDHKVtfYDubjKdZRUjAfBQQicXjoZWJ3qnK42ooD4eeJUfJBM4SSZVa2RE5lO0j6rWwzliby0j9u4bDg/640?wx_fmt=gif" alt=""></p>
<p><strong>©PaperWeekly 原创 · 作者 |</strong> 台运鹏</p>
<p><strong>研究方向 |</strong>  NLP、ML</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_jpg/Psho9dm7oDFqVRuEmSNBst7xN2ZyPMCjxhiaJJ1nv0JxHI54aUd1BhdG2w4lfAALicIKmAnmUMseVpNFRVTFQNcQ/640?wx_fmt=jpeg" alt=""></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDGhKg9nnSz5qQrwKvXibt3wulOVRfC18yCkd6xXqGq22h6QUk8chptF0fnQ4uXeZtAktYMrWwG2SyQ/640?wx_fmt=png" alt=""></p>
<p><strong>问题</strong></p>
<p>LLM 通过大量的语料来建模下一个 token 的概率，这种训练方式促成 LLM 成为一个「文科生」，那么我们不禁对以下几个问题好奇：</p>
<ol>
<li>
<p>LLM 目前在数学问题上取得的进展；</p>
</li>
<li>
<p>有哪些技术路线在未来可能会更进一步提升 LLM 解决数学问题的能力？</p>
</li>
</ol>
<p>在以下部分不妨来讨论上面两个问题。</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDGhKg9nnSz5qQrwKvXibt3wuhfgUpIfdPSqH8YjjHbCUiaaKsMA36bIMsMtGNKoBcus5py06M0fvx3A/640?wx_fmt=png" alt=""></p>
<p><strong>技术路线</strong></p>
<p>那么在目前的模型中，分别有哪些方案呢？这里只会介绍各种路线关于数学的 key points，不会特别关注其他细节。另外 MathGPT 和 Abel 目前没有详细的 report，不会涉及。</p>
<p><strong>2.1 Inference Prompts</strong></p>
<p>LLM 直接输出答案经常会出错，第一条路线是通过 prompt 的不同方法来让模型不断「shift the distribution」，让模型不断调整输出分布，让结果更稳定和准确。</p>
<p><strong>2.1.1 CoT</strong></p>
<p>众所周知，CoT（Chain of Thought）是一种通过让模型一步步思考来提高模型推理能力的，下图右侧是 CoT：</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDFqVRuEmSNBst7xN2ZyPMCjhMF9YbEvFAclG2JQb8B45txlO9Z0yTDuZvjmy2LgxVbh2gCicYcgQLA/640?wx_fmt=png" alt=""></p>
<p>下图横轴代表参数规模，当时通过 CoT 在 PaLM 上甚至超过了监督训练的 SOTA，CoT 这种方法就感觉像是「refine 模型输出的分布」，一步步思考也就意味着输出尽可能往正确的上面靠，相比之下，直接输出就像是「一锤定音」。</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDFqVRuEmSNBst7xN2ZyPMCjiblc5hYJSNkPM49DEIiayoteT1icp2eyeucbbsozjF5k4ds6LiaO8Uziagg/640?wx_fmt=png" alt=""></p>
<p><strong>2.1.2 Self-Verification</strong></p>
<p>这篇工作就是进行反向验证，是比较符合人类思考问题的直觉。</p>
<p>举个例子来说，我们问模型一个问题，模型回答了两种答案：</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDFqVRuEmSNBst7xN2ZyPMCjv017RwzGKibRW2h3766VuW0fwYsEDAT0RULV5j3JxOBLw3jAib5n7TKA/640?wx_fmt=png" alt=""></p>
<p>接下来我们将这个答案作为已知量，将题干中的变量当做未知，去反推，发现不一致，则说明答案有误。</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDFqVRuEmSNBst7xN2ZyPMCjgOzfknY7gclADUy9DWibpOeUu2hsh92XndHTW6hVLaRlRnAuDwgbjYg/640?wx_fmt=png" alt=""></p>
<p><strong>2.1.3 FORBAR</strong></p>
<p>FORBAR 代指的是 Forward-Backward Reasoning，跟上面几乎一致，可以看个例子：</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDFqVRuEmSNBst7xN2ZyPMCjwJfjxNlWTujHsHGNZ8libzF3YsT4d7HWWluvoKSH0wPJDd3Jz8kDlLg/640?wx_fmt=png" alt=""></p>
<p>Forward 就是我们之前让模型直接算的，而 Backward 的意思就是我把题干中的一个变量给遮盖掉，变成 x，而我告诉你答案，我要反问你 x 是啥。</p>
<p>不同的是 Self-Verification 是用以验证，不能扩充成为 Examples，而 FORBAR 就可以作为 Examples。</p>
<p>效果是相当不错的：</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDFqVRuEmSNBst7xN2ZyPMCjSvmrJ85iaxIuia5rWtJrkVIK9OHrBZticcdDQh8kvJErUPMP7S2WcMXBw/640?wx_fmt=png" alt=""></p>
<p><strong>2.1.4 PoT</strong></p>
<p>Program of Thought 是不同于 CoT 的工作，出发点也很简单，比较复杂的数值推理任务，靠一步步推理，其实 LLM 还是会搞错。</p>
<p>那就有人想了，能不能不让 LLM 来计算，让 LLM 学会调用 Python 程序来搞呢？程序计算肯定比 LLM 自己来靠谱，这就是 PoT 干的事情：</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDFqVRuEmSNBst7xN2ZyPMCjknuicJamqkQ5OapNYzkyWiaWdxgxiaHrWgvDqHZlnVL4BGjbXOmzIhVUg/640?wx_fmt=png" alt=""></p>
<p>效果也是会比 CoT 好上一些，下图是 Few-Shot 的结果：</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDFqVRuEmSNBst7xN2ZyPMCjTqESkNZ8vDb3nAa54DddZXHW5xNyDvcTcxKN68PLy8QOZZdURUyPXA/640?wx_fmt=png" alt=""></p>
<p><strong>2.1.5 Majority Voting</strong></p>
<p>多数投票法（或称之为 self-consistency）是在 CoT 的基础上，对结果进行投票，在推理时，我们采样多个可能的解决方案，然后看最后那个预测结果出现最多，就选哪个。</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDFqVRuEmSNBst7xN2ZyPMCj35YDJQibMeYqS4LiaIdCvd346csuZic9nibRNzaxuYTxMSMC9LAHtVQRibg/640?wx_fmt=png" alt=""></p>
<p>结果也是比较显著的，比单独的 CoT 要好：</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDFqVRuEmSNBst7xN2ZyPMCjf6H1bicGFKdicFmffop1jEdAlgc1mju6vTV5hAfVibvaicsjicZMDQAIH2w/640?wx_fmt=png" alt=""></p>
<p><strong>2.1.6 Work Memory</strong></p>
<p>Galactica 提出了「工作记忆」的概念，先引入人类思考的习惯，当我让你求几个数字的平均值时，比如 43, 29, 51, 13，你会写一些过程在纸上，这种写出来的称之为 External Work Memory，即为外在工作记忆。</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDFqVRuEmSNBst7xN2ZyPMCjFtJJU4Fx1knAg6HnDXqggplx3iaKx2sTePrAicYgE1J4pYPn3e8A1Khg/640?wx_fmt=png" alt=""></p>
<p>仔细看图中倒数两行的位置，有个 thinking，有些人算 136/4 是可以在脑中完成计算的，这种被称为 Internal Work Memory，而 Galatica 正是将两种记忆方式结合在一起解决问题。让我们看个 Work Memory 的例子：</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDFqVRuEmSNBst7xN2ZyPMCjW960DLkyJbftfveuQxhgibgNsoDxsjJ1vvslXADmYwLJsQSKmN8MNsg/640?wx_fmt=png" alt=""></p>
<p>有人说，乍一看，这不就是 CoT 吗？一步步展开思考，没毛病吧，但你看 calculate 那里，这个过程是可以不需要 LLM 自己得出答案的，它可以只写个代码让计算机执行 Python 程序，而 LLM 做的是下达指令，读取输出即可。</p>
<p>具体而言，它跟 CoT 的区别或者说 CoT 的不足之处在于：</p>
<p>CoT 相当于上面求平均值一直写在纸上的过程，但是有些 low-level 的思考人类是不需要写下来的，而 LLM 做不到这一点，这也是为什么 CoT 会产生看似正确但又模糊的答案，而 Work Memory 是将 internal 的思考交由更准确的工具去做，比如上述算力大小的例子，写程序，然后执行代码，最后 LLM 只需要读取就行了。</p>
<p>这个方法的难点在于数据集的准备，而 Galatica 是通过以下方法来创建：</p>
<ul>
<li>
<p>通过程序来控制一些变量，来生成例子（OneSmallStep）</p>
</li>
<li>
<p>从现成的数据集中拿（Workout, Khan Problems）</p>
</li>
<li>
<p>另一种就是直接用<work>这种模版转换（GSM8k train）</p>
</li>
</ul>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDFqVRuEmSNBst7xN2ZyPMCjXS65LiafajCLWKGGOvSKicP2gmYca79zJ2emSJhMX2Z1s9IYs1HcNmNw/640?wx_fmt=png" alt=""></p>
<p>mCoT 即代表 Majority Voting + CoT 一起的，可以看到 mCoT 还是效果优于工作记忆的，不过工作记忆并没有经过很好的养成，数据集既不大也不多样，后面可以优化的空间肯定更多。</p>
<pre><code>需要注意的是，不能单纯的拿\&lt;work&gt;和 mCoT 对比，因为上面 Galatica 的 prompt 数据集是预训练过的。  
</code></pre>
<p><strong>2.1.7 OPRO</strong></p>
<p>这篇文章比较有新意，目前 LLM 对于 prompt 不稳定，猜测可能是不一致的原因：用人工优化的 prompt 来调整 LLM 内在输出分布，那很自然就有一个想法，能不能让 LLM 自己来优化使用的 prompt 呢？</p>
<p>这就是 OPRO 做的事情，将 LLM 当做是优化器，不同于以往传统的优化任务，有各种公式做约束，OPRO 是用「自然语言描述」做约束，比如 Find the most effective prompt for this problem，具体框架如下：</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDFqVRuEmSNBst7xN2ZyPMCjhRmsxicxHoBGkF3ZusEGcKz7ZWVfa9AGQxTwXuLNXxfqj5TzkFmucCA/640?wx_fmt=png" alt=""></p>
<p>首先是用最开始 prompt，接着让负责优化 LLM 产生出多个 prompt，再根据目标用 负责打分 LLM 进行打分，将打分后的 prompt 和分数一起放进下一次优化中，最后输出分数最高的 prompt。</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDFqVRuEmSNBst7xN2ZyPMCjF4ZxHhA23bMVZljJbCe8EKgLoUXer74rAPWrLDZUqXia0XzVszsdvpA/640?wx_fmt=png" alt=""></p>
<p>下图是 GMS8k 在优化 prompt 过程中的准确度，这里图可能有误解，其实没有 train，只是在优化过程中找到 prompt，然后在 eval set 进行评测准确率。</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDFqVRuEmSNBst7xN2ZyPMCjpIAo85wgLT2SB1a2m7YB2ib7MEQDxaDLdSB2mSlaLsWEOV4yJJgfESw/640?wx_fmt=png" alt=""></p>
<p><strong>2.2 Reward Models</strong></p>
<p>第二条路线是改进 RLHF（Reinforcement Learning from Human Feedback）。</p>
<p><strong>2.2.1 Process Supervision</strong></p>
<blockquote>
<p>We&rsquo;ve trained a model to achieve a new state-of-the-art in mathematical problem solving by rewarding each correct step of reasoning (“process supervision”) instead of simply rewarding the correct final answer (“outcome supervision”).</p>
</blockquote>
<p>目前的奖励模型大多基于「结果监督」，追求让模型产出正确的结果，而这样的弊端在于两点：</p>
<ul>
<li>
<p>模型中间的思考过程是未知的，解释性和可靠性不高</p>
</li>
<li>
<p>无法真正实现 alignment with humans，只是去对齐结果</p>
</li>
</ul>
<blockquote>
<p>In addition to boosting performance relative to outcome supervision, process supervision also has an important alignment benefit: it directly trains the model to produce a chain-of-thought that is endorsed by humans.</p>
</blockquote>
<p>那么 OpenAI 的解决方案即引入「过程监督」，让模型每一步思考都和人类的进行对齐，这样就可以较好地解决以上的两个问题。作者还发现，尽管是过程监督，但只对比结果依然比结果监督要好。</p>
<p>同时 OpenAI 开源了过程监督的数据集 PRM800K，下图中 ORM 代表「Outcome Rewarded Model」，PRM 代表「Process Rewarded Model」。</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDFqVRuEmSNBst7xN2ZyPMCj41zmQDDX8Jf7GUsr0iau7ut4A7xxrGeib12YVRD8xc6c3SmB1ibGTVibbg/640?wx_fmt=png" alt=""></p>
<p>当问题越复杂时，过程监督的优点就逐渐显露出来，结果好的同时更具解释性，可谓是一举两得。</p>
<p><strong>2.2.2 Reinforcement Learning from Evol-Instruct Feedback</strong></p>
<p>Reinforcement Learning from Evol-Instruct Feedback（RLEIF）是 WizardMath 提出的技术，设计比较巧妙，一共分为三步：</p>
<p>第一步：是先用指令微调获得模型 Wizard-E；</p>
<p>第二步：接着模仿 Evol-Instruct 利用 Wizard-E 和 ChatGPT 来扩充训练集，主要是分为两个方面，让问题变得更简单（Downward）和让问题更复杂（Upward），比如加更多限制，复杂化原来的问题。然后用 Wizard-E 进行排序，获得训练 Instruction Reward Model（IRM）的数据集。</p>
<p>再让 Wizard-E 模型来生成解决问题的步骤，用 ChatGPT 来评判每步的正确性，以此来生成训练 Process Supervision 的数据集。</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDFqVRuEmSNBst7xN2ZyPMCj0pY38NEvsDzkrAtPJuiclvicFy4407ySAqGeNLoox8vN3YrOC8ZQG3qQ/640?wx_fmt=png" alt=""></p>
<p>第三步便是用 PPO 联合训练两个奖励模型，具体是将两个获得的奖励相乘，模型的效果也是比较惊艳的，不过在 MATH 数据集上还差点意思（可能是参数规模不够）。</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDFqVRuEmSNBst7xN2ZyPMCjdn60ZZREulg0ASvDAkdHGo05bsO1Hvj00Zguh17ouMHOZsfzvlN4bQ/640?wx_fmt=png" alt=""></p>
<p><strong>2.3 数据入手</strong></p>
<p><strong>2.3.1 Galactica</strong></p>
<p>Galactica 用于预训练的数据量比同等大模型用的要小得多，整个数据集是经过清洗的「科学领域」数据集，包括论文，参考资料，蛋白质序列，化学公式等等，大小为仅有 106 billion tokens。</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDFqVRuEmSNBst7xN2ZyPMCjFRUxYLY22E3kKykWMMd95YSLFrdoribt8lnPwgAOpKmU34ibIQsAfmXQ/640?wx_fmt=png" alt=""></p>
<p>从上述表格可以看出，论文占了大头（83%），令人惊讶的是 CommonCrawl 仅仅占了 1%。</p>
<pre><code>还将prompt数据集加入了预训练之中  
</code></pre>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDFqVRuEmSNBst7xN2ZyPMCjLick17NUP5ZefhiaL8YpPfJ3vCu6wEMLgdfVlC3sCYice92utr0vae9OA/640?wx_fmt=png" alt=""></p>
<p>作者还提到 prompt prertraining 可以增强模型 general 的能力：</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDFqVRuEmSNBst7xN2ZyPMCjBfibAPG72NSy8AEq9hK27cvj9TyxKEuC2k2K9x88PeLxKviboPz7RDyg/640?wx_fmt=png" alt=""></p>
<p><strong>2.3.2 Minerva</strong></p>
<p>无独有偶，Minerva 同样收集了来自 arxiv 上的论文，有趣的是 General 的语料也是占比很少。</p>
<p>另外还收集了带有数学公式的网站：</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDFqVRuEmSNBst7xN2ZyPMCj4hhYwREnNSicaLj6r3eAPtiaNMicpPA0ibBKd6vUkdBibnEnFUXJ33HuQ2Q/640?wx_fmt=png" alt=""></p>
<p>不一样的是利用方式，Minerva 采用的模型基座是 PaLM，然后在收集的数据上进行 Auto-regressive loss 微调。</p>
<p><strong>2.3.3 Rejection Sampling FineTuning</strong></p>
<p>上面的 Galatica 和 Minvera 都是收集数据，是否可以让 LLM 生成数据呢？</p>
<p>RFT（Rejection Sampling FineTuning）是阿里的工作，方法比较符合直觉，直接让微调后的模型来生成不同的例子用以扩充数据集，然后根据一些准则来筛选：</p>
<ol>
<li>
<p>答案不对的</p>
</li>
<li>
<p>计算结果和 Python 结果不一致的</p>
</li>
</ol>
<p>可以看到效果比单纯的 SFT 要好上不少：</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDFqVRuEmSNBst7xN2ZyPMCjjPxR1AuLPH7VwZVYsxLVZzt1HJ69RAwk3nlfSiaw7YOyRwbiczlMCZPg/640?wx_fmt=png" alt=""></p>
<p><strong>2.3.4 AugGSM8K</strong></p>
<p>还是那句话，我们可以用 LLM 来进行数据的扩充，问题是怎么扩充呢？这篇工作针对问题和回复分别进行了增强：</p>
<p>关于问题，分别采用了五种增强策略（见下图左列），这些原则的设计还是比较符合直觉的。</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDFqVRuEmSNBst7xN2ZyPMCjf6C63Y3tPMpDa0CXTvy7mRJMrX0kM6S1QHXOUHibhicv2tNNnUgYxialQ/640?wx_fmt=png" alt=""></p>
<p>对于回复，作者是采用不同的温度来生成多样化的推理路径，当然也会对于明显错误的进行过滤：</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDFqVRuEmSNBst7xN2ZyPMCj9bib9iarm0bO37dH16ym3iajBl0UzaRdBiac1YsHz7noHFeniacNSJSOopg/640?wx_fmt=png" alt=""></p>
<p>数据集概览如下：</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDFqVRuEmSNBst7xN2ZyPMCjeuwaCuoGk2lMDD4BMVKTU0gb1QyX3QiaFX7tAPgr5HshrKn0ziaroXZw/640?wx_fmt=png" alt=""></p>
<p>生成的数据集证明也是有效果的：</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDFqVRuEmSNBst7xN2ZyPMCjJI2ibBfdHZCpicXqZicjS9WyOxZsjPPvefN0tnyUenEv55q6VTgbU6ahw/640?wx_fmt=png" alt=""></p>
<p><strong>2.3.5 MetaMath</strong></p>
<p>这篇文章跟上面的 AugGSM8K 算是同期工作，也是利用 LLM 来对数据集进行增广，然后进行微调。</p>
<p>它主要是对问题进行增广，分为以下几种：</p>
<ul>
<li>
<p>Rephrase Question</p>
</li>
<li>
<p>Self-Verification</p>
</li>
<li>
<p>FORBAR</p>
</li>
</ul>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDFqVRuEmSNBst7xN2ZyPMCj9479mFicqWbbrR0YBRbDlKG0haiahFEIekfMkDeTSaCCLYsbPMgSotAg/640?wx_fmt=png" alt=""></p>
<p>效果的话基本和 AugGSM8K 也差不多，除了 70B 是 QLoRA，其他的参数是 LLaMA 的。</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDFqVRuEmSNBst7xN2ZyPMCj8BGmokHalcEU1zl1wLRlMbUDX29r0nZMGvS5k42pSbSYLQrBeWHaJw/640?wx_fmt=png" alt=""></p>
<p><strong>2.3.6 MAmmoTH</strong></p>
<p>这篇工作是我数据分支线中很喜欢的一篇，其实 RFT 类通过 Bootstrap 来搞数据集的路子，会丧失「通用性」，在 GMS8K 上效果好，而在 MMLU-Math 上就有可能掉很多点；而 Galatica 和 Minvera 付出的代价要很多。</p>
<p>那么 MAmmoTH 就构建了一个数据集：MathInstruct：</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDFqVRuEmSNBst7xN2ZyPMCjA0stRw1Ow1u352RnBOdHJQk049kB3ibLVJkH7A5I1I5vKk9IO95YwBw/640?wx_fmt=png" alt=""></p>
<p>下图是其与其他数据集的对比：</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDFqVRuEmSNBst7xN2ZyPMCj1wwlE6jSDpukoaQMZXp5GvmETQdiaPPmFbu7Gr13KiawQmziaOpic74CPg/640?wx_fmt=png" alt=""></p>
<p>特点有三个：</p>
<ul>
<li>
<p>量大，一共有 200K</p>
</li>
<li>
<p>覆盖比较多的数学领域</p>
</li>
<li>
<p>Prompt 同时包括了 CoT 和 PoT</p>
</li>
</ul>
<p>效果也很顶，比 RFT 以及 Wizard 更具备通用性，是 Llama 就能有如此效果，相信换更大的模型肯定会有不错的提升。</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDFqVRuEmSNBst7xN2ZyPMCjRhsFs2MG7DSguR9W3ichk4icduVibOicsaaeM3L9gkMa2hvmiccsSCr2cibw/640?wx_fmt=png" alt=""></p>
<p><strong>2.4 架构</strong></p>
<p>####<strong>2.4.1 MathGLM</strong></p>
<p>####<strong>2.4.1.1 具备算数能力</strong></p>
<p>这篇工作是相当有趣，它也解决了我的困惑，若 LLM 从一开始就学数值推理，能不能做好？同时，这篇工作给了一个全新的视角来做一个「专家模型」。</p>
<p>首先是其数据构造环节，很有启发性，无论是 CoT，还是工作区记忆，都强调需要把细节尽可能写下来，LLM 不能像人一样跳跃，这篇工作直接将推理的结果改成一步步得出。</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDFqVRuEmSNBst7xN2ZyPMCjibaElVib2xFqVxslczIt4jk0aJTzichjMbHj0UM7FsOIWtLnC30sAmR5g/640?wx_fmt=png" alt=""></p>
<p>接下来选用的主力 backbone 是 2B 的 Transformer-Decoder，你没有看错，这篇文章并没有使用 LLM Backbone，而是用 AutoRegressive loss 直接用上面数据集去训练。</p>
<p>下图的测试例子一共有 9592 条，直接碾压 GPT-4。当然，我认为这里的是裸模型，没加任何操作，如果用好的操作，GPT-4 应该可以做得更好。因为你拿一个垂域和一个通用模型比，至少也应该给 GPT-4 一些更好的 Prompt Method，或者一些上下文学习的例子。</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDFqVRuEmSNBst7xN2ZyPMCjSZMHgL6nxzRHTNQdP2fOSUU4buZnw1cw6lCeayHY94nYrdfpM0m1oQ/640?wx_fmt=png" alt=""></p>
<p>不过这个实验结果证明了以下两点：</p>
<ol>
<li>
<p>Decoder-Only 架构的确可以学习到算数规则；</p>
</li>
<li>
<p>参数的规模不需要那么大，2B 就能有很好的能力。</p>
</li>
</ol>
<p>####<strong>2.4.1.2 通用一点的专家模型</strong></p>
<p>为了让 MathGLM 可以解决文本描述的问题，MathGLM 还需要变得更通用，于是把目光放到了带有描述的数据集，同样的，将原来的解题步骤进行了细化，这样的好处是，既可以学习到数学知识，又可以建模文本，相当于比之前的专家模型更具通用性。</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDFqVRuEmSNBst7xN2ZyPMCjHVRbROiamILYvYye02kJ5fH8VD8PtibL4eyJeaictOrY00xvSDvhQIomA/640?wx_fmt=png" alt=""></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDGhKg9nnSz5qQrwKvXibt3wukOjHSmSsEuRCB0fJu69CtdNgLnvFPDUCgeicOppBKuDvniaD3q8XWQ0Q/640?wx_fmt=png" alt=""></p>
<p><strong>路在何方</strong></p>
<p>那么理想化 LLM 能解决数学问题的标准是什么呢？</p>
<ol>
<li>
<p>题目的解答得正确</p>
</li>
<li>
<p>解题过程正确可解释</p>
</li>
</ol>
<p>这两点是有可能做到的。综合以上的论文，我大胆预测一下未来的 Math LLM 可能的发展趋势，思路肯定是做一个较为通用的「专家模型」，具体怎么做呢？</p>
<p><strong>3.1 数据的收集和处理</strong></p>
<p>数据的收集前面的工作可谓是百花齐放，核心思路就三个：</p>
<ul>
<li>
<p>尽可能人工去搞高质量的数学推理数据集，你像 LLM 在文本领域能成功，肯定离不开大量的高质量数据集</p>
</li>
<li>
<p>如果特别强领域特性的没有，就找更接近的，比如论文这种科学领域的语料</p>
</li>
<li>
<p>实在不行，就想办法让 LLM 来自我产生数据集（Bootstrap），但这种很依赖于模型，且会引入模型的内在 Bias，但不失为提升模型的手段</p>
</li>
</ul>
<p>数据的处理这块看着比较简单，其实不然。</p>
<p>第一个原则：detailed，把 LLM 当成弱智，越详细越好。比如 MetaMath 就将答案改写成一步步过程；</p>
<p>第二个原则：保留不同的模态特征。比如 Minerva 的处理，它将公式单独处理成 LaTeX 中见到的样子，就相当于你用特殊的 token 来包裹公式，用某种方式来提示 LLM。</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDFqVRuEmSNBst7xN2ZyPMCjFxE9FN7g9EcCqoic7vepayIZwPOCM4pwwswF63lA15DTEic5D1GX7lug/640?wx_fmt=png" alt=""></p>
<p><strong>3.2 垂域模型的路线</strong></p>
<p>这一点其实 MathGLM 和 Galatica 给了我不少启发，但目前有一个问题尚未解决：</p>
<p>如果先预训练 Math LLM，后期去建模文本，究竟能保留多少通用性，或者反过来，如何保留数学的能力，说到底就是 how to be general and specific，如何衡量「通用性」和「专用型」，是值得考量的。</p>
<p>决定之后，其实就可以借鉴这两个模型的路子，比如将 Prompt 数据集直接放入预训练中，以及用 AutoRegressive loss 去建模数值推理的例子等。</p>
<p>预训练完毕，再利用 SFT 或是 RLHF 类方法去进一步微调。</p>
<p>参数的规模我觉得应该不会需要很大，相反，对于数据如何利用是值得考量的，正如在 Galatica 中提到的：作者们发现当重复训练时，性能也会稳步上升，作者将此归因于高质量的数据。</p>
<p>####<strong>3.3 更强的推理方法</strong></p>
<p>从简单的 Zero-Shot 再到 CoT，到 OPRO，经历了太多 Prompt 方法的变迁，我想未来应该很会有，但趋势应该是如何将 LLM 本身的知识引入其中来选择或构造 Prompt，这种一致性带来的提升会更稳定和持久。</p>
<p>####<strong>3.4 最后一个问题</strong></p>
<blockquote>
<p>How to leverage LLM&rsquo;s intrinsic ability to do reasoning?</p>
</blockquote>
<p>这也是我最近一直在想的问题，你看在数据那块，你会发现让模型生成一些例子再放进去推理会比直接推理要好一些，这都是模型自己的能力，有没有什么更优雅的方法可以将这种能力抽离出来。</p>
<p>换句话说，我们对 LLM 本身能力的压榨是不是还有上升的空间？</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_svg/lpHDr05YrITiah4YpYUhEV8J3RPaEujF4ActavpglniaMibQNd0dDvZzdg6EicQqzqNEdWEHL7m5kEanD5FTjjGh3F7s2070v23j/640?wx_fmt=svg" alt=""></p>
<p><strong>参考文献</strong></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_svg/lpHDr05YrITiah4YpYUhEV8J3RPaEujF4ActavpglniaMibQNd0dDvZzdg6EicQqzqNEdWEHL7m5kEanD5FTjjGh3F7s2070v23j/640?wx_fmt=svg" alt=""></p>
<p>[1] <a href="https://openai.com/research/improving-mathematical-reasoning-with-process-supervision">https://openai.com/research/improving-mathematical-reasoning-with-process-supervision</a></p>
<p>[2] <a href="https://galactica.org/static/paper.pdf">https://galactica.org/static/paper.pdf</a></p>
<p>[3] <a href="https://arxiv.org/abs/2203.11171">https://arxiv.org/abs/2203.11171</a></p>
<p>[4] <a href="https://arxiv.org/abs/2201.11903">https://arxiv.org/abs/2201.11903</a></p>
<p>[5] <a href="https://arxiv.org/abs/2206.14858">https://arxiv.org/abs/2206.14858</a></p>
<p>[6] <a href="http://arxiv.org/abs/2309.03409">http://arxiv.org/abs/2309.03409</a></p>
<p>[7] <a href="https://arxiv.org/pdf/2308.09583.pdf">https://arxiv.org/pdf/2308.09583.pdf</a></p>
<p>[8] <a href="https://arxiv.org/abs/2304.12244">https://arxiv.org/abs/2304.12244</a></p>
<p>[9] <a href="http://arxiv.org/abs/2308.01825">http://arxiv.org/abs/2308.01825</a></p>
<p>[10] <a href="https://arxiv.org/abs/2309.05653">https://arxiv.org/abs/2309.05653</a></p>
<p>[11] <a href="https://arxiv.org/abs/2211.12588">https://arxiv.org/abs/2211.12588</a></p>
<p>[12] <a href="http://arxiv.org/abs/2310.05506">http://arxiv.org/abs/2310.05506</a></p>
<p>[13] <a href="http://arxiv.org/abs/2309.03241">http://arxiv.org/abs/2309.03241</a></p>
<p>[14] <a href="https://arxiv.org/abs/2309.12284">https://arxiv.org/abs/2309.12284</a></p>
<p>[15] <a href="https://arxiv.org/abs/2308.07758">https://arxiv.org/abs/2308.07758</a></p>
<p>[16] <a href="https://arxiv.org/abs/2212.09561">https://arxiv.org/abs/2212.09561</a></p>
<p>[17] <a href="https://yunpengtai.top/posts/llm-math/">https://yunpengtai.top/posts/llm-math/</a></p>
<p><strong>更多阅读</strong></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDFqVRuEmSNBst7xN2ZyPMCjExY7U0ttGLHicKcGPcaIkvMdHeWv4EWicENsOpnLYLIsHBn1icw2ceCRg/640?wx_fmt=png" alt=""></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDFqVRuEmSNBst7xN2ZyPMCjvmAQmRy5nmFbgsO68p26YSJ8uK1vZxHibDaWMvkcWThOhEM74FyPwBw/640?wx_fmt=png" alt=""></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDFqVRuEmSNBst7xN2ZyPMCjoOGBkgHPNt3Zj39EXEgcPW9UrlfQC4dCmrQdZZEM3Y1ibqSDPAHESIw/640?wx_fmt=png" alt=""></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_gif/Psho9dm7oDHHMXQ2IicFvJwssWxgWhKuK7ulQVyw7gPTxZia00vCxia2vzhRH6pGq8t1FN1zY48ibULAEZpic41k6eg/640?wx_fmt=gif" alt=""></p>
<p><strong>#投 稿 通 道#</strong></p>
<p>** 让你的文字被更多人看到**</p>
<p>如何才能让更多的优质内容以更短路径到达读者群体，缩短读者寻找优质内容的成本呢？<strong>答案就是：你不认识的人。</strong></p>
<p>总有一些你不认识的人，知道你想知道的东西。PaperWeekly 或许可以成为一座桥梁，促使不同背景、不同方向的学者和学术灵感相互碰撞，迸发出更多的可能性。</p>
<p>PaperWeekly 鼓励高校实验室或个人，在我们的平台上分享各类优质内容，可以是<strong>最新论文解读</strong> ，也可以是<strong>学术热点剖析</strong> 、<strong>科研心得</strong> 或<strong>竞赛经验讲解</strong> 等。我们的目的只有一个，让知识真正流动起来。</p>
<p>📝<strong>稿件基本要求：</strong></p>
<p>• 文章确系个人<strong>原创作品</strong> ，未曾在公开渠道发表，如为其他平台已发表或待发表的文章，请明确标注</p>
<p>• 稿件建议以<strong>markdown</strong>  格式撰写，文中配图以附件形式发送，要求图片清晰，无版权问题</p>
<p>• PaperWeekly 尊重原作者署名权，并将为每篇被采纳的原创首发稿件，提供<strong>业内具有竞争力稿酬</strong> ，具体依据文章阅读量和文章质量阶梯制结算</p>
<p>📬<strong>投稿通道：</strong></p>
<p>• 投稿邮箱：hr@paperweekly.site</p>
<p>• 来稿请备注即时联系方式（微信），以便我们在稿件选用的第一时间联系作者</p>
<p>• 您也可以直接添加小编微信（<strong>pwbot02</strong> ）快速投稿，备注：姓名-投稿</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgmic1CRCSOKfDibC3dZ4BaJuYyYTWJyw8gFxqon34STk3icf9aJbY4rqMpmhNjTGJXIGGFsCdTBHy3Tw/640?wx_fmt=png" alt=""></p>
<p><strong>△长按添加PaperWeekly小编</strong></p>
<p>🔍</p>
<p>现在，在**「知乎」** 也能找到我们了</p>
<p>进入知乎首页搜索**「PaperWeekly」**</p>
<p>点击**「关注」** 订阅我们的专栏吧</p>
<p>·</p>
<p>·</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnZ3nlEAOI3MyTd7jqeD6cq8uTbkM2xZNpribyNr9liaPJ722zaHxd0YpQvib2nxOYmWibydCVY7W94ew/640?wx_fmt=jpeg" alt=""></p>
<p>更多AI工具，参考<a href="https://aibard123.com/">Github-AiBard123</a>，<a href="https://aibard123.com/">国内AiBard123</a></p>



          </div>

可关注我们的公众号：每天AI新工具

<p><img src="/images/aitools/2024/03/qrcode_for_gh_dde1b429630d_258.jpg" alt=""></p>

        </article>

      </div>
    </div>
  </div>
</section>
        </div>
    </div>
    </main>




<script type='text/javascript' src='/assets/js/jquery.ui.touch-punch.min-0.2.2.js' id='jqueryui-touch-js'></script>
<script type='text/javascript' src='/assets/js/clipboard.min-5.6.2.js' id='clipboard-js'></script>
<script type='text/javascript' src='/assets/js/tooltip-extend.js' id='iplaycode-nav-js'></script>
<script type='text/javascript' id='popper-js-extra'>
 

var theme = {"ajaxurl":"","addico":"https:\/\/nav.baidu.cn\/wp-content\/themes\/onenav\/images\/add.png","order":"asc","formpostion":"top","defaultclass":"io-grey-mode","isCustomize":"1","icourl":"","icopng":".png","urlformat":"1","customizemax":"10","newWindow":"0","lazyload":"1","minNav":"1","loading":"1","hotWords":"baidu","classColumns":" col-sm-6 col-md-4 col-xl-5a col-xxl-6a ","apikey":"TWpBeU1UVTNOekk1TWpVMEIvZ1M2bFVIQllUMmxsV1dZelkxQTVPVzB3UW04eldGQmxhM3BNWW14bVNtWk4="};
 
</script>
<script type='text/javascript' src='/assets/js/popper.min.js' id='popper-js'></script>
<script type='text/javascript' src='/assets/js/bootstrap.min-4.3.1.js' id='bootstrap-js'></script>
<script type='text/javascript' src='/assets/js/theia-sticky-sidebar-1.5.0.js' id='sidebar-js'></script>
<script type='text/javascript' src='/assets/js/lazyload.min-12.4.0.js' id='lazyload-js'></script>
<script type='text/javascript' src='/assets/js/fancybox.min-3.5.7.js' id='lightbox-js-js'></script>

<script type='text/javascript' src='/assets/js/app-anim.js' id='appanim-js'></script>

<script type="text/javascript">
    $(document).ready(function(){
        var siteWelcome = $('#loading');
        siteWelcome.addClass('close');
        setTimeout(function() {
            siteWelcome.remove();
        }, 600);
    });
</script>
<script>        
    $(document).ready(function(){
        setTimeout(function () {
            if ($('a.smooth[href="' + window.location.hash + '"]')[0]) {
                $('a.smooth[href="' + window.location.hash + '"]').click();
            }else if (window.location.hash != '') {
                $("html, body").animate({
                    scrollTop: $(window.location.hash).offset().top - 90
                }, {
                    duration: 500,
                    easing: "swing"
                });
            }
        }, 300);
        $(document).on('click','a.smooth',function(ev) {
            if($('#sidebar').hasClass('show') && !$(this).hasClass('change-href')){
                $('#sidebar').modal('toggle');
            }
            if($(this).attr("href").substr(0, 1) == "#"){
                $("html, body").animate({
                    scrollTop: $($(this).attr("href")).offset().top - 90
                }, {
                    duration: 500,
                    easing: "swing"
                });
            }
            if($(this).hasClass('go-search-btn')){
                $('#search-text').focus();
            }
            if(!$(this).hasClass('change-href')){
                var menu =  $("a"+$(this).attr("href"));
                menu.click();
                toTarget(menu.parent().parent(),true,true);
            }
        });
        $(document).on('click','a.tab-noajax',function(ev) {
            var url = $(this).data('link');
            if(url)
                $(this).parents('.d-flex.flex-fill.flex-tab').children('.btn-move.tab-move').show().attr('href', url);
            else
                $(this).parents('.d-flex.flex-fill.flex-tab').children('.btn-move.tab-move').hide();
        });
        
    });
</script>

<script>

(function(){
    if(document.cookie.replace(/(?:(?:^|.*;\s*)night\s*\=\s*([^;]*).*$)|^.*$/, "$1") === ''){
        if(new Date().getHours() > 22 || new Date().getHours() < 6){
            document.body.classList.remove('io-black-mode');
            document.body.classList.add('io-grey-mode');
            document.cookie = "night=1;path=/";
            console.log('夜间模式开启');
        }else{
            document.body.classList.remove('night');
            document.cookie = "night=0;path=/";
            console.log('夜间模式关闭');
        }
    }else{
        var night = document.cookie.replace(/(?:(?:^|.*;\s*)night\s*\=\s*([^;]*).*$)|^.*$/, "$1") || '0';
        if(night == '0'){
            document.body.classList.remove('night');
        }else if(night == '1'){
            document.body.classList.add('night');
        }
    }
})();

$("#search-bg").css("background", "linear-gradient(#e2c4c4, #d8d8d8)");   
function switchNightMode(){
    var night = document.cookie.replace(/(?:(?:^|.*;\s*)night\s*\=\s*([^;]*).*$)|^.*$/, "$1") || '0';
    if(night == '0'){
	$("#search-bg").css("background", "linear-gradient(#e2c4c4, #d8d8d8)");
        document.body.classList.remove('io-grey-mode');
        document.body.classList.add('io-black-mode');
        document.cookie = "night=1;path=/"
        console.log(' ');
        $(".switch-dark-mode").attr("data-original-title","日间模式");
        $(".mode-ico").removeClass("icon-night");
        $(".mode-ico").addClass("icon-light");
    }else{
	$("#search-bg").css("background", "linear-gradient(#4f4040, #1b1d1f)");
        document.body.classList.remove('io-black-mode');
        document.body.classList.add('io-grey-mode');
        document.cookie = "night=0;path=/"
        console.log(' ');
        $(".switch-dark-mode").attr("data-original-title","夜间模式");
        $(".mode-ico").removeClass("icon-light");
        $(".mode-ico").addClass("icon-night");
    }
}
</script>


<script>
    var newsContainer = document.getElementById('news-container');
    var newsItems = document.getElementsByClassName('news-item');
    var currentItem = 0;

    setInterval(function() {
        
        newsItems[currentItem].classList.remove('show');
        newsItems[currentItem].style.transform = 'translateY(-20px)';
        
        currentItem = (currentItem + 1) % newsItems.length;
        newsItems[currentItem].style.transform = 'translateY(' + (newsContainer.offsetHeight - 20) + 'px)';
        setTimeout(function() {
            newsItems[currentItem].classList.add('show');
        }, 500);
    }, 8000);
</script>

</body>
</html>



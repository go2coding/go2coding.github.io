

<!DOCTYPE html>
<html lang="zh-CN">

<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1" />
    <meta name="viewport"
        content="width=device-width, initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no" />
    <meta name="theme-color" content="#f9f9f9" />

	<title>ä»ChatGLM2-6Bæ¥çœ‹å¤§æ¨¡å‹æ‰©å±•ä¸Šä¸‹æ–‡ï¼šFlashAttentionå’ŒåŠ é€Ÿæ¨ç†Multi-QueryAttention ä½œè€…ï¼š AINLP æ¥æºï¼š AINLP ChatGLM2-6B æ˜¯å¼€æºä¸­è‹±åŒè¯­å¯¹è¯æ¨¡å‹ ChatGLM-6B çš„ç¬¬äºŒä»£ç‰ˆæœ¬ï¼Œåœ¨ä¿ç•™äº†åˆä»£æ¨¡å‹å¯¹è¯æµç•…ã€éƒ¨ç½²é—¨æ§›è¾ƒä½ç­‰ä¼—å¤šä¼˜ç§€ç‰¹æ€§çš„åŸºç¡€ä¹‹ä¸Šï¼ŒChatGLM2-6B å¼•å…¥äº†å¦‚ä¸‹æ–°ç‰¹æ€§ï¼š æ›´å¼ºå¤§çš„æ€§èƒ½ï¼šåŸºäº ChatGLM åˆä»£æ¨¡å‹çš„å¼€å‘ç»éªŒï¼Œå…¨é¢å‡çº§äº† ChatGLM2-6B çš„åŸºåº§æ¨¡å‹ã€‚ChatGLM2  | AiBard123| aiå·¥å…·ç½‘å€å¯¼èˆª,aiæœ€æ–°äº§å“</title>
	<link rel="shortcut icon" href="/assets/images/favicon.png" />
    <meta name="keywords" content="chatgpt,AI,AIèŠå¤©,AIæ–‡æœ¬ç”Ÿæˆ,AIç»˜ç”»,AIç¼–ç¨‹,AIç”µå•†" />
    <meta name="description" content="AiBard123 ç½‘å€å¯¼èˆª | å…è´¹chatgpt æ±‡é›†å„ç±»å…ˆè¿›çš„äººå·¥æ™ºèƒ½äº§å“ï¼Œæ—¨åœ¨å¸®åŠ©ç”¨æˆ·æ›´å¿«é€Ÿåœ°äº†è§£å’Œä½¿ç”¨è¿™äº›äº§å“,è½»æ¾åœ°æµè§ˆä¸åŒé¢†åŸŸçš„AIäº§å“ï¼ŒåŒ…æ‹¬è¯­éŸ³è¯†åˆ«ã€å›¾åƒå¤„ç†ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€‚" />
    
    <meta name="baidu-site-verification" content="codeva-cCAOSG8MBO" />
    
    <link rel="stylesheet" id="block-library-css"
        href="/assets/css/block-library.min-5.6.2.css" type="text/css" media="all" />
    <link rel="stylesheet" id="iconfont-css" href="/assets/css/iconfont-3.03029.1.css"
        type="text/css" media="all" />

    
    <link href="/scss/style.min.css" rel="stylesheet" />
    
		    <link rel="stylesheet" id="iowen-css" href="/assets/css/style-3.03029.1.css"
        type="text/css" media="all" />
    <link rel="stylesheet" id="custom-css" href="/assets/css/custom-style.css"
        type="text/css" media="all" />
		
		<link rel="stylesheet" href=/plugins/font-awesome/css/font-awesome.min.css />


    <link rel="stylesheet" id="fortawesome-css" href="/assets/fontawesome-5.15.4/css/all.min.css" type="text/css" />


    <script type="text/javascript" src="/assets/js/jquery.min-3.2.1.js" id="jquery-js"></script>
    <script type="text/javascript" src="/assets/js/content-search.js"  id="content-search-js"></script>

    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2073588164294660"
     crossorigin="anonymous"></script>

	
    <script>
        

		var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?8450bc732b2a86f7e4aec4ebd9fd8252";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();

        
    </script>
    

    
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-7071W80M2K"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-7071W80M2K');
    </script>

</head>


    <div class="page-container">
	
	<div id="sidebar" class="sticky sidebar-nav fade animate-nav" style="width: 170px">
        
            <div class="modal-dialog h-100 sidebar-nav-inner">
                <div class="sidebar-logo border-bottom border-color">
                    
                    <div class="logo overflow-hidden">
                        <a href="https://aibard123.com/" class="logo-expanded">
                            <img src="/assets/images/bt8-expand-light.png" height="40" class="logo-light"
                                alt="AiBard123| aiå·¥å…·ç½‘å€å¯¼èˆª,aiæœ€æ–°äº§å“">
                            <img src="/assets/images/bt8-expand-dark.png" height="40" class="logo-dark d-none"
                                alt="AiBard123| aiå·¥å…·ç½‘å€å¯¼èˆª,aiæœ€æ–°äº§å“">
                        </a>
                        <a href="https://aibard123.com/" class="logo-collapsed">
                            <img src="/assets/images/bt.png" height="40" class="logo-light"
                                alt="AiBard123| aiå·¥å…·ç½‘å€å¯¼èˆª,aiæœ€æ–°äº§å“">
                            <img src="/assets/images/bt.png" height="40" class="logo-dark d-none"
                                alt="AiBard123| aiå·¥å…·ç½‘å€å¯¼èˆª,aiæœ€æ–°äº§å“">
                        </a>
                    </div>
                    
                </div>
                <div class="sidebar-menu flex-fill">
                    <div class="sidebar-scroll">
                        <div class="sidebar-menu-inner">
                            <ul>
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#00834a9dd147b04c5d53d4368cdb0b57" class="smooth">
                                            <i class="fas fa-sun fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>æœ¬æœˆçƒ­é—¨</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#db0311e7ecfedd24d157f0ceb4a0897f" class="smooth">
                                            <i class="fas fa-star-and-crescent fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>çƒ­é—¨ç½‘ç«™</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#21b5cbb2c769010fec3ce029a5f8a4a3" class="smooth">
                                            <i class="far fa-star fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>å›½å†…çƒ­é—¨</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#8310718935e8ec25ce0350de01e3f7dc" class="smooth">
                                            <i class="fas fa-phone fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>å¯¹è¯å·¥å…·</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#d58e850d9115797306c2edf61ac6ddd8" class="smooth">
                                            <i class="fas fa-newspaper fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>å†™ä½œ</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#2a7418a5f8f1ca4e054364a9300657df" class="smooth">
                                            <i class="fas fa-image fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>å›¾åƒç”Ÿæˆ</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#7808a68ee1b34dab43011429a12de19e" class="smooth">
                                            <i class="fas fa-image fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>å›¾åƒå¤„ç†</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#6729afc51f5ac49a828812fa0eb0c82f" class="smooth">
                                            <i class="fas fa-video fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>éŸ³è§†é¢‘</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#e5ce844860451fff3faf3d8f8894971d" class="smooth">
                                            <i class="fas fa-music fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>éŸ³ä¹ç”Ÿæˆ</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#db53804b7d726967c58fcc8c9ca03d27" class="smooth">
                                            <i class="fas fa-language fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>åŠå…¬</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#47b7af9547e034d28fe6f6d439968ac8" class="smooth">
                                            <i class="fas fa-copy fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>æç¤ºè¯</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#41282bf95e43c64d579757573a03cdde" class="smooth">
                                            <i class="fas fa-code fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>ç¼–ç¨‹</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#fd71852fd52d5e18ef4f9a252f1eac58" class="smooth">
                                            <i class="fas fa-search fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>AIæœç´¢</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#81b1637fbe47625dbdf2094acd3b6683" class="smooth">
                                            <i class="fas fa-language fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>æ–‡æœ¬ç¿»è¯‘</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#2e9ba3fa6e1ed0e9311b3e97f97f9a40" class="smooth">
                                            <i class="fas fa-book fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>å­¦ä¹ ç½‘ç«™</span>
                                        </a>
                                    </li>
                                    
                                
                            </ul>           
                        </div>
                    </div>
                </div>
                <div class="border-top py-2 border-color">
                    <div class="flex-bottom">
                        <ul>
			    <li id="menu-item-212"
                                 class="menu-item menu-item-type-custom menu-item-object-custom menu-item-212 sidebar-item">
                                 <a href="#friendlink" class="smooth">
                                     <i class="fab fa-staylinked icon-fw icon-lg mr-2"></i>
                                     <span>å‹æƒ…é“¾æ¥</span>
                                 </a>
                            </li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
    </div>


<div class="flex-fill grid-bg">
    <div class="big-header-banner">
        <div id="header" class="page-header sticky">
            <div class="navbar navbar-expand-md">
                <div class="container-fluid p-0">

                    <a href="" class="navbar-brand d-md-none" title="AiBard123| aiå·¥å…·ç½‘å€å¯¼èˆª,aiæœ€æ–°äº§å“">
                        <img src="/assets/images/bt.png" class="logo-light"
                            alt="AiBard123| aiå·¥å…·ç½‘å€å¯¼èˆª,aiæœ€æ–°äº§å“">
                        <img src="/assets/images/bt.png" class="logo-dark d-none"
                            alt="AiBard123| aiå·¥å…·ç½‘å€å¯¼èˆª,aiæœ€æ–°äº§å“">
                    </a>

                    <div class="collapse navbar-collapse order-2 order-md-1">
                        <div class="header-mini-btn">
                            <label>
                                <input id="mini-button" type="checkbox">
                                <svg viewbox="0 0 100 100" xmlns="http://www.w3.org/2000/svg">
                                    <path class="line--1" d="M0 40h62c18 0 18-20-17 5L31 55"></path>
                                    <path class="line--2" d="M0 50h80"></path>
                                    <path class="line--3" d="M0 60h62c18 0 18 20-17-5L31 45"></path>
                                </svg>
                            </label>

                        </div>

                        <ul class="navbar-nav site-menu" style="margin-right: 16px;">
                        
			<li >
				<a href="/">
                                    <i class="fa fa-home fa-lg mr-2"></i>
                                    <span>é¦–é¡µ</span>
                                </a>
				<ul class="sub-menu">
				
				</ul>
			    </li>
			
			</ul>

                        
                        <div class="rounded-circle weather">
                            <div id="he-plugin-simple" style="display: contents;"></div>
                            <script>WIDGET = {
                                    CONFIG: {
                                        "modules": "01234",
                                        "background": 5,
                                        "tmpColor": "008000",
                                        "tmpSize": 14,
                                        "cityColor": "008000",
                                        "citySize": 14,
                                        "aqiColor": "#008000",
                                        "aqiSize": 14,
                                        "weatherIconSize": 24,
                                        "alertIconSize": 18,
                                        "padding": "10px 10px 10px 10px",
                                        "shadow": "1",
                                        "language": "auto",
                                        "borderRadius": 5,
                                        "fixed": "false",
                                        "vertical": "middle",
                                        "horizontal": "left",
                                        "key": "085791e805a24491b43b06cf58ab31e7"
                                    }
                                }
                            </script>
                            <script src="https://widget.qweather.net/simple/static/js/he-simple-common.js?v=2.0"></script>
                        </div>
                        
                    </div>

                    <ul class="nav navbar-menu text-xs order-1 order-md-2">
                        
                        
                        <li class="nav-item mr-3 mr-lg-0 d-none d-lg-block">
                            <script>
                                fetch('https://v1.hitokoto.cn')
                                    .then(response => response.json())
                                    .then(data => {
                                    const hitokoto = document.getElementById('hitokoto_text')
                                    hitokoto.href = 'https://hitokoto.cn/?uuid=' + data.uuid
                                    hitokoto.innerText = data.hitokoto
                                    })
                                    .catch(console.error)
                            </script>                           
                            <div id="hitokoto"><a href="#" target="_blank" id="hitokoto_text">ç–å½±æ¨ªæ–œæ°´æ¸…æµ…ï¼Œæš—é¦™æµ®åŠ¨æœˆé»„æ˜ã€‚</a></div>
                        </li>
                        
                        
                        <li class="nav-search ml-3 ml-md-4">
                            <a href="javascript:" data-toggle="modal" data-target="#search-modal"><i
                                    class="iconfont icon-search icon-2x"></i></a>
                        </li>
                        <li class="nav-item d-md-none mobile-menu ml-3 ml-md-4">
                            <a href="javascript:" id="sidebar-switch" data-toggle="modal"
                                data-target="#sidebar"><i class="iconfont icon-classification icon-2x"></i></a>
                        </li>
                    </ul>
                </div>
            </div>
        </div>
        <div class="placeholder" style="height:74px"></div>
    </div>




<body class="page-body boxed-container  io-grey-mode">
    <main role="main" class="flex-shrink-0">
    <div class="container">
        
        <div class="content">
            <style>
    body{
	    background: #f9f9f9;
	}

    h1, h2, h3, h4, h5, h6 {
        margin-top: 1.5rem;
        margin-bottom: 1.5rem;
    }


 
@media (min-width: 1000px) {
  .container, .container-sm {
    max-width: 800px;
  }
}

</style>

<div class="featured-post-content">

    <a href="/digest/" class="featured-post-title">
       AI æ–‡æ‘˜
    </a>

</div>

<section class="blog-single">
  <div class="container">
    <div class="row">

      <div class="col-lg-12 order-1 order-lg-2">
        <article class="single-blog">
          <p class="title">ä»ChatGLM2-6Bæ¥çœ‹å¤§æ¨¡å‹æ‰©å±•ä¸Šä¸‹æ–‡ï¼šFlashAttentionå’ŒåŠ é€Ÿæ¨ç†Multi-QueryAttention</p>
            <br/>
          <ul class="meta">
            <li>
              By <a href=https://aibard123.com/about>AiBard123</a>
            </li>
            <li>
              <i class="fa fa-clock-o"></i>
              December 7, 2023 - 2 min read
            </li>
          </ul>

          <div class="_1NCGf">
              <img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_jpg/N5aX12H1SickXvicAjKq5rMoFUic88u8pJ3N5wiaD1SCjXHtZEiaJUN3NQYyNlj6xdyZ1ic7pGarLBdaRv4HSj3u7TtQ/640?wx_fmt=jpeg" width="640" >
          </div>
            <br>
            <br>
            <br>
          
          <div class="single-blog-content">
            <p>ä½œè€…ï¼š AINLP  æ¥æºï¼š <a href="https://mp.weixin.qq.com/s/RJDfxgOGOqdFSq5907eoEw">AINLP</a></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_jpg/nW2ZPfuYqSJuK8UUBxdZXj1c20hUg374YPgXibgDGytAy87YxvVk4WCRFWrdKJPshStrlPJp4vGEGUQodxt7ibOw/640?wx_fmt=jpeg" alt=""></p>
<p>ChatGLM2-6B æ˜¯å¼€æºä¸­è‹±åŒè¯­å¯¹è¯æ¨¡å‹ ChatGLM-6B çš„ç¬¬äºŒä»£ç‰ˆæœ¬ï¼Œåœ¨ä¿ç•™äº†åˆä»£æ¨¡å‹å¯¹è¯æµç•…ã€éƒ¨ç½²é—¨æ§›è¾ƒä½ç­‰ä¼—å¤šä¼˜ç§€ç‰¹æ€§çš„åŸºç¡€ä¹‹ä¸Šï¼ŒChatGLM2-6B å¼•å…¥äº†å¦‚ä¸‹æ–°ç‰¹æ€§ï¼š</p>
<ol>
<li>
<p>æ›´å¼ºå¤§çš„æ€§èƒ½ï¼šåŸºäº ChatGLM åˆä»£æ¨¡å‹çš„å¼€å‘ç»éªŒï¼Œå…¨é¢å‡çº§äº† ChatGLM2-6B çš„åŸºåº§æ¨¡å‹ã€‚ChatGLM2-6B ä½¿ç”¨äº† GLM çš„æ··åˆç›®æ ‡å‡½æ•°ï¼Œç»è¿‡äº† 1.4T ä¸­è‹±æ ‡è¯†ç¬¦çš„é¢„è®­ç»ƒä¸äººç±»åå¥½å¯¹é½è®­ç»ƒï¼Œè¯„æµ‹ç»“æœæ˜¾ç¤ºï¼Œç›¸æ¯”äºåˆä»£æ¨¡å‹ï¼ŒChatGLM2-6B åœ¨ MMLUï¼ˆ+23%ï¼‰ã€CEvalï¼ˆ+33%ï¼‰ã€GSM8Kï¼ˆ+571%ï¼‰ ã€BBHï¼ˆ+60%ï¼‰ç­‰æ•°æ®é›†ä¸Šçš„æ€§èƒ½å–å¾—äº†å¤§å¹…åº¦çš„æå‡ï¼Œåœ¨åŒå°ºå¯¸å¼€æºæ¨¡å‹ä¸­å…·æœ‰è¾ƒå¼ºçš„ç«äº‰åŠ›ã€‚</p>
</li>
<li>
<p>æ›´é•¿çš„ä¸Šä¸‹æ–‡ï¼šåŸºäº<strong>FlashAttention</strong>  æŠ€æœ¯ï¼Œå°†åŸºåº§æ¨¡å‹çš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆContext Lengthï¼‰ç”± ChatGLM-6B çš„ 2K æ‰©å±•åˆ°äº† 32Kï¼Œå¹¶åœ¨å¯¹è¯é˜¶æ®µä½¿ç”¨ 8K çš„ä¸Šä¸‹æ–‡é•¿åº¦è®­ç»ƒã€‚å¯¹äºæ›´é•¿çš„ä¸Šä¸‹æ–‡ï¼Œå‘å¸ƒäº† ChatGLM2-6B-32K æ¨¡å‹ã€‚LongBench çš„æµ‹è¯„ç»“æœè¡¨æ˜ï¼Œåœ¨ç­‰é‡çº§çš„å¼€æºæ¨¡å‹ä¸­ï¼ŒChatGLM2-6B-32K æœ‰ç€è¾ƒä¸ºæ˜æ˜¾çš„ç«äº‰ä¼˜åŠ¿ã€‚</p>
</li>
<li>
<p>æ›´é«˜æ•ˆçš„æ¨ç†ï¼šåŸºäº<strong>Multi-Query Attention</strong> æŠ€æœ¯ï¼ŒChatGLM2-6B æœ‰æ›´é«˜æ•ˆçš„æ¨ç†é€Ÿåº¦å’Œæ›´ä½çš„æ˜¾å­˜å ç”¨ï¼šåœ¨å®˜æ–¹çš„æ¨¡å‹å®ç°ä¸‹ï¼Œæ¨ç†é€Ÿåº¦ç›¸æ¯”åˆä»£æå‡äº† 42%ï¼ŒINT4 é‡åŒ–ä¸‹ï¼Œ6G æ˜¾å­˜æ”¯æŒçš„å¯¹è¯é•¿åº¦ç”± 1K æå‡åˆ°äº† 8Kã€‚</p>
</li>
<li>
<p>æ›´å¼€æ”¾çš„åè®®ï¼šChatGLM2-6B æƒé‡å¯¹å­¦æœ¯ç ”ç©¶å®Œå…¨å¼€æ”¾ï¼Œåœ¨å¡«å†™é—®å·è¿›è¡Œç™»è®°åäº¦å…è®¸å…è´¹å•†ä¸šä½¿ç”¨ã€‚</p>
</li>
</ol>
<p>####<strong>ä¸€ã€ChatGLM2-6Bè¯„æµ‹ç»“æœ</strong></p>
<p>ä¸‹é¢æ˜¯ChatGLM2-6B æ¨¡å‹åœ¨ MMLU (è‹±æ–‡)ã€C-Evalï¼ˆä¸­æ–‡ï¼‰ã€GSM8Kï¼ˆæ•°å­¦ï¼‰ã€BBHï¼ˆè‹±æ–‡ï¼‰ ä¸Šçš„æµ‹è¯„ç»“æœã€‚åœ¨ evaluation ä¸­æä¾›äº†åœ¨ C-Eval ä¸Šè¿›è¡Œæµ‹è¯„çš„è„šæœ¬ã€‚</p>
<h4 id="mmlu">MMLU</h4>
<p>ModelAverageSTEMSocial SciencesHumanitiesOthers</p>
<p>ChatGLM-6B
40.63
33.89
44.84
39.02
45.71</p>
<p>ChatGLM2-6B (base)
47.86
41.20
54.44
43.66
54.46</p>
<p>ChatGLM2-6B
45.46
40.06
51.61
41.23
51.24</p>
<p>ChatGLM2-12B (base)
56.18
48.18
65.13
52.58
60.93</p>
<p>ChatGLM2-12B
52.13
47.00
61.00
46.10
56.05</p>
<blockquote>
<p>Chat æ¨¡å‹ä½¿ç”¨ zero-shot CoT (Chain-of-Thought) çš„æ–¹æ³•æµ‹è¯•ï¼ŒBase æ¨¡å‹ä½¿ç”¨ few-shot answer-only çš„æ–¹æ³•æµ‹è¯•</p>
</blockquote>
<h4 id="c-eval">C-Eval</h4>
<p>ModelAverageSTEMSocial SciencesHumanitiesOthers</p>
<p>ChatGLM-6B
38.9
33.3
48.3
41.3
38.0</p>
<p>ChatGLM2-6B (base)
51.7
48.6
60.5
51.3
49.8</p>
<p>ChatGLM2-6B
50.1
46.4
60.4
50.6
46.9</p>
<p>ChatGLM2-12B (base)
61.6
55.4
73.7
64.2
59.4</p>
<p>ChatGLM2-12B
57.0
52.1
69.3
58.5
53.2</p>
<blockquote>
<p>Chat æ¨¡å‹ä½¿ç”¨ zero-shot CoT çš„æ–¹æ³•æµ‹è¯•ï¼ŒBase æ¨¡å‹ä½¿ç”¨ few-shot answer only çš„æ–¹æ³•æµ‹è¯•</p>
</blockquote>
<h4 id="gsm8k">GSM8K</h4>
<p>ModelAccuracyAccuracy (Chinese)*</p>
<p>ChatGLM-6B
4.82
5.85</p>
<p>ChatGLM2-6B (base)
32.37
28.95</p>
<p>ChatGLM2-6B
28.05
20.45</p>
<p>ChatGLM2-12B (base)
40.94
42.71</p>
<p>ChatGLM2-12B
38.13
23.43</p>
<blockquote>
<p>æ‰€æœ‰æ¨¡å‹å‡ä½¿ç”¨ few-shot CoT çš„æ–¹æ³•æµ‹è¯•ï¼ŒCoT prompt æ¥è‡ª <a href="http://arxiv.org/abs/2201.11903">http://arxiv.org/abs/2201.11903</a></p>
<p>ä½¿ç”¨ç¿»è¯‘ API ç¿»è¯‘äº† GSM8K ä¸­çš„ 500 é“é¢˜ç›®å’Œ CoT prompt å¹¶è¿›è¡Œäº†äººå·¥æ ¡å¯¹</p>
</blockquote>
<h4 id="bbh">BBH</h4>
<p>ModelAccuracy</p>
<p>ChatGLM-6B
18.73</p>
<p>ChatGLM2-6B (base)
33.68</p>
<p>ChatGLM2-6B
30.00</p>
<p>ChatGLM2-12B (base)
36.02</p>
<p>ChatGLM2-12B
39.98</p>
<blockquote>
<p>æ‰€æœ‰æ¨¡å‹å‡ä½¿ç”¨ few-shot CoT çš„æ–¹æ³•æµ‹è¯•ï¼ŒCoT prompt æ¥è‡ª <a href="https://github.com/suzgunmirac/BIG-Bench-Hard/tree/main/cot-prompts">https://github.com/suzgunmirac/BIG-Bench-Hard/tree/main/cot-prompts</a></p>
</blockquote>
<p>####<strong>äºŒã€æ¨ç†æ€§èƒ½</strong></p>
<p>ChatGLM2-6B ä½¿ç”¨äº† Multi-Query Attentionï¼Œæé«˜äº†ç”Ÿæˆé€Ÿåº¦ã€‚ç”Ÿæˆ 2000 ä¸ªå­—ç¬¦çš„å¹³å‡é€Ÿåº¦å¯¹æ¯”å¦‚ä¸‹</p>
<p>Modelæ¨ç†é€Ÿåº¦ (å­—ç¬¦/ç§’)</p>
<p>ChatGLM-6B
31.49</p>
<p>ChatGLM2-6B
44.62</p>
<blockquote>
<p>ä½¿ç”¨å®˜æ–¹å®ç°ï¼Œbatch size = 1ï¼Œmax length = 2048ï¼Œbf16 ç²¾åº¦ï¼Œæµ‹è¯•ç¡¬ä»¶ä¸º A100-SXM4-80Gï¼Œè½¯ä»¶ç¯å¢ƒä¸º PyTorch 2.0.1</p>
</blockquote>
<p>Multi-Query Attention åŒæ—¶ä¹Ÿé™ä½äº†ç”Ÿæˆè¿‡ç¨‹ä¸­ KV Cache çš„æ˜¾å­˜å ç”¨ï¼Œæ­¤å¤–ï¼ŒChatGLM2-6B é‡‡ç”¨ Causal Mask è¿›è¡Œå¯¹è¯è®­ç»ƒï¼Œè¿ç»­å¯¹è¯æ—¶å¯å¤ç”¨å‰é¢è½®æ¬¡çš„ KV Cacheï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–äº†æ˜¾å­˜å ç”¨ã€‚å› æ­¤ï¼Œä½¿ç”¨ 6GB æ˜¾å­˜çš„æ˜¾å¡è¿›è¡Œ INT4 é‡åŒ–çš„æ¨ç†æ—¶ï¼Œåˆä»£çš„ ChatGLM-6B æ¨¡å‹æœ€å¤šèƒ½å¤Ÿç”Ÿæˆ 1119 ä¸ªå­—ç¬¦å°±ä¼šæç¤ºæ˜¾å­˜è€—å°½ï¼Œè€Œ ChatGLM2-6B èƒ½å¤Ÿç”Ÿæˆè‡³å°‘ 8192 ä¸ªå­—ç¬¦ã€‚</p>
<p>é‡åŒ–ç­‰çº§ç¼–ç  2048 é•¿åº¦çš„æœ€å°æ˜¾å­˜ç”Ÿæˆ 8192 é•¿åº¦çš„æœ€å°æ˜¾å­˜</p>
<p>FP16 / BF16
13.1 GB
12.8 GB</p>
<p>INT8
8.2 GB
8.1 GB</p>
<p>INT4
5.5 GB
5.1 GB</p>
<blockquote>
<p>ChatGLM2-6B åˆ©ç”¨äº† PyTorch 2.0 å¼•å…¥çš„ torch.nn.functional.scaled_dot_product_attention
å®ç°é«˜æ•ˆçš„ Attention è®¡ç®—ï¼Œå¦‚æœ PyTorch ç‰ˆæœ¬è¾ƒä½åˆ™ä¼š fallback åˆ°æœ´ç´ çš„ Attention å®ç°ï¼Œå‡ºç°æ˜¾å­˜å ç”¨é«˜äºä¸Šè¡¨çš„æƒ…å†µã€‚</p>
</blockquote>
<p>é‡åŒ–å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“å¦‚ä¸‹ï¼ŒåŸºæœ¬åœ¨å¯æ¥å—èŒƒå›´å†…ã€‚</p>
<p>é‡åŒ–ç­‰çº§Accuracy (MMLU)Accuracy (C-Eval dev)</p>
<p>BF16
45.47
53.57</p>
<p>INT4
43.13
50.30</p>
<p><strong>ä¸‰ã€Multi-Query-Attention</strong>**(MQA)**</p>
<p><strong>è®ºæ–‡åœ°å€</strong> ï¼šhttps://arxiv.org/pdf/1911.02150.pdf</p>
<p>MQA æ˜¯ 19 å¹´æå‡ºçš„ä¸€ç§æ–°çš„ Attention æœºåˆ¶ï¼Œå…¶èƒ½å¤Ÿåœ¨ä¿è¯æ¨¡å‹æ•ˆæœçš„åŒæ—¶åŠ å¿« decoder ç”Ÿæˆ token çš„é€Ÿåº¦ï¼Œå› æ­¤åœ¨ç›®å‰å¤§æ¨¡å‹æ—¶ä»£è¢«å¹¿æ³›åº”ç”¨ã€‚ä¸‹é¢çœ‹ä¸€ä¸‹è®ºæ–‡çš„å®éªŒæ•ˆæœï¼š</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_jpg/N5aX12H1SickXvicAjKq5rMoFUic88u8pJ3sIzSsJAlmb5l97elDR8AVDj9HoOUWTwObdYTp2uUAdgiaCfu0KytQ4g/640?wx_fmt=jpeg" alt=""></p>
<p>ä»ä¸Šå›¾è¡¨ä¸­å¯ä»¥çœ‹åˆ°ï¼ŒMQA åœ¨ encoder ä¸Šçš„æé€Ÿæ²¡æœ‰éå¸¸æ˜æ˜¾ï¼Œä½†åœ¨ decoder ä¸Šçš„æé€Ÿæ˜¯å¾ˆæ˜¾è‘—çš„ã€‚</p>
<p>ä¼ ç»Ÿçš„Transformeræ˜¯Multi Head Attentionï¼ˆMHAï¼‰ç»“æ„ï¼Œæ¯ä¸ª head åˆæ˜¯ç”±ï¼š queryï¼ˆQï¼‰ï¼Œkeyï¼ˆKï¼‰ï¼Œvalueï¼ˆVï¼‰ 3 ä¸ªçŸ©é˜µå…±åŒå®ç°çš„ï¼Œè¿™ä¸‰ä¸ªçŸ©é˜µçš„å‚æ•°éƒ½æ˜¯ç‹¬ç«‹çš„ï¼Œè€ŒMQA è®©æ‰€æœ‰çš„å¤´ä¹‹é—´ å…±äº« åŒä¸€ä»½ Key å’Œ Value çŸ©é˜µï¼Œæ¯ä¸ªå¤´<strong>åªå•ç‹¬ä¿ç•™äº†ä¸€ä»½ Query å‚æ•°</strong> ï¼Œä»è€Œå¤§å¤§<strong>å‡å°‘ Key å’Œ Value çŸ©é˜µçš„å‚æ•°é‡</strong> ã€‚</p>
<p>ä»–ä»¬çš„å…³é”®åŒºåˆ«åœ¨äºWqkvçš„å®ç°ä¸Šï¼Œä¸‹é¢å±•ç¤ºä¸€ä¸‹ä»£ç ç¤ºä¾‹ï¼š</p>
<hr>
<pre><code># Multi Head Attention
self.Wqkv = nn.Linear(                        # ã€å…³é”®ã€‘Multi-Head Attention çš„åˆ›å»ºæ–¹æ³•
    self.d_model, 
    3 * self.d_model,                         # æœ‰ query, key, value 3 ä¸ªçŸ©é˜µ, æ‰€ä»¥æ˜¯ 3 * d_model
    device=device
)
query, key, value = qkv.chunk(                # ã€å…³é”®ã€‘æ¯ä¸ª tensor éƒ½æ˜¯ (1, 512, 768)
    3, 
    dim=2
)
  

  

# Multi Query Attention
self.Wqkv = nn.Linear(                                # ã€å…³é”®ã€‘Multi-Query Attention çš„åˆ›å»ºæ–¹æ³•
    d_model,
    d_model + 2 * self.head_dim,                      # åªåˆ›å»º query çš„ head å‘é‡ï¼Œæ‰€ä»¥åªæœ‰ 1 ä¸ª d_model
    device=device,                                    # è€Œ key å’Œ value ä¸å†å…·å¤‡å•ç‹¬çš„å¤´å‘é‡
)
  

query, key, value = qkv.split(                        # query -&gt; (1, 512, 768)
    [self.d_model, self.head_dim, self.head_dim],     # key   -&gt; (1, 512, 96)
    dim=2                                             # value -&gt; (1, 512, 96)
)
</code></pre>
<p>åœ¨ MHA ä¸­ï¼Œquery, key, value æ¯ä¸ªå‘é‡å‡æœ‰ 768 ç»´åº¦ï¼›è€Œåœ¨ MQA ä¸­ï¼Œåªæœ‰ query æ˜¯ 768 ç»´ï¼Œè€Œ key å’Œ value åªæœ‰ 96 ç»´ï¼Œæ°å¥½æ˜¯ 1 ä¸ª head_dim çš„ç»´åº¦ã€‚é™¤äº† query å‘é‡è¿˜ä¿å­˜ç€ 8 ä¸ªå¤´ï¼Œkey å’Œ value å‘é‡éƒ½åªå‰© 1 ä¸ªã€Œ<strong>å…¬å…±å¤´</strong> ã€äº†</p>
<p>ä¸‹é¢æ¥æµ‹è¯•ä¸€ä¸‹MHAå’ŒMQAç»´åº¦çš„å˜åŒ–ï¼š</p>
<hr>
<pre><code>import math
  

import warnings
import torch
import torch.nn as nn
from einops import rearrange
  

from typing import Optional
  

  

def scaled_multihead_dot_product_attention(
        query,
        key,
        value,
        n_heads,
        past_key_value=None,
        softmax_scale=None,
        attn_bias=None,
        key_padding_mask=None,
        is_causal=False,
        dropout_p=0.0,
        training=False,
        needs_weights=False,
        multiquery=False,
    ):
    q = rearrange(query, 'b s (h d) -&gt; b h s d', h=n_heads)         # (1, 512, 768) -&gt; (1, 8, 512, 96)
    kv_n_heads = 1 if multiquery else n_heads
    k = rearrange(key, 'b s (h d) -&gt; b h d s', h=kv_n_heads)        # (1, 512, 768) -&gt; (1, 8, 96, 512) if not multiquery 
                                                                    # (1, 512, 96) -&gt; (1, 1, 96, 512)  if multiquery
    v = rearrange(value, 'b s (h d) -&gt; b h s d', h=kv_n_heads)      # (1, 512, 768) -&gt; (1, 8, 512, 96) if not multiquery 
                                                                    # (1, 512, 96) -&gt; (1, 1, 512, 96)  if multiquery
  

    attn_weight = q.matmul(k) * softmax_scale                       # (1, 8, 512, 512)
    attn_weight = torch.softmax(attn_weight, dim=-1)                # (1, 8, 512, 512)
  

    out = attn_weight.matmul(v)                                     # (1, 8, 512, 512) * (1, 1, 512, 96) = (1, 8, 512, 96)
    out = rearrange(out, 'b h s d -&gt; b s (h d)')                    # (1, 512, 768)
  

    return out, attn_weight, past_key_value
  

  

class MultiheadAttention(nn.Module):
    &quot;&quot;&quot;Multi-head self attention.
  

    Using torch or triton attention implemetation enables user to also use
    additive bias.
    &quot;&quot;&quot;
  

    def __init__(
        self,
        d_model: int,
        n_heads: int,
        attn_impl: str = 'triton',
        clip_qkv: Optional[float] = None,
        qk_ln: bool = False,
        softmax_scale: Optional[float] = None,
        attn_pdrop: float = 0.0,
        low_precision_layernorm: bool = False,
        verbose: int = 0,
        device: Optional[str] = None,
    ):
        super().__init__()
  

        self.attn_impl = attn_impl
        self.clip_qkv = clip_qkv
        self.qk_ln = qk_ln
  

        self.d_model = d_model
        self.n_heads = n_heads
        self.softmax_scale = softmax_scale
        if self.softmax_scale is None:
            self.softmax_scale = 1 / math.sqrt(self.d_model / self.n_heads)
        self.attn_dropout_p = attn_pdrop
  

        self.Wqkv = nn.Linear(self.d_model, 3 * self.d_model, device=device)
  

        fuse_splits = (d_model, 2 * d_model)
        self.Wqkv._fused = (0, fuse_splits)  # type: ignore
  

        self.attn_fn = scaled_multihead_dot_product_attention
        self.out_proj = nn.Linear(self.d_model, self.d_model, device=device)
        self.out_proj._is_residual = True  # type: ignore
  

    def forward(
        self,
        x,
        past_key_value=None,
        attn_bias=None,
        attention_mask=None,
        is_causal=True,
        needs_weights=False,
    ):
        qkv = self.Wqkv(x)                                              # (1, 512, 2304)
  

        if self.clip_qkv:
            qkv.clamp_(min=-self.clip_qkv, max=self.clip_qkv)
  

        query, key, value = qkv.chunk(3, dim=2)                         # both q, k, v: (1, 512, 768)
  

        key_padding_mask = attention_mask
  

        context, attn_weights, past_key_value = self.attn_fn(
            query,
            key,
            value,
            self.n_heads,
            past_key_value=past_key_value,
            softmax_scale=self.softmax_scale,
            attn_bias=attn_bias,
            key_padding_mask=key_padding_mask,
            is_causal=is_causal,
            dropout_p=self.attn_dropout_p,
            training=self.training,
            needs_weights=needs_weights,
        )
  

        return self.out_proj(context), attn_weights, past_key_value
  

  

class MultiQueryAttention(nn.Module):
    &quot;&quot;&quot;Multi-Query self attention.
  

    Using torch or triton attention implemetation enables user to also use
    additive bias.
    &quot;&quot;&quot;
  

    def __init__(
        self,
        d_model: int,
        n_heads: int,
        attn_impl: str = 'triton',
        clip_qkv: Optional[float] = None,
        qk_ln: bool = False,
        softmax_scale: Optional[float] = None,
        attn_pdrop: float = 0.0,
        low_precision_layernorm: bool = False,
        verbose: int = 0,
        device: Optional[str] = None,
    ):
        super().__init__()
  

        self.attn_impl = attn_impl
        self.clip_qkv = clip_qkv
        self.qk_ln = qk_ln
  

        self.d_model = d_model
        self.n_heads = n_heads
        self.head_dim = d_model // n_heads
        self.softmax_scale = softmax_scale
        if self.softmax_scale is None:
            self.softmax_scale = 1 / math.sqrt(self.head_dim)
        self.attn_dropout_p = attn_pdrop
  

        self.Wqkv = nn.Linear(
            d_model,
            d_model + 2 * self.head_dim,
            device=device,
        )
  

        fuse_splits = (d_model, d_model + self.head_dim)
        self.Wqkv._fused = (0, fuse_splits)  # type: ignore
  

        self.attn_fn = scaled_multihead_dot_product_attention
        self.out_proj = nn.Linear(self.d_model, self.d_model, device=device)
        self.out_proj._is_residual = True  # type: ignore
  

    def forward(
        self,
        x,
        past_key_value=None,
        attn_bias=None,
        attention_mask=None,
        is_causal=True,
        needs_weights=False,
    ):
        qkv = self.Wqkv(x)                                      # (1, 512, 960)
  

        if self.clip_qkv:
            qkv.clamp_(min=-self.clip_qkv, max=self.clip_qkv)
  

        query, key, value = qkv.split(                                  # query -&gt; (1, 512, 768)
            [self.d_model, self.head_dim, self.head_dim],               # key   -&gt; (1, 512, 96)
            dim=2                                                       # value -&gt; (1, 512, 96)
        )
  

        key_padding_mask = attention_mask
  

        if self.qk_ln:
            # Applying layernorm to qk
            dtype = query.dtype
            query = self.q_ln(query).to(dtype)
            key = self.k_ln(key).to(dtype)
  

        context, attn_weights, past_key_value = self.attn_fn(
            query,
            key,
            value,
            self.n_heads,
            past_key_value=past_key_value,
            softmax_scale=self.softmax_scale,
            attn_bias=attn_bias,
            key_padding_mask=key_padding_mask,
            is_causal=is_causal,
            dropout_p=self.attn_dropout_p,
            training=self.training,
            needs_weights=needs_weights,
            multiquery=True,
        )
  

        return self.out_proj(context), attn_weights, past_key_value
  

  

if __name__ == '__main__':
    # attn = MultiQueryAttention(
    #     768,
    #     8,
    #     'torch'
    # )
  

    attn = MultiheadAttention(
        768,
        8,
        'torch'
    )
  

    attn(
        torch.ones(size=(1, 512, 768))
    )
</code></pre>
<p><strong>å››ã€FlashAttention</strong></p>
<p><strong>è®ºæ–‡åœ°å€</strong> ï¼šhttps://arxiv.org/abs/2205.14135</p>
<p><strong>ä»£ç åœ°å€</strong> ï¼šhttps://github.com/HazyResearch/flash-attention</p>
<p>Transformerçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶(self-attention)çš„è®¡ç®—çš„æ—¶é—´å¤æ‚åº¦å’Œç©ºé—´å¤æ‚åº¦éƒ½ä¸åºåˆ—é•¿åº¦æœ‰å…³ï¼Œæ—¶é—´å¤æ‚åº¦æ˜¯  ï¼Œæ‰€ä»¥åœ¨å¤„ç†é•¿åºåˆ—çš„æ—¶å€™ä¼šå˜çš„æ›´æ…¢ï¼ŒåŒæ—¶å†…å­˜ä¼šå¢é•¿æ›´å¤šã€‚é€šå¸¸çš„ä¼˜åŒ–æ˜¯é’ˆå¯¹è®¡ç®—å¤æ‚åº¦(é€šè¿‡FLOPs æ•°è¡¡é‡), ä¼˜åŒ–ä¼šæƒè¡¡æ¨¡å‹è´¨é‡å’Œè®¡ç®—é€Ÿåº¦ã€‚</p>
<p>åœ¨FlashAttentionä¸­è€ƒè™‘åˆ°attentionç®—æ³•ä¹Ÿæ˜¯IOæ•æ„Ÿçš„ï¼Œé€šè¿‡å¯¹GPUæ˜¾å­˜è®¿é—®çš„æ”¹è¿›æ¥å¯¹attentionç®—æ³•çš„å®ç°è¿›è¡Œä¼˜åŒ–ã€‚å¦‚ä¸‹å›¾ï¼Œåœ¨GPUä¸­ç‰‡ä¸Šå­˜å‚¨SRAMè®¿é—®é€Ÿåº¦æœ€å¿«ï¼Œå¯¹åº”çš„HBM(high bandwidth memory)è®¿é—®é€Ÿåº¦è¾ƒæ…¢ï¼Œä¸ºäº†åŠ é€Ÿè¦å°½é‡å‡å°‘HBMçš„è®¿é—®æ¬¡æ•°ã€‚</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_jpg/N5aX12H1SickXvicAjKq5rMoFUic88u8pJ3dGibibTBiaHTmxWNMAOxvBGlmgHxAy1QC44F2AVhdrKwlXrQYm8TXydpQ/640?wx_fmt=jpeg" alt=""></p>
<p><strong>4.1 æ ‡å‡†Transformerç®€è¿°</strong></p>
<p>æ ‡å‡†çš„attentionç®—æ³•å®ç°ä¸­çš„QKVéƒ½æ˜¯ä¸HBMäº¤äº’çš„ï¼Œå…·ä½“å¦‚ä¸‹ï¼š</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_jpg/N5aX12H1SickXvicAjKq5rMoFUic88u8pJ3bgu6SUSibnpia3UEMHKeeM3BzzDymUlIV3wFVJ75w6a6Qicy76uib4lddQ/640?wx_fmt=jpeg" alt=""></p>
<p>####<strong>4.2 FlashAttentionç®—æ³•å®ç°çš„å…³é”®ä¸‰ç‚¹ï¼š</strong></p>
<ol>
<li>
<p>softmaxçš„tilingå±•å¼€ï¼Œå¯ä»¥æ”¯æŒsoftmaxçš„æ‹†åˆ†å¹¶è¡Œè®¡ç®—ï¼Œä»è€Œæå‡è®¡ç®—æ•ˆç‡</p>
</li>
<li>
<p>åå‘è¿‡ç¨‹ä¸­çš„é‡è®¡ç®—ï¼Œå‡å°‘å¤§é‡çš„æ˜¾å­˜å ç”¨ï¼ŒèŠ‚çœæ˜¾å­˜å¼€é”€ã€‚</p>
</li>
<li>
<p>é€šè¿‡CUDAç¼–ç¨‹å®ç°fusion kernel</p>
</li>
</ol>
<p>####<strong>4.2.1 softmaxå±•å¼€(tiling)</strong></p>
<pre><code>* åŸºæœ¬softmaxï¼šåœ¨è®¡ç®—  çš„å€¼çš„æ—¶å€™éœ€è¦ç”¨åˆ°æ‰€æœ‰çš„  å€¼ï¼Œè®¡ç®—å…¬å¼å¦‚ä¸‹ï¼š
</code></pre>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_jpg/N5aX12H1SickXvicAjKq5rMoFUic88u8pJ3fbcO6jStjEfzW6iaOEnHN3x6DRn2APF3q24P6pc7WEIcS1sNpbDaXcA/640?wx_fmt=jpeg" alt=""></p>
<pre><code>* å®‰å…¨(safe) softmaxï¼šç”±äº  å¾ˆå®¹æ˜“æº¢å‡º, æ¯”å¦‚FP16æ”¯æŒèŒƒå›´æ˜¯ ï¼Œå½“  çš„æ—¶å€™ï¼Œ ä¼šè¶…è¿‡float16çš„æœ‰æ•ˆä½ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜æå‡º safe softmax, å¯¹æ¯ä¸ª  éƒ½å‡å»ä¸€ä¸ª  , ä½¿å¾—  , è¿™æ—¶å¹‚æ“ä½œç¬¦å¯¹è´Ÿæ•°è¾“å…¥çš„è®¡ç®—æ˜¯å‡†ç¡®ä¸”å®‰å…¨çš„ã€‚
</code></pre>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_jpg/N5aX12H1SickXvicAjKq5rMoFUic88u8pJ3mYj2UjbHz4ia2sADJIB0SU7k2dE0Kn6ZYtAJCz4nZEvyoU8lsnyd9Fw/640?wx_fmt=jpeg" alt=""></p>
<pre><code>* Safe softmax tilingï¼šå¯¹äº X åˆ†ä¸ºä¸¤ç»„æƒ…å†µè¿›è¡Œè¯´æ˜ï¼Œå…¶ä¸­ 
</code></pre>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_jpg/N5aX12H1SickXvicAjKq5rMoFUic88u8pJ3icrJLFXtjjWgbfoLnMECtgFXgE6TXXZuqvGwXM6zC0fEEmLDOib4L4wg/640?wx_fmt=jpeg" alt=""></p>
<ul>
<li>safe softmaxåŸºæœ¬è®¡ç®—ç¤ºä¾‹</li>
</ul>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_jpg/N5aX12H1SickXvicAjKq5rMoFUic88u8pJ3T2xflI30ll8QjoshwP58o6HmPJvdGQiauJ09P5gianYVY5OZexHHfzGQ/640?wx_fmt=jpeg" alt=""></p>
<ul>
<li>safe softmax tilingè®¡ç®—ç¤ºä¾‹ï¼ˆç»“æœè·ŸåŸºæœ¬è®¡ç®—ç¤ºä¾‹ä¸€è‡´ï¼‰</li>
</ul>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_jpg/N5aX12H1SickXvicAjKq5rMoFUic88u8pJ3N5wiaD1SCjXHtZEiaJUN3NQYyNlj6xdyZ1ic7pGarLBdaRv4HSj3u7TtQ/640?wx_fmt=jpeg" alt=""></p>
<p>æœ‰äº†softmax tilingçš„åŸºç¡€ä»¥åï¼Œåœ¨æ‰§è¡Œçš„æ—¶å€™å¯ä»¥å¯¹Qã€Kã€V ä¸‰ä¸ªçŸ©é˜µè¿›è¡Œåˆ†å—æ“ä½œå¹¶è¡Œè®¡ç®—äº†ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_jpg/N5aX12H1SickXvicAjKq5rMoFUic88u8pJ3kjHD7vWaPqKDNduXYzHdgxKWb6ZIV2QArSsLcQCg4zAACdNeXcbfrA/640?wx_fmt=jpeg" alt=""></p>
<p><strong>4.2.2 åå‘è¿‡ç¨‹ä¸­çš„é‡è®¡ç®—</strong></p>
<p>ç±»ä¼¼äºgradient checkpointæ–¹æ³•ï¼Œåœ¨å‰å‘çš„æ—¶å€™æŠŠè¾“å‡ºç»“æœ  ã€  ã€  å­˜å…¥HBMä¸­, åœ¨åå‘æ—¶å€™é‡æ–°è®¡ç®—éœ€è¦çš„æ•°æ®ï¼Œæœ€ç»ˆå®Œæ•´çš„ç®—æ³•è¯´æ˜å¦‚ä¸‹ï¼š</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_jpg/N5aX12H1SickXvicAjKq5rMoFUic88u8pJ3voMFF9qelAhbqiaaIicL4uWDNdvZ7V244tSFqAe1YL8ia3kUFictvg5Jbg/640?wx_fmt=jpeg" alt=""></p>
<p><strong>4.3 å®éªŒæ•ˆæœ</strong></p>
<p><strong>BERT</strong></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_jpg/N5aX12H1SickXvicAjKq5rMoFUic88u8pJ30BQU5JceSlkGyGyZVwo9pSkQUEmIKnK6U0ZictYFB4IJEicDwc4icGicFg/640?wx_fmt=jpeg" alt=""></p>
<p><strong>GPT-2</strong></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_jpg/N5aX12H1SickXvicAjKq5rMoFUic88u8pJ3lUWZ8hKTYOIdelYTndb6h1wPdCA8PpU6e8NFXKdU1j7NrsBicWR9hKA/640?wx_fmt=jpeg" alt=""></p>
<p><strong>Long-range Arena</strong></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_jpg/N5aX12H1SickXvicAjKq5rMoFUic88u8pJ3dg56GaqS6jdnty7iaFWtywBq2JfBZcQJU2V9ceNM4rOo856xSXm83Xg/640?wx_fmt=jpeg" alt=""></p>
<p><strong>å‚è€ƒæ–‡çŒ®</strong> ï¼š</p>
<p>[1] <a href="https://github.com/THUDM/ChatGLM2-6B">https://github.com/THUDM/ChatGLM2-6B</a></p>
<p>[2] <a href="https://github.com/mosaicml/llm-foundry/blob/9c89ab263e72fb9610f28c8ab9cde5d2205b6bff/llmfoundry/models/layers/attention.py">https://github.com/mosaicml/llm-foundry/blob/9c89ab263e72fb9610f28c8ab9cde5d2205b6bff/llmfoundry/models/layers/attention.py</a></p>
<p>[3]https://paperswithcode.com/paper/flashattention-fast-and-memory-efficient</p>
<p><strong>è¿›æŠ€æœ¯äº¤æµç¾¤è¯·æ·»åŠ AINLPå°åŠ©æ‰‹å¾®ä¿¡ï¼ˆid:Â ainlp2)</strong></p>
<p><strong>è¯·å¤‡æ³¨å…·ä½“æ–¹å‘+æ‰€ç”¨åˆ°çš„ç›¸å…³æŠ€æœ¯ç‚¹</strong></p>
<pre><code>![](https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_jpg/nW2ZPfuYqSJADkmZ2IX6Z23znAibuEevotDMq9iaMxiapK7jfMibiauGFkycicAJEs6x5U9SGyDJZ0S1tRed9TPNUUDQ/640?wx_fmt=jpeg&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1)
</code></pre>
<p><strong>å…³äºAINLP</strong></p>
<pre><code>AINLP æ˜¯ä¸€ä¸ªæœ‰è¶£æœ‰AIçš„è‡ªç„¶è¯­è¨€å¤„ç†ç¤¾åŒºï¼Œä¸“æ³¨äº AIã€NLPã€æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€æ¨èç®—æ³•ç­‰ç›¸å…³æŠ€æœ¯çš„åˆ†äº«ï¼Œä¸»é¢˜åŒ…æ‹¬LLMã€é¢„è®­ç»ƒæ¨¡å‹ã€è‡ªåŠ¨ç”Ÿæˆã€æ–‡æœ¬æ‘˜è¦ã€æ™ºèƒ½é—®ç­”ã€èŠå¤©æœºå™¨äººã€æœºå™¨ç¿»è¯‘ã€çŸ¥è¯†å›¾è°±ã€æ¨èç³»ç»Ÿã€è®¡ç®—å¹¿å‘Šã€æ‹›è˜ä¿¡æ¯ã€æ±‚èŒç»éªŒåˆ†äº«ç­‰ï¼Œæ¬¢è¿å…³æ³¨ï¼åŠ æŠ€æœ¯äº¤æµç¾¤è¯·æ·»åŠ AINLPå°åŠ©æ‰‹å¾®ä¿¡(idï¼šainlp2)ï¼Œå¤‡æ³¨å·¥ä½œ/ç ”ç©¶æ–¹å‘+åŠ ç¾¤ç›®çš„ã€‚

  


  


![](https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_jpg/nW2ZPfuYqSKABHCqVVQkVYPrM4XY1vsd0iaeuXzyJnoFc8cibd5mYb4wdA3WMQtiaPVmr0XLZHMuVibqWncibpnTSnQ/640?wx_fmt=jpeg&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1)
</code></pre>
<p><strong>é˜…è¯»è‡³æ­¤äº†ï¼Œåˆ†äº«ã€ç‚¹èµã€åœ¨çœ‹ä¸‰é€‰ä¸€å§ğŸ™</strong></p>
<p>æ›´å¤šAIå·¥å…·ï¼Œå‚è€ƒ<a href="https://aibard123.com/">Github-AiBard123</a>ï¼Œ<a href="https://aibard123.com/">å›½å†…AiBard123</a></p>



          </div>

å¯å…³æ³¨æˆ‘ä»¬çš„å…¬ä¼—å·ï¼šæ¯å¤©AIæ–°å·¥å…·

<p><img src="/images/aitools/2024/03/qrcode_for_gh_dde1b429630d_258.jpg" alt=""></p>

        </article>

      </div>
    </div>
  </div>
</section>
        </div>
    </div>
    </main>




<script type='text/javascript' src='/assets/js/jquery.ui.touch-punch.min-0.2.2.js' id='jqueryui-touch-js'></script>
<script type='text/javascript' src='/assets/js/clipboard.min-5.6.2.js' id='clipboard-js'></script>
<script type='text/javascript' src='/assets/js/tooltip-extend.js' id='iplaycode-nav-js'></script>
<script type='text/javascript' id='popper-js-extra'>
 

var theme = {"ajaxurl":"","addico":"https:\/\/nav.baidu.cn\/wp-content\/themes\/onenav\/images\/add.png","order":"asc","formpostion":"top","defaultclass":"io-grey-mode","isCustomize":"1","icourl":"","icopng":".png","urlformat":"1","customizemax":"10","newWindow":"0","lazyload":"1","minNav":"1","loading":"1","hotWords":"baidu","classColumns":" col-sm-6 col-md-4 col-xl-5a col-xxl-6a ","apikey":"TWpBeU1UVTNOekk1TWpVMEIvZ1M2bFVIQllUMmxsV1dZelkxQTVPVzB3UW04eldGQmxhM3BNWW14bVNtWk4="};
 
</script>
<script type='text/javascript' src='/assets/js/popper.min.js' id='popper-js'></script>
<script type='text/javascript' src='/assets/js/bootstrap.min-4.3.1.js' id='bootstrap-js'></script>
<script type='text/javascript' src='/assets/js/theia-sticky-sidebar-1.5.0.js' id='sidebar-js'></script>
<script type='text/javascript' src='/assets/js/lazyload.min-12.4.0.js' id='lazyload-js'></script>
<script type='text/javascript' src='/assets/js/fancybox.min-3.5.7.js' id='lightbox-js-js'></script>

<script type='text/javascript' src='/assets/js/app-anim.js' id='appanim-js'></script>

<script type="text/javascript">
    $(document).ready(function(){
        var siteWelcome = $('#loading');
        siteWelcome.addClass('close');
        setTimeout(function() {
            siteWelcome.remove();
        }, 600);
    });
</script>
<script>        
    $(document).ready(function(){
        setTimeout(function () {
            if ($('a.smooth[href="' + window.location.hash + '"]')[0]) {
                $('a.smooth[href="' + window.location.hash + '"]').click();
            }else if (window.location.hash != '') {
                $("html, body").animate({
                    scrollTop: $(window.location.hash).offset().top - 90
                }, {
                    duration: 500,
                    easing: "swing"
                });
            }
        }, 300);
        $(document).on('click','a.smooth',function(ev) {
            if($('#sidebar').hasClass('show') && !$(this).hasClass('change-href')){
                $('#sidebar').modal('toggle');
            }
            if($(this).attr("href").substr(0, 1) == "#"){
                $("html, body").animate({
                    scrollTop: $($(this).attr("href")).offset().top - 90
                }, {
                    duration: 500,
                    easing: "swing"
                });
            }
            if($(this).hasClass('go-search-btn')){
                $('#search-text').focus();
            }
            if(!$(this).hasClass('change-href')){
                var menu =  $("a"+$(this).attr("href"));
                menu.click();
                toTarget(menu.parent().parent(),true,true);
            }
        });
        $(document).on('click','a.tab-noajax',function(ev) {
            var url = $(this).data('link');
            if(url)
                $(this).parents('.d-flex.flex-fill.flex-tab').children('.btn-move.tab-move').show().attr('href', url);
            else
                $(this).parents('.d-flex.flex-fill.flex-tab').children('.btn-move.tab-move').hide();
        });
        
    });
</script>

<script>

(function(){
    if(document.cookie.replace(/(?:(?:^|.*;\s*)night\s*\=\s*([^;]*).*$)|^.*$/, "$1") === ''){
        if(new Date().getHours() > 22 || new Date().getHours() < 6){
            document.body.classList.remove('io-black-mode');
            document.body.classList.add('io-grey-mode');
            document.cookie = "night=1;path=/";
            console.log('å¤œé—´æ¨¡å¼å¼€å¯');
        }else{
            document.body.classList.remove('night');
            document.cookie = "night=0;path=/";
            console.log('å¤œé—´æ¨¡å¼å…³é—­');
        }
    }else{
        var night = document.cookie.replace(/(?:(?:^|.*;\s*)night\s*\=\s*([^;]*).*$)|^.*$/, "$1") || '0';
        if(night == '0'){
            document.body.classList.remove('night');
        }else if(night == '1'){
            document.body.classList.add('night');
        }
    }
})();

$("#search-bg").css("background", "linear-gradient(#e2c4c4, #d8d8d8)");   
function switchNightMode(){
    var night = document.cookie.replace(/(?:(?:^|.*;\s*)night\s*\=\s*([^;]*).*$)|^.*$/, "$1") || '0';
    if(night == '0'){
	$("#search-bg").css("background", "linear-gradient(#e2c4c4, #d8d8d8)");
        document.body.classList.remove('io-grey-mode');
        document.body.classList.add('io-black-mode');
        document.cookie = "night=1;path=/"
        console.log(' ');
        $(".switch-dark-mode").attr("data-original-title","æ—¥é—´æ¨¡å¼");
        $(".mode-ico").removeClass("icon-night");
        $(".mode-ico").addClass("icon-light");
    }else{
	$("#search-bg").css("background", "linear-gradient(#4f4040, #1b1d1f)");
        document.body.classList.remove('io-black-mode');
        document.body.classList.add('io-grey-mode');
        document.cookie = "night=0;path=/"
        console.log(' ');
        $(".switch-dark-mode").attr("data-original-title","å¤œé—´æ¨¡å¼");
        $(".mode-ico").removeClass("icon-light");
        $(".mode-ico").addClass("icon-night");
    }
}
</script>


<script>
    var newsContainer = document.getElementById('news-container');
    var newsItems = document.getElementsByClassName('news-item');
    var currentItem = 0;

    setInterval(function() {
        
        newsItems[currentItem].classList.remove('show');
        newsItems[currentItem].style.transform = 'translateY(-20px)';
        
        currentItem = (currentItem + 1) % newsItems.length;
        newsItems[currentItem].style.transform = 'translateY(' + (newsContainer.offsetHeight - 20) + 'px)';
        setTimeout(function() {
            newsItems[currentItem].classList.add('show');
        }, 500);
    }, 8000);
</script>

</body>
</html>


